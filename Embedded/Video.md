# 영상

> 영상 관련된 것들이다

# AUDIO CODEC의 이해와 역사

---

- `오디오 코덱` : 오디오의 디지털 데이터 스트림을 부호화하거나 복호화하는 컴퓨터 프로그램이나 장치.

소프트웨어에서 오디오코덱은 주어진 오디오 파일이나 스트리밍 미디어 오디오 코딩 포맷에 따라 디지털 오디오 데이터를 압축하고 압축 해제를 하는 알고리즘을 구현하는 컴퓨터 프로그램이다. 알고리즘의 목적은 고품질의 오디오 신호를 품질을 유지하면서 최소한의 수의 비트로 표현하는 것이다. 이로써 저장된 오디오 파일의 전송에 요구되는 스토리지 공간과 대역을 효율적으로 줄일 수 있다. 대부분의 코덱들은 하나 이상의 `멀티미디어 플레이어`와 `라이브러리`형태로 구현된다. 하드웨어에서 오디오 코덱은 아날로그 신호를 디지털 신호로 인코딩하고 디지털 신호를 아날로그로 디코딩하는 단일장치를 말한다.

20세기 중반까지 코덱은 아날로그 신호를 PCM(펄스 코드 변조)를 이용해서 디지털 형태로 부호화하는 하드웨어 장치였다. 20세기 후반에는 압축을 포함한 다양한 디지털 신호 포맷 간의 변환을 위한 소프트웨어 종류도 포함한다.

오디오코덱은 아날로그 오디오신호를 전송, 저장을 위한 디지털신호로 변환하고 / 재생을 위해 디지털 신호를 다시 아날로그로 변환한다. 비디오 코덱은 비디오 신호에 같은 작업을 수행한다.

그래서 코덱이 뭐냐? -> 카메라로 촬영한 영상 데이터를 압축을 하고 또 압축을 푸는 과정

![image-20200716154423362](../../work/images/image-20200716154423362.png)

## MPEG-1

MP3로 대표되는 현세의 오디오 코덱은 1991년경에 MPEG-1이 표준화하면서 그 틀이 잡힌다. Moving Picture Expert Group의 약자.

MPEG-1에서는 CD안에 어떻게 하면 동영상까지를 넣을까, 그러니까 기존의 음악듣던 씨디로 동영상까지 재생되는 V-CD를 만들까 하는 것을 목표로 시작한 표준이다.

MPEG-1의 목표를 달성하기 위해서는 일당 영상이든 오디오든 압축이 필요하겠고, 압축을 한 오디오랑 비디오 이쁘게 잘 감싸주는 시스템이 필요할거다. 그래서 MPEG-1의 Part-1은 시스템, Part-2는 비디오, Part-3는 오디오를 각각 정의하도록 표준화 한다. 그래서 MPEG-1의 오디오 파트에 대한 표준은 ISO/IEC 11172-3이 된다.

MPEG-1 오디오는 당시 layer라는 표현으로 구별된 서로 다른 세 종류의 코덱이 복수로 표준화되었다. 그렇게 Lyaer-I,II,III의 세개 코덱이 MPEG-1의 표준이 되었다. 이들 간에는 서로 호환되는 것은 아니다. 위 그림에 화살표로 표시된 것 처럼 Lyaer-I,II는 MUSICAM이라는 한 뿌리에서 나왔는데 정확히는 MUSICAM=Layer-II로 같은거고 이를 좀 더 simplify한 것이 Layer-I이다.

당시에는 Layer-II도 연산량이 너무 높아서 쉽게 제품을 못만들꺼라 생각했는지 압축은 덜 되더라도 간단한 코덱을 만들어 Layer-I이라는 새끼를 치게된다. 반면 이제 MP3라는 이름(그러니까 MPEG-1 Layer-III를 줄여 MP3가 되었다)으로 웬만한 포터블 기기들은 물론 싸구려 USB memory stick에도 거져 들어가 있는 이 코덱이 당시의 하드웨어 기술로는 힘들어보였을 것임. 그래서 MP3가 압축 성능은 좋기는 하나 상용화는 먼 훗날의 얘기로 생각했을 것이다.

Layer-I과II의 뿌리가 된 MUSICAM은 유럽의 IRT와 Phillips에서 공동제안한 Subband Polypahse Filterbank계열의 코덱. 같은 시기에 EUREKA-147이라는 유럽권의 프로젝트로 디지털 라디오 방송을 만들고 있었는데 여기에 채택된 것이 MUSICAM. 현재 우리나라에서 즐기고 있는 DMB에서 라디오 방송일 때 사용되는 코덱이 바로 MUSICAM 혹은 MP2인것이다.



## MP3

MP3는 ASPEC이라는 코덱에 뿌리를 두고 만들었다. APSEC은 Adaptive Spectral Entropy Coding의 약자. 앞서 언급한 것 처럼 MDCT Transform 계열의 코덱인데, 당대 열강의 업체들에서 각자 만들어서 각자이름을 붙이고 있었던 Transform코더의 조상들 MSC, OCF, PXFM, AT&*T hybird, CNET 등등이 하나로 뭉쳐서 단일 코덱으로 만든게 ASPEC 그리고 이게 MPEG-1 Layer-III로 들어가서 MP3가 된 것이다. 



## Dolby AC-3

MPEG-1이 만들어지던 같은 시기에 샌프란시스코의 돌비사는 AC-3이라는 기념비적 코덱을 만들었다. 디지털시대의 돌비기술을 준비하며 AC-1, AC-2 업그레이드 하다가 AC-3라는 쿨한 코덱을 독자로 개발.

AC-3는 MDCT계열의 코덱이면서 MUSICAM과 엇비슷한 연산량을 가지면서, 심지어 5.1채널까지 압축이 가능한 코덱. 1991년에 MPEG에선 스테레오 코덱을 만들었는데, Dolby에선 5.1을 그것도 fully MDCT로 만들었으니 상당히 앞서간것.

게다가 자신들의 텃밭인 헐리우드의 컨텐츠 제작사들과 북미의 Digital TV 전송 규격인 ATSC에 기본 코덱으로 채택시킴



## apt-X & DTS

비슷한 시기 유럽 변방 북아일랜드 apt라는 회사에서 apt-X100이라는 서브밴드 계열 코덱으로 시작.

후에 DTS에 라이센싱 되어 현재 DVD타이틀에 AC-3와 함께 쌍벽을 이루는 코덱으로 자리잡은 그 DTS. DTS는 처음부터 AC-3보다 고음질이다! 라고 하는 것에 중점을 하였다. 이를 위해서 비트를 좀 더 쓰더라도(압축을 좀 덜하더라도) 절대 음질에선 비교우위에 있을 수 있는 서브밴드 필터뱅크 계열의 코덱을 채용한다.

즉 MUSICAM과 유사한 방법인데, 그 뿌리가 apt-X였다.

코덱의 성능이 기술적으로 우위에 있다기 보다는 마케팅과 로비의 힘(?)

최근 와서 무서운 속도로 다시 시장속으로 오고있음. 앞서 어디선가 다룬바대로 BlueTooth용 Hi-Fi 코덱 솔루션을 가지고.



## PAC, ATRAC

MP3의 전신인 ASPEC에는 J.Johnston(JJ)을 대표선수로 하는 AT&T의 기술들이 상당부분 반영되어 있음. 하지만 거기에 안주하지 않고 AT&T 독자의 코덱을 계속 개발해옴. 그렇게 만든 것이 PAC(Perceptual Audio Coder)

PAC 역시 정통 MDCT 계열의 코덱이고 훗날 만들어 지는 AAC와 상당히 유사하다. 그림에 나오는 MPAC는 PAC을 멀티채널로 확장한 것이다.

그리고 또 하나의 사제 코덱 ATRAC는 소니(Sony)것이다. 아직도 꾸준히 살아서 소니가 만드는 mp3 player(워크맨)들이나 Sony-Ericsson의 워크맨폰에서는 지원을 하는데 처음 만들때는 AC-3, DTS처럼 극장용 사운드 트랙의 경쟁기술로써 그리고 당시에 있었던 모바일 기기중 하나인 MD의 압축 포맷으로 사용할 목적이었음.

ATRAC은 subband-MDCT를 둘 다 쓰는 hybrid 필터뱅크 코덱의 전형이다. 서브밴드 필터뱅크는 QMF를 이용하여 먼저 3~4개 밴드 신호를 만들어내고, 각 밴드 신호를 MDCT로 변환해서 추가적인 주파수 해상도를 얻어낸다.

이 때 전송 레이트를 바꾸고 싶으면 높은 밴드 하나를 통째로 날려도 나머지 밴드만으로 소리가 만들어지는 것이 가능한데, 이런 scalability를 제공하는 것이 ATRAC의 장점.

현재는 ATRAC 3라는 포맷으로 진화를 하였고, ATRAC Lossless라는 별도 포맷도 만들어짐.

ATRAC의 Hybrid필터뱅크 구조는 나중에 MPEG-2 AAC 표준화 할 때, gain control(혹은 pre-processor)라는 tool로써 표준에 채택이 되었는데, 이 기술은 SSR(Scalable Sample Rate) 프로파일에서만 사용이 가능하고 현재 SSR 프로파일 AAC는 아무도 사용하지 않기 때문에 죽은 기술이 되어버림. 그런 이유로 그림에서는 ATRAC로부터 AAC로 화살표가 이어짐



## 다시 MPEG, MPEG-2

MPEG-01을 완성해놓고 보니, 당시 시대의 요구사항은 HDTV와 DVD였다. CD에다 동영상을 넣어서 V-CD를 만들자는 생각은 이미 old fashion이 되어버린 것. 그래서 MPEG-2를 기획하게 된다. MPEG-2의 제목은 "Information technology -- Generic coding of moving pictures and associated audio information"이다. 앞서 MPEG-1에서의 1.5mbps어쩌구를 빼고 좀 더 generic하게 간것.

Video는 HD급 영상을 어떻게 압축하냐가 관건이고 , 오디오는 이제 5.1채널을 다뤄야한다는 것. 문제는 이미 1991년에 5.1채널 지원의 Dolby AC-3가 만들어져 있떤 시점. MPEG에서는 부랴부랴, 그리고 직전에 만든 MPEG-1을 카니발라이즈 하지 않는 표준을 원했고, 그래서 오디오는 MPEG-1에서 만든 3개 layer의 코덱을 그대로 들어다가 MPEG-2 도장을 찍어준다.

대신 스테레오만 지원하던 것을 5.1까지 지원해주고 확장된 표본화 주파수를 지원해주고 하는 것이 달라진 점 !

또한 MPEG-2는 하향 호환성(Backward Compatibility)을 보장하기 위해 5.1채널로 부호화된 비트열이더라도 그 안에 MPEG-1에 호환되는 스테레오 파트를 먼저 집어넣고, 나머지 3.1에 해당하는 부분을 MPEG-1로 보면 extension에 해당하는 신택스 공간에 밀어 넣는 방법을 사용한다. 물론 16, 22.05, 24kHz처럼 지원할 수 없는 것들은 포기.

그 밖에도 사실 몇 개의 다채널에서 압축을 잘할 수 있는 툴들을 포함시키긴 했는데, 다 사용되진 않음. 왜냐하면 MPEG-2가 비록 5.1을 지원하는 표준이긴 하지만 아무도 MPEG-2를 멀티채널로 서비스하지 않았기 때문.

MPEG-2가 유럽방식의 DTV표준인 DVB-T에 채택되어 사용되고 있는데 유럽에서는 방송으로 5.1을 안날렸음. 불행히도

여튼 그래서 MPEG-2라고는 하나 오디오는 사실상 MPEG-1과 달라진 것이 없다. MPEG-1 Layer-III나 MPEG-2 Layer-III나 MPEG-2.5 Layer-III나 다 같은 MP3인것이다. 이렇게 MPEG-2는 1994년에 완성이 된다.



## 집대성, MPEG-2 AAC

MPEG-2 표준이 완성된 이후, 체급이 같던 MPEG Layer-II와 AC-3의 시장 쟁탈전에서 슬슬 AC-3로 기우는 분위기가 연출

음질적으로 비교우위라는 입소문과 5.1채널 지원, 헐리우드와의 커넥션까지 Dolby가 가진 비교우위가 막강했던 것. DVD나 ATSC의 예처럼 비디오는 MPEG-2를 수용하면서도 오디오는 사제코덱이랄 수 있는 AC-3를 채택해버리는것이죠.

DVD가 AC-를 사용한다는 것은 시장에 돌아다니는 컨텐츠가 모두 AC-3라는 얘기가 되어버려 사실상 경쟁이 어려워지는 것. Ray Dolby(돌비 창업자)는 공돌이로도 비즈맨으로도 참 영리한 사람이었던 것 같음. 여튼 그런상황에서 MPEG 오디오쟁이들은 다시 고민을 한다.

지난 MPEG-2가 급하게, 그것도 MPEG-1과의 호환성을 고려하여 만들다보니, 더 좋은 기술들이 있음에도 불구하고 채택이 되지 못했다. 그러니 새로이 하향 호환성을 포기하더라도 더 좋은 압축성능을 낼 수 있는 새로운 코덱을 표준화 해보자. 그래서 MPEG-2 NBC라는 프로젝트에 착수하게 된다.

이제 호환성을 포기했으니 세상에 좋다는 기술은 다 가져다 붙일수 있게 된다. 그림의 화살표에 나온것 처럼 기존 코덱들의 장점은 일단 다 끌어온다. 기본적으로 연산량 고려안한 기술 집약적 코덱이었던 MP3는 NBC에서도 기본뿌리 역할을 한다. 심리 음향 모델에 근거한 스케일팩터를 사용하여 비트를 할당하는 Analysis by Synthesis 구조에다가 3/4승의 비선형 양자화며, 세련되기 face lift했지만 여전히 뼈대는 그대로인 구조임. 대신 subband후 MDCT하는 비효율적 hybird구조를 빼버리고 2048point까지 MDCT의 블럭 사이즈를 확장하고 양자화된 MDCT 계수를 추가로 압축하는 Huffman coding에도 세련미를 가미.

하지만, 무엇보다 TNS(Ternporal Noise Shaping)라는 새로운 툴을 도입한 것이 NBC에서의 큰 변화로 보여짐.(TNS가 얼마 만큼 음질 향상 효과가 있을까를 떠나서, 기존 코덱이 가지고 있던 문제를 명확히 정의하고, 문제를 풀기위해 머리를 굴리고, 그러다가 남들이 생각하지 못했던 새로운 방법을 찾아내고, 이를 끝내 구현해내고... 우리 엔지니어들이 답습해야할 BEST PRACTICE)

TNS역시 MP3 개국 공신들이었던 JJ(AT&T)와 J.Herre(FhG)의 작품인데, 처음에 이 기술을 논문을 접하고 매우 놀람.

ATRAC에서 언급했던 4 band 서브밴드 필터뱅크 후 MDCT 계수로부터 예측하고 그 찌꺼기(residual) 신호만 코딩을 하겠다는 생각만 해도 엄청난 연산량의 코딩 툴도 이때 도입이 됨

그렇게 완성된 표준은 NBC(Non-Backward Compatible)라는 표준화 단계에서의 어색한 이름을 버리고 MPEG-2 AAC(Advanced Audio Coding)이라는 깔끔한 이름을 부여받는다. 표준번호로는 1381807(MPEG-2의 part7라는 뜻). 완성시점은 1997년.

AAC는 최대 48채널까지도 부호화가 가능하고, 표본화 주파수도 최대 192kHz까지 아무 샘플링이나 가능하고, 등등 기존에 MPEG-2 BC가 가지고 있던 신택스의 경직성도 최대한 풀어두었다.

이렇게 틀을 완성한 AAC가 현재(2010년)까지도 업계 전반에서 주력으로 사용하는 오디오 코덱의 마지막이다. 현재 표준화가 진행중인 USAC 이 그 바통을 이어받을지 모르지만, USAC안에도 AAC가 다 딸려들어가기 때문에 기술도 유행도 끝나지 않을 불멸의 코덱이 되는 셈.

...

기타 등등

# 코덱(codec)이란 무엇일까?

---

코덱이란 영상장비에서 촬영된 영상을 전송하기에는 파일 사이즈가 너무 커지므로 용량이 큰 파일을 작게 만들어주는 과정이 꼭 필요한데 이것이 바로 `인코딩`이라고 합니다. 인코딩된 이미지는 그냥 볼 수 없고 본래대로 재생할 수 있게 압축을 푸는 기술인 `디코딩`을 거쳐야 합니다. 압축을하고 또 압축을 푸는 하나의 프로그램을 `코덱(codec)`이라고 합니다. (코덱마다 인코딩, 디코딩 알고리즘 자체가 다르다)

압축방식은 크게 2가지로 나눌 수 있다. MJPEG와 H.264방식이다. 2개의 코덱을 선택한 이유는 하나는 이미지 압축 방식(MJPEG)을 따르고 있고 다른 하나는 비디오 압축 방식을 따르고 있기 때문이다.

## 1. MJPEG

MJPEG는 이미지 압축방식이다.

![image-20200717100525005](../../work/images/image-20200717100525005.png)

## 2. H.264

H.264는 비디오 압축 방식이다.

![image-20200717100558667](../../work/images/image-20200717100558667.png)

## 차이점

MJPEG의 경우 처음부터 끝까지 자동차와 배경을 꾸준히 담아내고 있는 반면, H.264 코덱의 경우 첫 번째만 배경을 담고 나머지는 자동차의 움직임만을 기록하는 방식이다. 그렇다면 어떤 차이점이 있을까?

- `MPEG` 
  - 압축된 데이터 크기는 다른 코덱에 비해 클 수 밖에 없다.
  - 프레임 손실이 있을 때 영상 손실이 적을 수 있다.
- `H.264 `
  - 비디오를 구성하는 프레임간 차이 데이터만 압축하는 방식을 사용하고 있다. 차이만 압축을 하므로 압축되는 데이터 크기가 이미지 압축방식보다 작다. (배경이미지를 저장하지 않아 데이터 감소를 기대할 수 있다.)
  - 프레임 간 연계성을 가지고 있어 프레임 손실이 있는 경우, 연계가 있는 프레임들은 재생을 할 수 없는 단점이 있다.

![image-20200717100951900](../../work/images/image-20200717100951900.png)

![image-20200717101016515](../../work/images/image-20200717101016515.png)

![image-20200717101028037](../../work/images/image-20200717101028037.png)

# 화질

---

>  화질에 중요한 영향을 미치는 2가지 요소

## 프레임레이트(Frame rate)

카메라에서 촬영된 영상은 한 장 한 장의 이미지로 구성되어 있으며 각각의 이미지를 프레임이라고 합니다.

프레임을 나타내는 단위는 fps(frame per second)이고, 1초 동안 재생되는 이미지 수를 의미합니다. 예를 들어 24프레임이라고 하면 초당 24장의 이미지가 있다는 의미. 프레임이 높을수록 자연스러운 영상을 얻을 수 있지만, 데이터 크기가 커지는 단점이 있다!

## 비트레이트(Bit rate)

앞에서 다룬 프레임 레이트는 초당 이미지의 갯수라면 비트레이트는 bps(Bits Per Second)로 1초의 영상을 구성하는 데이터 크기라고 생각하면 된다.

아무래도 데이터 크기가 작은것 보다는 데이터 크기가 커진다면 화면을 표현함에 있어 유리할 것이다!

초당 192kbps vs 26Mbps영상 자료 비교 : 26Mbps로 인코딩한 영상은 뭉개짐이 없는 것을 볼 수 있음

만약 192kbps 인코딩된 영상을 26Mbps로 인코딩하면 안될까? 라는 질문을 할 수 도 있는데 화질이 뭉개지는 영상을 26Mbps로 인코딩한다고 해서 퀄리티가 높아지지 않는다. 요약하면 높은 화질을 나쁘게 만들 수는 있어도 낮은 화질을 좋게 만들 수는 없다. (원본의 소중함~)

# 해상도 그리고 인터레이스(Interlaced)와 프로그레시브(Progressive)

---

해상도는 픽셀의 숫자를 말하는데 가로X세로를 곱한 것을 의미한다. 여기서 말하는 픽셀(화소)은 화면을 구성하는 가장 작은 단위를 말하며 하나의 점이라고 생각.

즉, 화소가 높다는 것은 이미지를 구성함에 있어 더 많은 픽셀로 표현한 것이다.

![image-20200717101311562](../../work/images/image-20200717101311562.png)

=> 왼쪽 이미지는 선명하게 보이는 반면 오른쪽 이미지는 흐릿



## 해상도 규격

![image-20200717101748973](../../work/images/image-20200717101748973.png)

| 구분        | 해상도        | 픽셀 수    | 포맷      |
| ----------- | ------------- | ---------- | --------- |
| SD          | 720 X 480     | 345,600    | 480p      |
| HD          | 1,280 X 720   | 921,600    | 720p      |
| FHD (Full)  | 1,920 X 1,080 | 2,073,600  | 1080p / i |
| QHD (Quad)  | 2.560 X 1,440 | 3,686,400  |           |
| UHD (Ultra) | 3,840 X 2,160 | 8,294,400  | 2160p     |
| 4K          | 4,096 X 2,160 | 8,847,360  | 2160p     |
| 8K          | 7,680 X 4,320 | 33,177,600 |           |

모바일과 같이 작은화면에서만 재생될 동영상을 만든다면, 'FHD' / 1080p, 720p, 혹은 그 이하도 큰 문제가 없다.

4K 등의 초고화질 영상 편집에는 어마어마한 4K 리소스가 필요한데 작은 화면에서만 재생될 영상에 이 많으나 리소스들을 낭비할 필요는 없다.

***그렇다면 픽셀수가 많을수록 무조건 좋은기기인가요?***

대게 높은 수준의 해상도를 가지고 있는 기기는 좋은 기기가 맞다. 하지만, 높은 해상도를 가지더라도 화면의 크기가 상대적으로 더 크거나 시청 거리가 가까워지게 되면(눈으로 식별할 수 있는 픽셀의 수가 많아지면), 사용자들이 느끼는 영상의 품질은 떨어질 수 밖에 없다. 이와 관련된 사항을 지표로 표현한 것이 바로 `PPI(Pixel Per Inch)`이다.

## PPI (Pixel Per Inch) 

> 해상도를 결정하는 지표

PPI는 1인치당 표현되어 있는 픽셀의 개수를 의미한다. 화면의 크기와 픽셀의 개수가 모두 포함된 지표이기 때문에 PPI 지표가 높을 수록 좋은 해상도를 가진 기기라는 것을 알 수 있다.

그렇다고 모바일 기기의 300ppi가 TV나 컴퓨터의 300ppi와 같은 품질을 사용자에게 경험시켜주진 않는다. 약간의 눈과 디스플레이의 거리가 멀어질수록 인간이 식별할 수 있는 픽셀의 수는 상대적으로 줄어들기 때문에, TV나 컴퓨터같이 먼 시청거리를 가진 기기가 모바일 기기보다 작은 PPI로도 높은 품질 경험을 사용자에게 줄 수 있다. 반면, 모바일 기기의 시청 거리(20cm ~ 30cm)는 컴퓨터나 TV보다 월등히 짧기 때문에, 상대적으로 TV보다 높은 PPI를 가져야만 TV의 해상도와 동일한 품질로 느낄 수 있다.

- PPI를 구하는 공식

```bash
[가로픽셀]^2 X [세로픽셀]^2 = [대각선픽셀]^2
[대각선픽셀] / [화면인치](대각선 길이) : `PPI`
```

![image-20200717103219092](../../work/images/image-20200717103219092.png)

## 인터레이스, 프로그레시브

TV광고나 카메라 광고를 보면 '1080p', '1080i'라고 써있는 것을 본적 있을 것이다.

여기서 말하는 'i'는 인터레이스, 'p'는 프로그레시브 스캔방식을 말하는데 이는 화면에 이미지를 '뿌려주는' 방식의 차이이다.

### 1. 인터레이스(Interlaced)

인터레이스 방식은 하나의 프레임에 수평 주사선을 1개 간격으로  뛰어 넘어 주사하는 방식이다. 홀수열과 짝수열을 주사한 것을 필드라고 불리며, 두 필드를 합쳐 하나의 완전한 영상을 만들어 완전한 하나의 프레임이 구성된다.

![image-20200717103548829](../../work/images/image-20200717103548829.png)

### 2. 프로그레시브(Progressive)

프로그레시브 스캔의 경우 아래 그림과 같이 완전한 한 장의 프레임 하나 하나를 연속적으로 보여주기 때문에 고화질 화면을 감상할 때 유리하다.

![image-20200717103704333](../../work/images/image-20200717103704333.png)

프로그레시브 방식은 한 장의 프레임 하나하나를 연속적으로 보여 주기 때문에 화면이 흔들리거나 `Flicker`(깜빡임 현상)가 발생한다. 이러한 프로그레시브 스캔의 단점을 보완할 수 있는 방식이 인터레이스이다.

인터레이스 방식은 프로그래시브 방식에 비해 깜빡거림이 적고 비교적 안정된 영상을 얻을 수 있다는 장점이 있으나 한 화면을 두 번(홀수, 짝수)에 걸쳐 주사하기 때문에 고화질 정보를 전송하기에는 불리하다.

# 채널

---

신호가 이동하는 경로를 채널(channel)이라고 하는데, 통신에서는 두 가지의 의미로 사용된다. 첫째로 각각의 신호들이 송수신되는 주파수대역 하나의 단위를 채널이라 부르고, 둘째로 신호가 전달되는 매질 특성 자체를 채널로 분류하기도 한다. 유선의 경우라면 케이블 자체를 채널이라 할 수 있고, 전자파를 이용한 무선통신에서는 공기중 자체가 채널이 된다. 실제로 통신에서 사용하는 채널은 매질 그 자체가 아니라 신호가 이동하고 있는 매질상에 존재하는 잡음(Noise) 성분에 따라 여러가지 채널로 분류된다.

즉 매질을 의미하는 채널은 잡음이라는 개념과 따로 생각할 수 없다.

ACOLADE의 Channel이라는 class에서 있는 모델들은 이러한 매질특성과 관련된 채널을 의미하며, 이러한 점에서 채널이 '특정한 잡음을 가진 신호전송로'라고 재정의될 수 있다.

공기중에는 수많은 종류의 잡음이 있으며, 이 잡음들은 모든 주파수에서 발생하기 때문에 이러한 잡음을 백색잡음(white noise)라고 한다. 주파수가 높은 전자파의 일종인 가시광선이 주파수에 따라 색상이 다르지만 여러 색광이 겹치면 하얀색이 되듯이, 여러 주파수의 잡음이 모인 것이라서 붙여진 의미이다. 그래서 이와는 반대로 특정 주파수에 집중된 잡음을 유색잡음(Color noice)라고 한다.

## AWGN

> Additive White Gaussian Noise

가장 일반적인 형태의 채널이며, 전 주파수 대역에서 고르게 잡음이 발생하는 채널이다. 특별한 주변요소 없이 자연상태 그대로의 랜덤한 잡음이다.

![image-20200717110352458](../../work/images/image-20200717110352458.png)

## 페이딩

> ~multipath fading

주변의 사물들에 의해 다중반사되는 전자파들이 서로 합성되어 일어나느 종류의 간섭잡음을 의미한다.

TV의 경우 여러 곳에서 반사되어 들어온 전자파가 합성되면서 윗아자가 틀리게 합성되어 화면이 흐릿하게 겹쳐보이는 고스트(Ghost)현상이 대표적인 fading의 예

이러한 종류의 페이딩은 주파수 조건에 따라서 Reayleigh와 Rician채널 등으로 세분화되기도 한다. 이동통신 환경에서의 채널은 기본적으로 Rayleigh fading으로 모델링하는 경우가 많다.

## SNR

> Signal-to-Noise Ratio : 신호대 잡음비

채널에서 뿐만 아니라 SNR은 자체적으로 대단히 중요한 의미를 가지는 지표인데 말 그대로 통신시호와 잡음간의 전력비를 의미한다.

주로 decibel(dB) 단위로 사용하게 되는데 이것의 좋고 나쁨이 채널을 통과한 신호의 왜곡정도에 가장 주요한 역할을 하게 된다.

```bash
SNR = 10log신호전력(W)/잡음전력(W)
```

## 수학적 모델

한 간단한 수학적인 채널 모델의 예는 다음과 같다.

```bash
y = hx + n
```

여기서 x는 송신신호 , h는 채널, n은 가산잡음 그리고 y는 수신 신호이다. 이 경우에 채널의 특성을 나타내는 신호대 잡음비는 다음과 같다.

![image-20200717110916843](../../work/images/image-20200717110916843.png)

여기서 Px는 송신 신호인 x의 평균 크기이며 No는 가산 잡음인 n의 스펙트럴 덴시티이다. 이 채널의 최대 용량 (capacity)를 구해보면 다음과 같다.

![image-20200717111027739](../../work/images/image-20200717111027739.png)

여기서 C의 단위는 BPS/Hz이다. 오류없이 보낼 수 있는 최대 데이터 전송률은 최대 용량인 C보다 작다는 것을 의미한다.

**유선 채널**

유선채널은 구리선에 의해 전기신호가 전달 되면서 생기는 특성을 나타낸다. 유선채널에서 h는 일반적으로 시간에 따라 크게 변하지 않아 결정된 한 값으로 표현

**무선 채널**

무선 채널은 공기전으로 전파 신호가 전달되면서 생기는 특성을 나타낸다. 무선채널에서 h는 일반적으로 시간에 따라 가변하는 랜덤한 값으로 표현된다. 따라서 무선채널에서 받은 신호의 신호대잡음비는 시간이 지마에 따라 가변적으로 변하게 된다.

