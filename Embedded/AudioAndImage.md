# **영상과 음성:arrow_forward:**

> 영상과 음성에 관련된 것들이다

# **AUDIO CODEC의 이해와 역사**

---

- `오디오 코덱` : 오디오의 디지털 데이터 스트림을 부호화하거나 복호화하는 컴퓨터 프로그램이나 장치.

소프트웨어에서 오디오코덱은 주어진 오디오 파일이나 스트리밍 미디어 오디오 코딩 포맷에 따라 디지털 오디오 데이터를 압축하고 압축 해제를 하는 알고리즘을 구현하는 컴퓨터 프로그램이다. 알고리즘의 목적은 고품질의 오디오 신호를 품질을 유지하면서 최소한의 수의 비트로 표현하는 것이다. 이로써 저장된 오디오 파일의 전송에 요구되는 스토리지 공간과 대역을 효율적으로 줄일 수 있다. 대부분의 코덱들은 하나 이상의 `멀티미디어 플레이어`와 `라이브러리`형태로 구현된다. 하드웨어에서 오디오 코덱은 아날로그 신호를 디지털 신호로 인코딩하고 디지털 신호를 아날로그로 디코딩하는 단일장치를 말한다.

20세기 중반까지 코덱은 아날로그 신호를 PCM(펄스 코드 변조)를 이용해서 디지털 형태로 부호화하는 하드웨어 장치였다. 20세기 후반에는 압축을 포함한 다양한 디지털 신호 포맷 간의 변환을 위한 소프트웨어 종류도 포함한다.

오디오코덱은 아날로그 오디오신호를 전송, 저장을 위한 디지털신호로 변환하고 / 재생을 위해 디지털 신호를 다시 아날로그로 변환한다. 비디오 코덱은 비디오 신호에 같은 작업을 수행한다.

그래서 코덱이 뭐냐? -> 카메라로 촬영한 영상 데이터를 압축을 하고 또 압축을 푸는 과정

![image-20200716154423362](https://user-images.githubusercontent.com/58545240/89979900-b9074080-dcab-11ea-97e1-2c53dbc417ce.png)

## MPEG-1

MP3로 대표되는 현세의 오디오 코덱은 1991년경에 MPEG-1이 표준화하면서 그 틀이 잡힌다. Moving Picture Expert Group의 약자.

MPEG-1에서는 CD안에 어떻게 하면 동영상까지를 넣을까, 그러니까 기존의 음악듣던 씨디로 동영상까지 재생되는 V-CD를 만들까 하는 것을 목표로 시작한 표준이다.

MPEG-1의 목표를 달성하기 위해서는 일당 영상이든 오디오든 압축이 필요하겠고, 압축을 한 오디오랑 비디오 이쁘게 잘 감싸주는 시스템이 필요할거다. 그래서 MPEG-1의 Part-1은 시스템, Part-2는 비디오, Part-3는 오디오를 각각 정의하도록 표준화 한다. 그래서 MPEG-1의 오디오 파트에 대한 표준은 ISO/IEC 11172-3이 된다.

MPEG-1 오디오는 당시 layer라는 표현으로 구별된 서로 다른 세 종류의 코덱이 복수로 표준화되었다. 그렇게 Lyaer-I,II,III의 세개 코덱이 MPEG-1의 표준이 되었다. 이들 간에는 서로 호환되는 것은 아니다. 위 그림에 화살표로 표시된 것 처럼 Lyaer-I,II는 MUSICAM이라는 한 뿌리에서 나왔는데 정확히는 MUSICAM=Layer-II로 같은거고 이를 좀 더 simplify한 것이 Layer-I이다.

당시에는 Layer-II도 연산량이 너무 높아서 쉽게 제품을 못만들꺼라 생각했는지 압축은 덜 되더라도 간단한 코덱을 만들어 Layer-I이라는 새끼를 치게된다. 반면 이제 MP3라는 이름(그러니까 MPEG-1 Layer-III를 줄여 MP3가 되었다)으로 웬만한 포터블 기기들은 물론 싸구려 USB memory stick에도 거져 들어가 있는 이 코덱이 당시의 하드웨어 기술로는 힘들어보였을 것임. 그래서 MP3가 압축 성능은 좋기는 하나 상용화는 먼 훗날의 얘기로 생각했을 것이다.

Layer-I과II의 뿌리가 된 MUSICAM은 유럽의 IRT와 Phillips에서 공동제안한 Subband Polypahse Filterbank계열의 코덱. 같은 시기에 EUREKA-147이라는 유럽권의 프로젝트로 디지털 라디오 방송을 만들고 있었는데 여기에 채택된 것이 MUSICAM. 현재 우리나라에서 즐기고 있는 DMB에서 라디오 방송일 때 사용되는 코덱이 바로 MUSICAM 혹은 MP2인것이다.



## MP3

MP3는 ASPEC이라는 코덱에 뿌리를 두고 만들었다. APSEC은 Adaptive Spectral Entropy Coding의 약자. 앞서 언급한 것 처럼 MDCT Transform 계열의 코덱인데, 당대 열강의 업체들에서 각자 만들어서 각자이름을 붙이고 있었던 Transform코더의 조상들 MSC, OCF, PXFM, AT&*T hybird, CNET 등등이 하나로 뭉쳐서 단일 코덱으로 만든게 ASPEC 그리고 이게 MPEG-1 Layer-III로 들어가서 MP3가 된 것이다. 



## Dolby AC-3

MPEG-1이 만들어지던 같은 시기에 샌프란시스코의 돌비사는 AC-3이라는 기념비적 코덱을 만들었다. 디지털시대의 돌비기술을 준비하며 AC-1, AC-2 업그레이드 하다가 AC-3라는 쿨한 코덱을 독자로 개발.

AC-3는 MDCT계열의 코덱이면서 MUSICAM과 엇비슷한 연산량을 가지면서, 심지어 5.1채널까지 압축이 가능한 코덱. 1991년에 MPEG에선 스테레오 코덱을 만들었는데, Dolby에선 5.1을 그것도 fully MDCT로 만들었으니 상당히 앞서간것.

게다가 자신들의 텃밭인 헐리우드의 컨텐츠 제작사들과 북미의 Digital TV 전송 규격인 ATSC에 기본 코덱으로 채택시킴



## apt-X & DTS

비슷한 시기 유럽 변방 북아일랜드 apt라는 회사에서 apt-X100이라는 서브밴드 계열 코덱으로 시작.

후에 DTS에 라이센싱 되어 현재 DVD타이틀에 AC-3와 함께 쌍벽을 이루는 코덱으로 자리잡은 그 DTS. DTS는 처음부터 AC-3보다 고음질이다! 라고 하는 것에 중점을 하였다. 이를 위해서 비트를 좀 더 쓰더라도(압축을 좀 덜하더라도) 절대 음질에선 비교우위에 있을 수 있는 서브밴드 필터뱅크 계열의 코덱을 채용한다.

즉 MUSICAM과 유사한 방법인데, 그 뿌리가 apt-X였다.

코덱의 성능이 기술적으로 우위에 있다기 보다는 마케팅과 로비의 힘(?)

최근 와서 무서운 속도로 다시 시장속으로 오고있음. 앞서 어디선가 다룬바대로 BlueTooth용 Hi-Fi 코덱 솔루션을 가지고.



## PAC, ATRAC

MP3의 전신인 ASPEC에는 J.Johnston(JJ)을 대표선수로 하는 AT&T의 기술들이 상당부분 반영되어 있음. 하지만 거기에 안주하지 않고 AT&T 독자의 코덱을 계속 개발해옴. 그렇게 만든 것이 PAC(Perceptual Audio Coder)

PAC 역시 정통 MDCT 계열의 코덱이고 훗날 만들어 지는 AAC와 상당히 유사하다. 그림에 나오는 MPAC는 PAC을 멀티채널로 확장한 것이다.

그리고 또 하나의 사제 코덱 ATRAC는 소니(Sony)것이다. 아직도 꾸준히 살아서 소니가 만드는 mp3 player(워크맨)들이나 Sony-Ericsson의 워크맨폰에서는 지원을 하는데 처음 만들때는 AC-3, DTS처럼 극장용 사운드 트랙의 경쟁기술로써 그리고 당시에 있었던 모바일 기기중 하나인 MD의 압축 포맷으로 사용할 목적이었음.

ATRAC은 subband-MDCT를 둘 다 쓰는 hybrid 필터뱅크 코덱의 전형이다. 서브밴드 필터뱅크는 QMF를 이용하여 먼저 3~4개 밴드 신호를 만들어내고, 각 밴드 신호를 MDCT로 변환해서 추가적인 주파수 해상도를 얻어낸다.

이 때 전송 레이트를 바꾸고 싶으면 높은 밴드 하나를 통째로 날려도 나머지 밴드만으로 소리가 만들어지는 것이 가능한데, 이런 scalability를 제공하는 것이 ATRAC의 장점.

현재는 ATRAC 3라는 포맷으로 진화를 하였고, ATRAC Lossless라는 별도 포맷도 만들어짐.

ATRAC의 Hybrid필터뱅크 구조는 나중에 MPEG-2 AAC 표준화 할 때, gain control(혹은 pre-processor)라는 tool로써 표준에 채택이 되었는데, 이 기술은 SSR(Scalable Sample Rate) 프로파일에서만 사용이 가능하고 현재 SSR 프로파일 AAC는 아무도 사용하지 않기 때문에 죽은 기술이 되어버림. 그런 이유로 그림에서는 ATRAC로부터 AAC로 화살표가 이어짐



## 다시 MPEG, MPEG-2

MPEG-01을 완성해놓고 보니, 당시 시대의 요구사항은 HDTV와 DVD였다. CD에다 동영상을 넣어서 V-CD를 만들자는 생각은 이미 old fashion이 되어버린 것. 그래서 MPEG-2를 기획하게 된다. MPEG-2의 제목은 "Information technology -- Generic coding of moving pictures and associated audio information"이다. 앞서 MPEG-1에서의 1.5mbps어쩌구를 빼고 좀 더 generic하게 간것.

Video는 HD급 영상을 어떻게 압축하냐가 관건이고 , 오디오는 이제 5.1채널을 다뤄야한다는 것. 문제는 이미 1991년에 5.1채널 지원의 Dolby AC-3가 만들어져 있떤 시점. MPEG에서는 부랴부랴, 그리고 직전에 만든 MPEG-1을 카니발라이즈 하지 않는 표준을 원했고, 그래서 오디오는 MPEG-1에서 만든 3개 layer의 코덱을 그대로 들어다가 MPEG-2 도장을 찍어준다.

대신 스테레오만 지원하던 것을 5.1까지 지원해주고 확장된 표본화 주파수를 지원해주고 하는 것이 달라진 점 !

또한 MPEG-2는 하향 호환성(Backward Compatibility)을 보장하기 위해 5.1채널로 부호화된 비트열이더라도 그 안에 MPEG-1에 호환되는 스테레오 파트를 먼저 집어넣고, 나머지 3.1에 해당하는 부분을 MPEG-1로 보면 extension에 해당하는 신택스 공간에 밀어 넣는 방법을 사용한다. 물론 16, 22.05, 24kHz처럼 지원할 수 없는 것들은 포기.

그 밖에도 사실 몇 개의 다채널에서 압축을 잘할 수 있는 툴들을 포함시키긴 했는데, 다 사용되진 않음. 왜냐하면 MPEG-2가 비록 5.1을 지원하는 표준이긴 하지만 아무도 MPEG-2를 멀티채널로 서비스하지 않았기 때문.

MPEG-2가 유럽방식의 DTV표준인 DVB-T에 채택되어 사용되고 있는데 유럽에서는 방송으로 5.1을 안날렸음. 불행히도

여튼 그래서 MPEG-2라고는 하나 오디오는 사실상 MPEG-1과 달라진 것이 없다. MPEG-1 Layer-III나 MPEG-2 Layer-III나 MPEG-2.5 Layer-III나 다 같은 MP3인것이다. 이렇게 MPEG-2는 1994년에 완성이 된다.



## 집대성, MPEG-2 AAC

MPEG-2 표준이 완성된 이후, 체급이 같던 MPEG Layer-II와 AC-3의 시장 쟁탈전에서 슬슬 AC-3로 기우는 분위기가 연출

음질적으로 비교우위라는 입소문과 5.1채널 지원, 헐리우드와의 커넥션까지 Dolby가 가진 비교우위가 막강했던 것. DVD나 ATSC의 예처럼 비디오는 MPEG-2를 수용하면서도 오디오는 사제코덱이랄 수 있는 AC-3를 채택해버리는것이죠.

DVD가 AC-를 사용한다는 것은 시장에 돌아다니는 컨텐츠가 모두 AC-3라는 얘기가 되어버려 사실상 경쟁이 어려워지는 것. Ray Dolby(돌비 창업자)는 공돌이로도 비즈맨으로도 참 영리한 사람이었던 것 같음. 여튼 그런상황에서 MPEG 오디오쟁이들은 다시 고민을 한다.

지난 MPEG-2가 급하게, 그것도 MPEG-1과의 호환성을 고려하여 만들다보니, 더 좋은 기술들이 있음에도 불구하고 채택이 되지 못했다. 그러니 새로이 하향 호환성을 포기하더라도 더 좋은 압축성능을 낼 수 있는 새로운 코덱을 표준화 해보자. 그래서 MPEG-2 NBC라는 프로젝트에 착수하게 된다.

이제 호환성을 포기했으니 세상에 좋다는 기술은 다 가져다 붙일수 있게 된다. 그림의 화살표에 나온것 처럼 기존 코덱들의 장점은 일단 다 끌어온다. 기본적으로 연산량 고려안한 기술 집약적 코덱이었던 MP3는 NBC에서도 기본뿌리 역할을 한다. 심리 음향 모델에 근거한 스케일팩터를 사용하여 비트를 할당하는 Analysis by Synthesis 구조에다가 3/4승의 비선형 양자화며, 세련되기 face lift했지만 여전히 뼈대는 그대로인 구조임. 대신 subband후 MDCT하는 비효율적 hybird구조를 빼버리고 2048point까지 MDCT의 블럭 사이즈를 확장하고 양자화된 MDCT 계수를 추가로 압축하는 Huffman coding에도 세련미를 가미.

하지만, 무엇보다 TNS(Ternporal Noise Shaping)라는 새로운 툴을 도입한 것이 NBC에서의 큰 변화로 보여짐.(TNS가 얼마 만큼 음질 향상 효과가 있을까를 떠나서, 기존 코덱이 가지고 있던 문제를 명확히 정의하고, 문제를 풀기위해 머리를 굴리고, 그러다가 남들이 생각하지 못했던 새로운 방법을 찾아내고, 이를 끝내 구현해내고... 우리 엔지니어들이 답습해야할 BEST PRACTICE)

TNS역시 MP3 개국 공신들이었던 JJ(AT&T)와 J.Herre(FhG)의 작품인데, 처음에 이 기술을 논문을 접하고 매우 놀람.

ATRAC에서 언급했던 4 band 서브밴드 필터뱅크 후 MDCT 계수로부터 예측하고 그 찌꺼기(residual) 신호만 코딩을 하겠다는 생각만 해도 엄청난 연산량의 코딩 툴도 이때 도입이 됨

그렇게 완성된 표준은 NBC(Non-Backward Compatible)라는 표준화 단계에서의 어색한 이름을 버리고 MPEG-2 AAC(Advanced Audio Coding)이라는 깔끔한 이름을 부여받는다. 표준번호로는 1381807(MPEG-2의 part7라는 뜻). 완성시점은 1997년.

AAC는 최대 48채널까지도 부호화가 가능하고, 표본화 주파수도 최대 192kHz까지 아무 샘플링이나 가능하고, 등등 기존에 MPEG-2 BC가 가지고 있던 신택스의 경직성도 최대한 풀어두었다.

이렇게 틀을 완성한 AAC가 현재(2010년)까지도 업계 전반에서 주력으로 사용하는 오디오 코덱의 마지막이다. 현재 표준화가 진행중인 USAC 이 그 바통을 이어받을지 모르지만, USAC안에도 AAC가 다 딸려들어가기 때문에 기술도 유행도 끝나지 않을 불멸의 코덱이 되는 셈.

...

기타 등등

![image-20200813153113141](https://user-images.githubusercontent.com/58545240/90101827-5c268b80-dd7a-11ea-8400-7ac2619a41f2.png)



![image-20200813153117074](https://user-images.githubusercontent.com/58545240/90101834-60eb3f80-dd7a-11ea-8c06-1e14c13c5599.png)

오디오코덱의 역사로 보자면 MPEG-1부터 MP3, PAC, MPEG-2, MPEG-4 AAC등등 꾸준히 발전해오고 있습니다.

사실 인간의 음성생성 모델을 기반으로 하고 있는 음성 부호화 기술과, 인간의 청각 모델을 기반으로 하고 있는 오디오 부호화 기술은 음성 통신과 음악 방송 등 각각 독립적인 서비스 영역에서 독자적으로 기술발전을 이루어왔습니다.

하지만 최근 방송과 통신이 융합되면서 더 이상 음성과 오디오 신호를 별도의 콘텐츠로 분리하는것이 어렵게 되었습니다.

그래서 하나의 부호화기로 음성과 오디오신호를 모두 처리할 수 있는 새로운 부호화 기술이 요구되면서 2012년에 MPEG-D USAC표준을 승인하게 됩니다.

USAC 표준화 이후 UHDTV 환경에서 다수의 스피커를 이용해 공간상의 임의의 위치에 음원을 위치시키기 위해 MPEG-H 3D Audio 표준을 진행하게 됩니다.

새로운 압축모듈을 개발하고 탑재하는 대신 USAC 기술을 활용하되 채널신호 압축효율을 높이기 위해 MPEG Surrond 기술도 접목하게 됩니다.

# **코덱(codec)이란 무엇일까?**

---

> Coder(코더) + decode(디코더)
>
> **Coder** (부호화기) => 영상신호를 디지털신호로(인코딩)
>
> **Decoder**(복호화) => 디지털신호를 영상으로(디코딩)
>
> 즉, 코덱이란 데이터(정보)를 관리(처리)하기 쉽도록 부호화시켰다가, 다시 데이터를 읽기 위해 복호화 하는 것을 의미한다.

코덱이란 영상장비에서 촬영된 영상을 전송하기에는 파일 사이즈가 너무 커지므로 용량이 큰 파일을 작게 만들어주는 과정이 꼭 필요한데 이것이 바로 `인코딩`이라고 합니다. 인코딩된 이미지는 그냥 볼 수 없고 본래대로 재생할 수 있게 압축을 푸는 기술인 `디코딩`을 거쳐야 합니다. 압축을하고 또 압축을 푸는 하나의 프로그램을 `코덱(codec)`이라고 합니다. (코덱마다 인코딩, 디코딩 알고리즘 자체가 다르다)

압축방식은 크게 2가지로 나눌 수 있다. MJPEG와 H.264방식이다. 2개의 코덱을 선택한 이유는 하나는 이미지 압축 방식(MJPEG)을 따르고 있고 다른 하나는 비디오 압축 방식을 따르고 있기 때문이다.

## 1. MJPEG

MJPEG는 이미지 압축방식이다.

![image-20200717100525005](https://user-images.githubusercontent.com/58545240/89979921-c4f30280-dcab-11ea-958e-8d06a968160e.png)

## 2. H.264

H.264는 비디오 압축 방식이다.

![image-20200717100558667](https://user-images.githubusercontent.com/58545240/89979934-ce7c6a80-dcab-11ea-9c18-0bb796acfbd0.png)

## 차이점

MJPEG의 경우 처음부터 끝까지 자동차와 배경을 꾸준히 담아내고 있는 반면, H.264 코덱의 경우 첫 번째만 배경을 담고 나머지는 자동차의 움직임만을 기록하는 방식이다. 그렇다면 어떤 차이점이 있을까?

- `MPEG` 
  - 압축된 데이터 크기는 다른 코덱에 비해 클 수 밖에 없다.
  - 프레임 손실이 있을 때 영상 손실이 적을 수 있다.

![image-20200717101016515](https://user-images.githubusercontent.com/58545240/89979972-db995980-dcab-11ea-8104-1d11fafcfe93.png)

- `H.264 `
  - 비디오를 구성하는 프레임간 차이 데이터만 압축하는 방식을 사용하고 있다. 차이만 압축을 하므로 압축되는 데이터 크기가 이미지 압축방식보다 작다. (배경이미지를 저장하지 않아 데이터 감소를 기대할 수 있다.)
  
  ![image-20200717100951900](https://user-images.githubusercontent.com/58545240/89979962-d3d9b500-dcab-11ea-8543-165c4d4274d7.png)
  
  - 프레임 간 연계성을 가지고 있어 프레임 손실이 있는 경우, 연계가 있는 프레임들은 재생을 할 수 없는 단점이 있다.

![image-20200717101028037](https://user-images.githubusercontent.com/58545240/89979981-e2c06780-dcab-11ea-9c47-eec5d386a674.png)

# **화질**

---

>  화질에 중요한 영향을 미치는 2가지 요소

## :black_nib: ​프레임레이트(Frame rate)

카메라에서 촬영된 영상은 한 장 한 장의 이미지로 구성되어 있으며 각각의 이미지를 프레임이라고 합니다.

프레임을 나타내는 단위는 fps(frame per second)이고, 1초 동안 재생되는 이미지 수를 의미합니다. 예를 들어 24프레임이라고 하면 초당 24장의 이미지가 있다는 의미. 프레임이 높을수록 자연스러운 영상을 얻을 수 있지만, 데이터 크기가 커지는 단점이 있다!

## :black_nib: ​비트레이트(Bit rate)

앞에서 다룬 프레임 레이트는 초당 이미지의 갯수라면 비트레이트는 bps(Bits Per Second)로 1초의 영상을 구성하는 데이터 크기라고 생각하면 된다.

아무래도 데이터 크기가 작은것 보다는 데이터 크기가 커진다면 화면을 표현함에 있어 유리할 것이다!

초당 192kbps vs 26Mbps영상 자료 비교 : 26Mbps로 인코딩한 영상은 뭉개짐이 없는 것을 볼 수 있음

만약 192kbps 인코딩된 영상을 26Mbps로 인코딩하면 안될까? 라는 질문을 할 수 도 있는데 화질이 뭉개지는 영상을 26Mbps로 인코딩한다고 해서 퀄리티가 높아지지 않는다. 요약하면 높은 화질을 나쁘게 만들 수는 있어도 낮은 화질을 좋게 만들 수는 없다. (원본의 소중함~)

# **해상도 그리고 인터레이스(Interlaced)와 프로그레시브(Progressive)**

---

해상도는 픽셀의 숫자를 말하는데 가로X세로를 곱한 것을 의미한다. 여기서 말하는 픽셀(화소)은 화면을 구성하는 가장 작은 단위를 말하며 하나의 점이라고 생각.

즉, 화소가 높다는 것은 이미지를 구성함에 있어 더 많은 픽셀로 표현한 것이다.

![image-20200717101311562](https://user-images.githubusercontent.com/58545240/89980156-4a76b280-dcac-11ea-88f4-fa1c37c5eb6b.png)

=> 왼쪽 이미지는 선명하게 보이는 반면 오른쪽 이미지는 흐릿



## :black_nib: ​해상도 규격

![image-20200717101748973](https://user-images.githubusercontent.com/58545240/89980174-53678400-dcac-11ea-938e-2f8ef896cde8.png)

| 구분        | 해상도        | 픽셀 수    | 포맷      |
| ----------- | ------------- | ---------- | --------- |
| SD          | 720 X 480     | 345,600    | 480p      |
| HD          | 1,280 X 720   | 921,600    | 720p      |
| FHD (Full)  | 1,920 X 1,080 | 2,073,600  | 1080p / i |
| QHD (Quad)  | 2.560 X 1,440 | 3,686,400  |           |
| UHD (Ultra) | 3,840 X 2,160 | 8,294,400  | 2160p     |
| 4K          | 4,096 X 2,160 | 8,847,360  | 2160p     |
| 8K          | 7,680 X 4,320 | 33,177,600 |           |

모바일과 같이 작은화면에서만 재생될 동영상을 만든다면, 'FHD' / 1080p, 720p, 혹은 그 이하도 큰 문제가 없다.

4K 등의 초고화질 영상 편집에는 어마어마한 4K 리소스가 필요한데 작은 화면에서만 재생될 영상에 이 많은 리소스들을 낭비할 필요는 없다.

***그렇다면 픽셀수가 많을수록 무조건 좋은기기인가요?***

대게 높은 수준의 해상도를 가지고 있는 기기는 좋은 기기가 맞다. 하지만, 높은 해상도를 가지더라도 화면의 크기가 상대적으로 더 크거나 시청 거리가 가까워지게 되면(눈으로 식별할 수 있는 픽셀의 수가 많아지면), 사용자들이 느끼는 영상의 품질은 떨어질 수 밖에 없다. 이와 관련된 사항을 지표로 표현한 것이 바로 `PPI(Pixel Per Inch)`이다.

## :black_nib: ​PPI (Pixel Per Inch) 

> 해상도를 결정하는 지표

PPI는 1인치당 표현되어 있는 픽셀의 개수를 의미한다. 화면의 크기와 픽셀의 개수가 모두 포함된 지표이기 때문에 PPI 지표가 높을 수록 좋은 해상도를 가진 기기라는 것을 알 수 있다.

그렇다고 모바일 기기의 300ppi가 TV나 컴퓨터의 300ppi와 같은 품질을 사용자에게 경험시켜주진 않는다. 약간의 눈과 디스플레이의 거리가 멀어질수록 인간이 식별할 수 있는 픽셀의 수는 상대적으로 줄어들기 때문에, TV나 컴퓨터같이 먼 시청거리를 가진 기기가 모바일 기기보다 작은 PPI로도 높은 품질 경험을 사용자에게 줄 수 있다. 반면, 모바일 기기의 시청 거리(20cm ~ 30cm)는 컴퓨터나 TV보다 월등히 짧기 때문에, 상대적으로 TV보다 높은 PPI를 가져야만 TV의 해상도와 동일한 품질로 느낄 수 있다.

- PPI를 구하는 공식

```bash
[가로픽셀]^2 X [세로픽셀]^2 = [대각선픽셀]^2
[대각선픽셀] / [화면인치](대각선 길이) : `PPI`
```

![image-20200717103219092](https://user-images.githubusercontent.com/58545240/89980185-595d6500-dcac-11ea-9b34-7ad611a0ecb0.png)

## :black_nib: ​인터레이스, 프로그레시브

TV광고나 카메라 광고를 보면 '1080p', '1080i'라고 써있는 것을 본적 있을 것이다.

여기서 말하는 'i'는 인터레이스, 'p'는 프로그레시브 스캔방식을 말하는데 이는 화면에 이미지를 '뿌려주는' 방식의 차이이다.

### 1. 인터레이스(Interlaced)

인터레이스 방식은 하나의 프레임에 수평 주사선을 1개 간격으로  뛰어 넘어 주사하는 방식이다. 홀수열과 짝수열을 주사한 것을 필드라고 불리며, 두 필드를 합쳐 하나의 완전한 영상을 만들어 완전한 하나의 프레임이 구성된다.

![image-20200717103548829](https://user-images.githubusercontent.com/58545240/89980200-5febdc80-dcac-11ea-94d0-47ca760e1a1d.png)

### 2. 프로그레시브(Progressive)

프로그레시브 스캔의 경우 아래 그림과 같이 완전한 한 장의 프레임 하나 하나를 연속적으로 보여주기 때문에 고화질 화면을 감상할 때 유리하다.

![image-20200717103704333](https://user-images.githubusercontent.com/58545240/89980209-65492700-dcac-11ea-9cb5-66fc716353d1.png)

프로그레시브 방식은 한 장의 프레임 하나하나를 연속적으로 보여 주기 때문에 화면이 흔들리거나 `Flicker`(깜빡임 현상)가 발생한다. 이러한 프로그레시브 스캔의 단점을 보완할 수 있는 방식이 인터레이스이다.

인터레이스 방식은 프로그래시브 방식에 비해 깜빡거림이 적고 비교적 안정된 영상을 얻을 수 있다는 장점이 있으나 한 화면을 두 번(홀수, 짝수)에 걸쳐 주사하기 때문에 고화질 정보를 전송하기에는 불리하다.

# **채널**

---

신호가 이동하는 경로를 채널(channel)이라고 하는데, 통신에서는 두 가지의 의미로 사용된다. 첫째로 각각의 신호들이 송수신되는 주파수대역 하나의 단위를 채널이라 부르고, 둘째로 신호가 전달되는 매질 특성 자체를 채널로 분류하기도 한다. 유선의 경우라면 케이블 자체를 채널이라 할 수 있고, 전자파를 이용한 무선통신에서는 공기중 자체가 채널이 된다. 실제로 통신에서 사용하는 채널은 매질 그 자체가 아니라 신호가 이동하고 있는 매질상에 존재하는 잡음(Noise) 성분에 따라 여러가지 채널로 분류된다.

즉 매질을 의미하는 채널은 잡음이라는 개념과 따로 생각할 수 없다.

ACOLADE의 Channel이라는 class에서 있는 모델들은 이러한 매질특성과 관련된 채널을 의미하며, 이러한 점에서 채널이 '특정한 잡음을 가진 신호전송로'라고 재정의될 수 있다.

공기중에는 수많은 종류의 잡음이 있으며, 이 잡음들은 모든 주파수에서 발생하기 때문에 이러한 잡음을 백색잡음(white noise)라고 한다. 주파수가 높은 전자파의 일종인 가시광선이 주파수에 따라 색상이 다르지만 여러 색광이 겹치면 하얀색이 되듯이, 여러 주파수의 잡음이 모인 것이라서 붙여진 의미이다. 그래서 이와는 반대로 특정 주파수에 집중된 잡음을 유색잡음(Color noice)라고 한다.

## :black_nib: ​AWGN

> Additive White Gaussian Noise

가장 일반적인 형태의 채널이며, 전 주파수 대역에서 고르게 잡음이 발생하는 채널이다. 특별한 주변요소 없이 자연상태 그대로의 랜덤한 잡음이다.

![image-20200717110352458](https://user-images.githubusercontent.com/58545240/89980225-6da16200-dcac-11ea-91b8-f460eb4827c6.png)

## :black_nib: ​페이딩

> ~multipath fading

주변의 사물들에 의해 다중반사되는 전자파들이 서로 합성되어 일어나느 종류의 간섭잡음을 의미한다.

TV의 경우 여러 곳에서 반사되어 들어온 전자파가 합성되면서 윗아자가 틀리게 합성되어 화면이 흐릿하게 겹쳐보이는 고스트(Ghost)현상이 대표적인 fading의 예

이러한 종류의 페이딩은 주파수 조건에 따라서 Reayleigh와 Rician채널 등으로 세분화되기도 한다. 이동통신 환경에서의 채널은 기본적으로 Rayleigh fading으로 모델링하는 경우가 많다.

## :black_nib: ​SNR

> Signal-to-Noise Ratio : 신호대 잡음비

채널에서 뿐만 아니라 SNR은 자체적으로 대단히 중요한 의미를 가지는 지표인데 말 그대로 통신시호와 잡음간의 전력비를 의미한다.

주로 decibel(dB) 단위로 사용하게 되는데 이것의 좋고 나쁨이 채널을 통과한 신호의 왜곡정도에 가장 주요한 역할을 하게 된다.

```bash
SNR = 10log신호전력(W)/잡음전력(W)
```

## :black_nib: ​수학적 모델

한 간단한 수학적인 채널 모델의 예는 다음과 같다.

```bash
y = hx + n
```

여기서 x는 송신신호 , h는 채널, n은 가산잡음 그리고 y는 수신 신호이다. 이 경우에 채널의 특성을 나타내는 신호대 잡음비는 다음과 같다.

![image-20200717110916843](https://user-images.githubusercontent.com/58545240/89980235-742fd980-dcac-11ea-8f8a-ece50b4ace83.png)

여기서 Px는 송신 신호인 x의 평균 크기이며 No는 가산 잡음인 n의 스펙트럴 덴시티이다. 이 채널의 최대 용량 (capacity)를 구해보면 다음과 같다.

![image-20200717111027739](https://user-images.githubusercontent.com/58545240/89980244-785bf700-dcac-11ea-935c-7e580fa77b23.png)

여기서 C의 단위는 BPS/Hz이다. 오류없이 보낼 수 있는 최대 데이터 전송률은 최대 용량인 C보다 작다는 것을 의미한다.

**유선 채널**

유선채널은 구리선에 의해 전기신호가 전달 되면서 생기는 특성을 나타낸다. 유선채널에서 h는 일반적으로 시간에 따라 크게 변하지 않아 결정된 한 값으로 표현

**무선 채널**

무선 채널은 공기전으로 전파 신호가 전달되면서 생기는 특성을 나타낸다. 무선채널에서 h는 일반적으로 시간에 따라 가변하는 랜덤한 값으로 표현된다. 따라서 무선채널에서 받은 신호의 신호대잡음비는 시간이 지마에 따라 가변적으로 변하게 된다.

# **Sampling Rate**

---

- 소리는 공기 중의 떨림이다.
- 그 공기 중의 미세한 움직임을 우리의 귀는 우리의 뇌가 이해할 수 있는 형태로 바꿔주는 것이다.
- 즉, 하나의 형태의 요소가 다른 형태로 바뀌는 것이다.
- 마찬가지로 우리가 소리를 디지털화 시키기 위해선, 연속적인 아날로그 소리를 컴퓨터가 이해할 수 있는 디지털 언어로 바꿔주어야 한다. 이 과정에서 바로 `Sampling` 이 등장하게 된다.

![image-20200813152618102](https://user-images.githubusercontent.com/58545240/90101293-698f4600-dd79-11ea-88da-5a8bc9080a1a.png)

```bash
`샘플링 레이트(Sampling rate)` : 1초에 몇개의 샘플을 추출할 것인지
`Bit Depth` : 한 개의 샘플이 얼마만큼의 크기를 가지는지
```

=> 잘개 쪼갤 수록 부드러운 곡선이 된다.

=> **Sampling rate가 높을 수록 아날로그와 유사한 디지털 값(고음질)을 얻을 수 있다.**



## :black_nib: ​나이퀴스트 이론과 기본적인 샘플링

- 스웨덴 출신의 미국에서 거주하던 엔지니어인 `Harold Nyquist`는 1918년부터 1924년까지 여러가지 연구를 하던 중, 1928년 세상에 처음으로 소리의 디지털 샘플링 이론을 내놓게 된다. 수학에 기반을 한 그 이론은 테스트 해볼 수 없는 당시의 현실적 여건에 부딪혀 증명을 하지 못하다가 20년이 지난 1948년 Claude E Sahnnon에 의하여 수학적 증명을 받게 된다.

  - 그리고 우리는 그 이론을 **`나이퀴스트 이론`** 혹은 **`섀년/나이퀴스트 이론`**이라고 부른다.

  -> **우리가 샘플(sample)하려는 소리의 가장 높은 주파수 보다(Highgst frequency) 2배 이상의 샘플링 속도(Sampling Rate)를 사용하면 정확하게 소리를 다시 만들어 낼 수 있다.(아날로그에서 디지털로, 디지털에서 아날로그로)**

- 즉 우리가 만약에 1kHz 주파수를 녹음하고 싶다면 최소 2kHz Sampling Rate를 사용해야 한다는 것.

- 사람의 가청 주파수는 20Hz부터 20kHz까지이다. 최소한 인체학적으로 귀를 통하여 들을 수 있는 소리의 가청 주파수이다. 즉, 사람이 듣기 위한 소리를 녹음하기 위해서는 40kHz sampling rate 이상을 사용하면 된다.

그러면 여기서 질문!

`44.1kHz`, `48kHz`, `88.2kHz`, `96kHz` 등 여러 다른 샘플링 수치는 무엇이냐?

샘플링을 "쪼개는 것"이라고 설명하는 것을 참 많이 볼 수 있다. 즉 44.1kHz는 1초에 44100번 소리르 쪼갠다는 것.

그렇다면 1초에 44100번 쪼개는 게 이득일까 아니면 96000번 쪼개는게 이득일까? 어느쪽이 원본에 더 가깝게 소리를 재탄생시킬까? 당연히 96000이겟죠?



다시 44.1kHz로 돌아와 보겠다. 

1Hz의 진동 주파수가 있다고 생각해보자. 1Hz란 1초에 진동을 1번 한다는 뜻이다.

1000Hz진동주파수가 있다. 1초에 1000번 진동한다는 뜻이다.

만약에 `Sampling Rate`이 1초에 44100번 "쪼개는" 것이라면 1Hz를 더 많이 쪼갤까 1000Hz를 더 많이 쪼갤까

1초라는 시간안에 1000Hz보다 1Hz가 진동이 더 작으니 1Hz를 훨씬 많이 쪼개게 되겠죠.

그러면 1Hz가 1000Hz보다 더 원본에 가깝게 표현이 될 것이다. 왜냐면 1초에 44100번 쪼개니, 1Hz는 44100번 쪼갤 수 있고 1000Hz는 44.1번밖에 못 쪼개기때문.



**하지만 다행이도, "쪼갠다"라는 표현은 틀린 것이다.**

적어도 나이퀴스트 이론에는 맞지 않다.



나이퀴스트 이론을 다시 보자면 

**"우리가 샘플 하려는 소리의 가장 높은 주파수보다 2배 이상의 샘플링 속도를 사용하면 정확하게 소리를 다시 만들어낼 수 있다"**

즉, 몇번을 쪼개냐가 아니라 2배 이상이면 정확하게 다시 소리를 만들어 낼 수 있는 것이다.

1Hz와 1000Hz 둘 다 정확하게 말이다.



그러니 만약에 누군가가 "나는 가청 주파수 안의 소리를 더 **정확하게** 담고 싶어서 높은 Sampling Rate를 선호해"라고 한다면 틀린 말이 될것이다.

우리의 가청 주파수는 20kHz까지라고 했다. 그리고 40kHz의 sampling rate이면 가청 주파수 이내의 모든 소리를 담을 수 있다.

그런데 왜? 디지털 미디엄의 시작인 CD는 **`44.1kHz`** 일까? 40kHz여야하는 것이 아닐까?(20kHz * 2)



나이퀴스트의 이론이 적용되기 위해선 우리가 샘플 하려는 소리가 `Band-limited`되야 한다.

무슨말이냐면, 세상에는 우리가 듣지 못하는 소리도 있다.(가청 주파수 위/아래).

즉, 우리가 듣지 못할 뿐이지공기 중의 소리는 존재한다. 우리가 녹음하려는 가청 주파수 안의 소리를 샘플하기 위해선, 우리가 샘플 하려는 주파수보다 높은 주파수들은 샘플하지 않아야 한다.



그러기 위해서는 `Anti-Aliasing Filter`라는 쉽게 말해서는 **`Low-Pass-Filter`** 를 걸어 주어야 한다.

왜냐면 우리가 샘플을 하기 위해선 샘플 하려는 신호보다 2배 빠르게 샘플해야 하는데 너무 높은 주파수들이 샘플되기 시작하면 `Aliasing`이 시작되기 때문이다.

그 말은 원래 존재하지 않던 소리들이 생겨나면서 소리가 정확하지 않게 되는 것이다. ( 소리에 에러가 생기기 시작한 것! )

이것 때문에 우리가 정한 `Sampling Rate` 이상의 소리는 아예 차단해버리는 `Anti-Aliasing Filter`를 만들어야 한다. 그러나 우리가 생각하는 것처럼 이상적인 필터(Brick Wall Filter)는 만들기가 힘이 든다.

![image-20200811134657471](https://user-images.githubusercontent.com/58545240/89980418-cffa6280-dcac-11ea-8935-55cab9e7d9fc.png)

그래서 천천히 줄어드는 필터를 만들게 되고 그 필터의 특성상 22.05Hz까지 뻗어나가게 되기 때문에**`44.1kHz`**라는 `Sampling-Rate`이 CD에 채택된 것이다.

![image-20200811134803164](https://user-images.githubusercontent.com/58545240/89980435-d688da00-dcac-11ea-8177-f8034a98e879.png)

## :black_nib: ​44.1kHz, 충분한가?

높은 `Sampling Rate`가 낮은 , 주로 `44.1kHz`보다 좋다, 아니다 라는 논쟁은 수십년동안 지속되고 있다.

나이퀴스트 이론은 어디까지나 이론이다. 그것을 완벽하게 현실화 하기 위해서는 이론을 뒷받침할 기술이 필요하다.

1983년도에야 되서 제대로 된 디지털 `44.1kHz, 16bit` 기술이 나온걸 감안하면 아주 오랜 시간이 걸렸다. 거대한 마케팅에 비해 현실적으론 문제 투성이었고 많은 사람들에게 거부감을 심어주었다.

*- 그렇다면 도대체 그 문제는 무엇이었는가?*

1. **클락의 부정확함**

   - 바로 `digital clock`을 이야기 한다. 정확하지 않은 클럭은 `Jitter`가 많이 발생하며 그것은 고주파의 `distortion`과 비슷한 왜곡된 소리를 내어 준다.

2. **16 bit**

   - 16bit. 대부분의 컨버터가 `24bit`인 지금 `16bit`밖에 수용할 수 없던 기술은 높은 **`Signal to noise ratio`**, **`quantization error`**등 많은 문제가 있었다.
   - "무조건 크게 녹음해야 한다"라는 녹음 기술이 이때 나오게 된것

3. **아날로그 필터의 문제**

   - 이것이 우리가 깊게 파볼 부분이다.

   - 나이퀴스트 이론을 실행하기 위해선 우리가 샘플링하고자 하는 가장 높은 주파수의 두 배 이상은 `sampling rate`를 설정해야 하며, 그 신호는 `band-limited` 즉, 필터를 사용하여 원하는 주파수 이상의 소리가 들어오지 않도록 차단해주어야 한다. 우리가 마이크로 소리를 녹음할 때, 마이크를 통하여 전달되는 소리가 디지털로 샘플링되기 전에 아날로그 필터(`Low-Pass`)를 거쳐야 한다는 뜻이다.

   - 그러면 이렇게 생각할 수 있겠다.

     *"20Hz부터 20kHz까지만 샘플링 할 예정이니 20kHz에 로패스필터를 걸어져면 되겠네? 끝!"*

     - 아쉽게도 아날로그 필터는 이렇게 간단하지 않다.

# **Bit Depth**

Bit Depth를 이해하기 가장 쉬운 방법은 **"Bit Depth는 오디오의 음량 총량을 담는 컨테이너다"**라고 생각하는 것이다.

Bit Depth는 역시 헤비록, EDM이 만들어지는 홈스튜디오라면 크게 중요하지 않을 수 있다. 하지만 만약 적절한 음량에서 듣고 녹음하는 경우라면, 전체 프로덕션 과정에서 Bit Depth의 차이는 크게 느껴질 수 있을 것이다. 적절한 Gain 스테이징과 모니터링 레벨에서는 이 차이는 더욱 크게 느껴진다.

![image-20200820104249503](https://user-images.githubusercontent.com/58545240/91411197-f4486880-e882-11ea-99dd-88b171d959cb.png)



Bit Depth는 Recording 하려는 음원이나 Audio File의 Line Level(0dB)부터, Noise Floor까지에 이르는 Dynamic Range의 거리를 조절해준다.

- **Line Level**은 Recording 또는 Mixing 시에 웬만하면 넘어가선 안되는 음량의 마지노선이다. Audio가 `0dB`을 넘어가는 것은, 시스템이 받아들일 수 있는 최대 Volume을 넘어갔다는 것으로 Logic Pro X 는 Volume이 넘어간 만큼 Audio를 찌그러트려 강제로 `0dB`로 낮춰버리므로 음질에 손실을 가져온다.
- **Noise Floor**란 받아들일 수 있는 가장 작은 Volume을 의미한다.
  - **Bit Depth를 크게 설정해줄수록 이 Noise Floor가 내려가게 된다.**
- **Dynamic Range**란 Line Level과  Noise Floor의 사이를 일컫는다.

![image-20200820104526216](https://user-images.githubusercontent.com/58545240/91411219-fad6e000-e882-11ea-8c81-13a4ca72c97a.png)

**Bit Depth를 크게 설정해 줄수록 Noise Floor가 내려간다**고 했는데, 이 말은 즉 Dynamic Range의 폭이 커진다는 얘기도 딘다.

낼 수 있는 가장 큰 Volume부터 가장 작은 Volume의 폭이 늘어나면 표현할 수 있는 볼륨의 변화(Dynamic)이 더 풍부해진다. 표현할 수 있는 Volume의 가짓수가 더 많아진다는 소리다.

=> 노래의 조용한 파트에서도 그냥 작은 소리와 더 작은 소리의 차이가 확연해지면서 더 좋은 표현을 해 줄 수 있다는 것이다.

또한 Mix, Mastering 작업에서의 Dynamic 작업에서의 편의성이 올라간다고도 한다.



실제 소리를 Recording할 때, `Sample Rate`가 음원을 세로로 잘게 쪼개는 거라면, `Bit Depth`는 가로로 쪼개는 것이다. `Bit Depth`가 높을 수록 더 많이 쪼개는 것. 소리가 점차적으로 커진다고 가정할 때, 낮은 `Bit Depth`에서는 우.우.웅!!! 하고 커진다면 **높은 `Bit Depth`**에서는 우우우우우웅!!하고 커진다. **더 실제에 가깝게 부드럽게 커지는 것**이다.

![image-20200820105020826](https://user-images.githubusercontent.com/58545240/91411229-ff9b9400-e882-11ea-9303-1ce67615f90a.png)

# **ADC와 DAC**

---

> - ADC : Analog to Digital Converter
> - DAC : Digital to Analog Converter

Sample Rate와 Bit Depth를 활용해서 **DAC**는 우리의 디지털데이터를 다시 아날로그 오디오로 변환합니다. 이것은 각 샘플 포인트를 보간법을 이용해서 전환시킵니다.(끝과 끝을 통해서 예측)

보통 176.4kHZ나 192kHZ와 같은 높은 Sapmle Rate 들이 샘플링 단계에서의 에러와 불일치함을 만들수 있습니다. 하지만 **다운샘플링**과정에서 보간하거나 재계산을 통해 이런 **에러 들을 보정**한다.  **때문에 이런 에러들보다 중요한 핵심은 캡쳐하는 순간의 해상도**이다. 

```bash
# Note
실제로 높은 Sample Rate에서 녹음되고 처리된 후 다운커버팅된 음원이 처음부터 낮은 Sample Rate에서 녹음되고 처리된 음원보다 더 좋다.
이는 사진작업으로 생각해보면 간단한데, 고해상도에서 편집한 뒤 저화질로 바뀐 사진이 처음부터 저해상도에서 작업된 사진보다 편집도 더 세세하게 가능하고 이에따라 저화질 상태에서도 결과가 더 좋을 것이다.
```



# **ADC**

---

> **Analog-to-Digital Converter**

## :black_nib: 데이터 변환기

- **A/D 변환**(Analog-to-Digital Conversion)
  - 연속성 아날로그 신호를 표본화, 양자화, 부호화을 거쳐 이진 디지털 신호로 변환시키는 과정
- **D/A 변환**(Digital-to-Analog Conversion)
  - A/D 변환의 역변환 과정

![image-20200904101228755](https://user-images.githubusercontent.com/58545240/92191816-aabdd600-ee9f-11ea-8633-5a18ba4fcad0.png)

## :black_nib: A/D 변환 및 D/A 변환 과정

- 핵심 이론 : 나이퀴스트의 **표본화정리**
- 전체 A/D 및 D/A 변환 구성

![image-20200904101315702](https://user-images.githubusercontent.com/58545240/92191831-af828a00-ee9f-11ea-9f50-7f81ba3675a7.png)

## :black_nib: A/D변환기 내부구성

- **샘플링** : *Digitizer, Sampler*
  - 시간축 방향에서 일정 간격으로 샘플을 추출하여 **이산신호**로 변환시키는 과정
- **양자화**
  - 샘플된 진폭치를 특정 대표값으로 바꾸는 과정
- **부호화**
  - 신호처리가 용이한 디지털 코드(Binary Code) 형태로 변환하는 과정(비트 할당)

![image-20200904101513407](https://user-images.githubusercontent.com/58545240/92191842-b7dac500-ee9f-11ea-93ce-abf66dc9d8b3.png)

## :black_nib: A/D 변환기 구분

- **클럭 동작 속도**에 따른 구분

  - 저속, 중속, 고속 ADC

- **샘플링율**에 따른 구분

  - `Nyquist-rete ADC`

    => 입력 신호의 최대주파스의 2배로 데이터 샘플링 수행

  - `Oversampling ADC` 또는 시그마 델타 ADC

    - **특징**
      - **나이퀴스트율** 보다 훨씬 높은 **샘플링율**로 **SNR**을 증가시킴
      - 양자화잡음을 신호 대역폭 밖으로 보내 이를 필터를 통해 제거해냄
      - 높은 해상도 구현이 가능

  *! Over Sampling이란 필요 나이퀴스트율보다 의도적으로 더 많은 샘플링을 하는 방식으로, 주파수영역에서 양자화잡음을 넓게 퍼지도록 만든다.*

- **해상도 또는 분해능(Resolution)**에 따른 구분
  - 저 해상도 수준 ADC : **6 비트** 이하의 양자화 비트 수를 갖는 ADC
  - 중 해상도 수준 ADC : **7, 8, 9 비트** 정도의 양자화 비트 수를 갖는 ADC
  - 고 해상도 수준 ADC : **10**비트 이상의 양자화 비트 수를 갖는 ADC
- **구조에 따른 구분**
  
  - 플래시 구조, 파이프라이닝 구조, SAR 구조, 폴딩 구조 등

# **DAC**

---

> **Digitl-to-Analog Converter**
>
> **디지털 이진 코드를 받아 아날로그 신호를 발생시키는 장치**
>
> ​	=> n 비트 디지털 입력 신호에 대해 디코더에서 2<sup>n</sup>개의 아날로그 전압 기준신호를 발생

## :black_nib: DAC 성능 표현

- 정정 석능 : DNL, INL, 단조증가성
- 동적 성능 : 글리치 에너지(Glitch Energy), SFDR(Supurious Free Dynamic Range), SNR(Signal to Noise Ratio), 유효 비트 수(Effiective Number of Bit)

# **분해능**

---

> **Resolving Power, Resolution Power,  분해능, 해상도**
>
> **아주 작은 차이를 분별해내는 기기의 능력**

*렌즈와 같은 광학계의 결상 능력에서 **해상도, 분해능, 해상력** 의미 차이*

- **해상도** : 동영상/이미지 등이 얼마나 자세하게 표현될 수 있는가의 척도

- **분해능** : 기기가 갖는 최대 능력 (가장 좋은 조건하에서의 해상력)

  - 흔히, 분해능은 파장에 따라 달라짐

  - 아주 짧은 파장을 쓰면 분해능을 높일 수 있다.

    ex) 전자현미경

- **해상력** : 주변여건에 따른 기기의 한계 분해능

## :black_nib: 분해능 척도

- 판별 가능 최소 눈금 변화량 = `(동작범위) / (눈금 수)`
  - 단위 눈금 당 신호 변화량
  - 백분율 분해능(% 분해능) = `[ (최소눈금단위 동작범위) / (전체 동작범위)] x 100%`

*!일반적으로 분해능 수치가 작을수록 성능이 좋아지며, 그만큼 기기 가격도 높아짐*

## :black_nib: 주요 기기별 분해능

> **작을 수록 좋은 분해능**

- **센서, 계측기** 분해능

  - 분리 식별이 가능한 아날로그 신호의 최소 변화량

    (주로 `백분율 분해능(%)`로 표현)

- **광학계** 해상도 또는 분해능

  - 사람의 눈, 현미경, 망원경 등 광학계가 확대 이미지의 가까이 있는 두 점 간을 구분할 수 있는 능력/서능

    (주로 `최소횡축거리`로 표현)

    *표현단위 : `um, nm, pm` 등*

  - **현미경** 분해능 : 파장이 짧을 수록, 렌즈의 크기가 커질수록 공간분해능이 좋아짐

- **눈** 해상력

  - 주로, 대상 물체에 대해 변별이 가능한 시야 각도

    (최소분리시각 1분(1')각 = 1.0 표준 시력)

    *표현단위 : 각도*

- **스펙트럼 분석기** 분해능 대역폭

  - 인접 주파수 성분들 간을 분리식별할 수 있는 능력

    *표현단위 : `Hz`*

- **레이더** 거리 분해능

  - 인접한 두 표적 사이의 최대 분리가능 거리

    *표현단위 : `m, km` 등*

# **FDMA**

---

> frequency division multiple access : 주파수분할 다중접속

통신망을 통해 전달되는 데이터중 가장 많은 양을 차지하는 것이 음성일 것이다. 사람의 음성을 전달하는 데 필요한 최소한의 주파수 대역폭은 300 ~ 3400 Hz로 알려져 있는데 이러한 음성 신호를 전기적인 신호로 변환하여, 통신망을 통해서 다른 곳으로 전달하기 위해서는 음성 신호를 전송하기에 적합한 전기적인 신호로 바꾸어 주고, 수신하는 쪽에서 이를 다시 음성 신호로 역변환을 하게 된다.

통신망에서 사용하는 전송 시스템의 특성을 살펴보면 전송가능한 주파수 대역이 음성 대역폭 보다는 월등이 크고, 훨씬 높은 주파수까지도 전송할 수 있다. 이런 전송로는 단 한 사람의 통신을 위해서만 사용하는 것은 경제적으로나 시설 운용면에서 매우 좋지 않다. **따라서 현재 대개의 통신망에서는 하나의 전송로를 이용하여 동시에 여러 사람의 데이터를 보내기 위한 기술이 적용되어 있으며, 이를 *다중화 방법* 이라 한다.**

다중화 방법 중에 가장 쉽게 생각할 수 있는 방법이 전송로가 가지는 주파수 대역폭을 전송신호 대역폭 단위, 즉 채널로 분할하고, 각 신호를 서로 다른 채널로 전송하는 방법을 **주파수 분할 다중화**라고 한다.

만일 전송 신호가 디지털 신호이면, 각 채널을 사용하는 시간을 나누어서 다중화 하는 시분할 다중화 방법을 사용할 수 있다.

이를 이동통신 환경에 적용하여 보면, 각 가입자는 전자파를 이용하여, 기지국과 연결되므로 공기중을 전파하는 전자파가 전성로이고, 이 전송로 중에 사용 가능한 주파수 대역폭이 있으므로, 이 제한된 주파수를 사용하여 가급적 많은 사람이 동시에 통화할 수 있도록 하는 것이므로 기지국과 가입자의 접송 방법인 무선 접속 방식은 다중화 방법과 동일한 개념이라는 것을 알 수 있다.

**주파수 분할 무선 접속방식(FDMA)**는 주파수 분할 다중화 방법을 이동통신망의 기지국과 단말기 사이에 적용한 것으로, 사업자에 할당된 주파수를 통신에 필요한 최소한의 주파수 대역으로 잘게 나눈 뒤에 이를 각 가입자에게 할당하는 방식이다.

현재 `FDMA`방식에서 사용하는 통화채널의 대역폭은 `20kHz ~ 30kHz`정도 이다. 이는 실제 음성 대역폭 `4kHz`보다는 훨씬 큰 대역폭임을 알 수 있다. 이는 음성 신호를 그대로 사용하면, 무선 구간에서 음성 품질이 너무 떨어지기 때문에 음성 품질을 어느 정도 유지하기 위하여 음성신호를 먼저 잡음에 강한 FM 변조를 한 후에 FM 변조된 신호로 기지국과 단말기 사이의 통신을 한다.

이론적으로 FM변조는 광대역으로 변조할 수록 잡음에 강한 특성을 보인다. 그러나 음성 품질을 높이기 위해서 FM 변조 대역폭을 넓게 하면, 통화 채널이 줄어들기 때문에 음성 품질과 통화 채널 수를 적절히 조정하여 FM 변조 대역폭을 결정해야 한다.

- **FDMA**는 무선 셀룰러 통신에 할당된 주파수 대역을 30개의 채널로 분할한 것이며, 각 채널은 음성 대화나 디지털 데이터를 옮기는 서비스에 사용될 수 있다. **FDMA**는 북미에 가장 광범위하게 설치된 셀룰러폰 시스템인 아날로그 AMPS의 기본 기술이다.
- **FDMA**에서는 각 채널이 한번에 오직 단 한명의 사용자에게 할당될 수 있다.
- **FDMA**는 TACS(Total Access Communication System)에서도 역시 사용될 수 있다.
- `D-AMPS(Digital-Advanced Mobile Phone Service)` 역시 **FDMA**를 사용하지만, 시분할 다중접속 기술을 가미함으로써 각 FDMA 채널당 3개의 채널을 가질 수 있어, 종전에 비해 채널당 3배 더 많은 통화량을 감당할 수 있게 되었다.

# **DMA 모드**

---

> - Direct Memory Access
> - I/O로 인한 성능 감소 방지를 위해 CPU 개입 없이 I/O 장치와 기억장치 간 직접 데이터 전송 방식

처리기를 거치지 않고 DMA컨트롤러에 의해 주변기기로부터 데이터가 메모리로 직접 전송되는 것을 뜻한다. 이 때 각 주변기기는 DMA 컨트롤러 칩의 지시에 따라 데이터를 주고받게 되며 DMA가 지원되지 않는 보드나 주변기기에서 이 부분을 체크하게 되면 컴퓨터가 작동을 멈추는 수가 있으므로 확인하고 사용하는 것이 좋다.

| DMA I/O                                        | Direct I/O(DMA 미사용)                             |
| ---------------------------------------------- | -------------------------------------------------- |
| DMA 기반 Data 처리 중 CPU는 다른 프로세스 처리 | Data I/O 처리 시 완료까지 작업 중지, CPU 대기 필요 |

## DMA 구성도/구성요소

![image-20200819162114280](https://user-images.githubusercontent.com/58545240/90611745-1a965480-e242-11ea-8b73-1514024b5482.png)

- DMA 전송을 위한 중앙처리장치 버스 신호와 레지스터로 구성

## DMA 동작 모드

| 구분                         | 개념도                                                       | 설명                                                         |
| ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Cycle Stealing<br />(Word)   | ![image-20200819162310604](https://user-images.githubusercontent.com/58545240/90611763-1f5b0880-e242-11ea-86fb-e6e85bb22769.png) | - Word 단위 데이터 전송<br />- DMAC와 CPU 동시 BUS 요청 시 DMA우선권 |
| Burtst Transfer<br />(Block) | ![image-20200819162324048](https://user-images.githubusercontent.com/58545240/90611785-26821680-e242-11ea-9505-810b297c0b7c.png) | - Block 단위 데이터 전송<br />- 여러 메모리 워드로 구성<br />- Block 전송 시 BUS 독점 |

## DMA 동작 순서

![image-20200819162540341](https://user-images.githubusercontent.com/58545240/90611803-2b46ca80-e242-11ea-8816-27933a68dd33.png)

## DMA 주요 구성요소

|   구성   | 구성요소                                    | 설명                                                        |
| :------: | ------------------------------------------- | ----------------------------------------------------------- |
|  명령어  | - DMA Select(DS)<br />- Register Select(RS) | - 주소 버스로 DS, RS enable 하여 DMA Register 선택          |
|  명령어  | - BUS Request(BR)                           | - DMAC를 통해 버스 요청<br />- add/data 버스 제어권 양도    |
|  명령어  | - BUS Grant(BG)                             | - BR에 따른 상태 변경 후 DMA BUS 사용 가능 알림             |
| 레지스터 | - Address Register                          | - 메모리의 접근 주소 저장<br />- Work 전송 후  Increment    |
| 레지스터 | - Word Count Register                       | - 전송할 블록 Word 수 저장<br />- Work 전송 후 Decrement    |
| 레지스터 | - Control Register                          | - 레지스터 동작 모드 설정<br />- Burst Mode, Cycle Stealing |

## DMA와 채널제어 방식 비교

|   항목    |                   DMA                   |                  채널제어                  |
| :-------: | :-------------------------------------: | :----------------------------------------: |
| 처리 단위 | – 하나의 Instruction에 하나의 Block I/O | – 하나의 Instruction에 여러 Block I/O 처리 |
|  메모리   |             – 기억장치 필요             |    – 로컬 메모리 기반 데이터 블록 저장     |
|   활용    |      – 소형 컴퓨터 – PC, 태블릿 등      |     – 대형 컴퓨터<br />– 대용량 서버**     |