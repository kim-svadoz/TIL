# LINUX 커널 :black_nib:

---

>리눅스 커널을 한 사람이 전부 아는 것은 불가능하다. 커널 관련 두터운원서를 75~80권 정도는 읽어야 "아, 한 번씩은 훑어봤다"라고 말할 수 있을 정도다. `IBM`같은 대형회사에서도 리눅스를 다루는 사람만 250명 정도 있다고 한다. 250명의 사람이 방대한 커널에서 각자 분야를 맡아서 일을 처리한다.
>
>따라서 "모든걸 다 알아야지~!"라는 마음가짐 보다는 **"커널과 운영체제가 어떤식으로 동작하는지 개략적으로 이해해보자"**라는 수준의 마음가짐으로 임할것이다.

---

# 1. 운영체제란?

운영체제(Operating System)이란 하드웨어 자원들(cpu, memory, disk, tty)을 관리하고 프로그램들을 지원(support)해주는 것이다. 

(아래 그림을 살펴보면 Operating System의 아래에 하드웨어들이 있고 그 위로는 프로그램들이 있다.)

![image-20200806143341661](https://user-images.githubusercontent.com/58545240/90211904-05c65500-de2d-11ea-8d7a-b2c26e1b2c12.png)

다르게 표현하면, **하드웨어를 감추고 겉으로 다른 프로그램들을 지원해준다**라고 생각할 수 있다.

하드웨어를 감춘다는 건, 프로그램을 사용하는 사람이 편하게 쓸 수 있게 각종 기반 작업을 지원한다는 것으로 이해할 수 있다.

```bash
# Note
일반적으로 우리는 파워포인트나 워드를 쓸 때 프로그램이 cpu와 memory와 어떻게 소통하는지 등에 대해 따로 신경쓰지 않는다.
이는 다 운영체제 덕분이다.
```

---

## 1.1 프로그램이란?

코딩을 해봤다면 `main()`함수의 존재에 대해서 알 것이다. 프로그램이란 `main()`함수를 포함하여 다른 다양한 기능들을 하는 함수들이 모인 존재라고 생각할 수있다. 함수들이 적혀 있는 소스코드 파일을 컴파일 하면 프로그램이 된다는 것을 우리는 알고 있다.

`*.c`파일을 컴파일해서 `a.out` 혹은 `*.exe`등이 생성되고 우리는 이걸 프로그램이라 부른다.

여기서 한 가지 생각해볼 점이 있다. **왜 대부분의 프로그램은 분리되어 있는가?**에 대한 점이다. Microsoft사를 예로 들어보자. Microsoft사는 Word, PowerPoint 등 많은 프로그램을 보유하고 있다. Office관련 프로그램을 통틀어 우리는 Microsoft Office라고 부르기도 하는데, 왜 Microsoft사에서는 **왜 하나의 Office프로그램이 아니라 여러 프로그램(Word, PowerPoint)으로 분할해놨을까?**

사업적인 목적일 수 있겠으나 본질적으로는 **하나의 커다란 프로그램으로 운영할 경우 발생하는 비효율성 때문**이다. 거대한 프로그램은 실행할 때 부팅 시간도 오래 걸리고 메모리 사용에 있어서도 심각한 비효율성을 초래한다. 이런 여러가지 불편한 점이 있기에 여러가지 프로그램으로 분할해 놓은 것이다.

위와같은 이유로 리눅스 운영체제 또한 **Kernel, Shell, Utility 등 여러가지 프로그램으로 나뉘어져 있다.**

---

## 1.2 커널(Kernel)이란?

`Kernel`은 본질적으로 프로그램이다. 우리가 흔히 아는 `main()`으로 시작하는 프로그램 말이다.

하지만 다른 모든 프로그램과는 다르게 커널만이 가지고 있는 특별한 점이 있다. **그것은 바로 'Memory Resident'라는 점이다.** 메모리에 항상 상주해있는 것이 바로 커널이다.

![image-20200806144052100](https://user-images.githubusercontent.com/58545240/90211909-0c54cc80-de2d-11ea-9086-63628e64fb0d.png)

커널이 아닌 다른 프로그램들은 메모리에 있어도 되고 없어도 된다. **'Disk Resident'**라고 표현한다. 필요할때마다 그 때 그 때 메모리에 로딩해서 사용하면 된다는 의미이다.

**커널은 'Memory Resident'특징을 제외하곤 아주 평범한 C Program이다.** 커널을제외한 다른 프로그램들을 우리는 `Utility`라고 하는데 위에서 언급했듯 disk resident하다. 항상 현 주소가 disk라는 의미다. 유저가 필요할 때 요청을 하면 그 때 메모리에 올라오는(로딩되는) 것이다. 그런 의미에서 **Utility**를 우리는 **Command**라고도 칭한다. **Utility와 Command를 동의어로 생각하고 공부**해보자.

---

## 1.3 쉘(Shell)이란?

우리 디스크에는 수십 수백개의 프로그램들이 존재한다. **이 프로그램들이 언제 메모리에 로딩되고 언제 메모리에서 해제되는지 누가 관리해줄까?** 관리해주는 프로그램이 꼭 필요하지 않을까?

위와 같은 필요에 의해 탄생한 것이 쉘이다. **많은 프로그램들의 메모리 교통정리를 해주는 역할을 한다.** 유틸리티 중 하나로 쉘의 1차적인 임무는 **'Job Control'**이다. `Utility`, `Command`, `Job`을 동의어로 생각하고 공부해보자.

---

## 1.4 파일(file)이란?(유닉스 한정)

유닉스(Unix)에서 파일은 **sequence of bytes**를 의미한다. 말 그대로 바이트들의 배열이란 뜻이다. 모든 함수, 명령어들은 결국 기계어로 해석하면 0과 1의 나열에 불과하다.

특히 유닉스, 리눅스에서는 **I/O device**도 file로 취급한다. 당장 이해가 안되겠지만, 입출력 기계들(하드디스크, USB, 키보드 등)을 파일로 취급한다고 알고있어라.

```bash
# Note
유닉스, 리눅스에서 입출력 기계들은 `/dev/hd0`, `/dev/hd1`, `/dev/tty2` 등 파일로서 다뤄지고 입출력 기계와 1:1로 대응된다.
```

---

## 1.5 커널과 쉘, 그리고 유틸리티의 관계

아래의 이미지를 살펴보면, 좌측에 메모리 그리고 우측에 디스크가 있다. 커널과 쉘, 그리고 유틸리티들이 디스크와 메모리에서 어떻게 작동하는지 살펴보자.

![image-20200806144925170](https://user-images.githubusercontent.com/58545240/90211922-11b21700-de2d-11ea-9473-5599df267ffd.png)

맨 처음 시스템을 부팅하면 제일 먼저 메인 메모리에 `Kernel(a.out)`이 올라온다. 커널 실행파일이 메모리에 로딩된다는 말이다. 리눅스는 **멀티 유저 시스템으로 하나의 시스템에 다양한 유저가 들어온다**는 사실을 상기하고 아래의 그림을 살펴보자.

![image-20200806145025885](https://user-images.githubusercontent.com/58545240/90211933-1aa2e880-de2d-11ea-9dd2-02a9643ed66c.png)

유저가 터미널에 전원을 키면 그 터미널 위에서 쉘(shell)이란 프로그램을 메인 메모리에 올라온다. 그 후 쉘은 유저가 키보드로 커맨드를 입력하기 기다린다. 유저가 커맨드를 입력하면, 쉘은 커맨드에 대응하는 유틸리티를 디스크로부터 가져와서 실행시킨다. **각 유저로부터 전원이 들어올 때마다 이 3개의 프로그램(커널, 쉘, 유틸리티)의 관계가 형성됨을 알 수 있다.**

그런데 키보드로 커맨드를 입력하길 기다리고 그에 대응하는 유틸리티를 디스크로부터 가져와 실행시킨다는 것이 무슨말일까? 이 말의 의미를 이해하기 위해서는 **리눅스가 멀티유저 시스템이라는 점을 상기해야한다.** 더 자세한 설명 고고

위 내용들을 다시한번 정리하면 이렇다.

- 커널은 운영체제이며 항상 메모리에 상주해 있다. 나머지 프로그램들은 전부 유틸리티이며 디스크에 상주한다.
- 유틸리티는 항상 디스크에 있다가 필요할 때마다 메모리에 올라오고 사용하지 않을 때는 다시 내려간다. 유틸리티를 우리는 커맨드라고도 칭하며 커맨드들이 교통정리를 하는 것이 쉘의 역할이다
- 쉘한테 우리가 ppt라고 커맨드를 입력하면 쉘은 `child process`로 ppt를 생성한다. `child process`도 추후에 나오는 개념이니 일단 넘어가자

---

## 1.6 터미널은 뭐고 콘솔은 뭘까?

> 이 파트는 *KLDP의 게시물*을 보고 참조하여 작성하였다.

콘솔은 전통적으로 보면 계기판과 입력장치의 모음과 비슷한 것들의 집합으로 컴퓨터를 조작하기 위한 조작부라고 생각하면 된다.

옛날 대형컴퓨터는 `serial(rs232, 422, 485)`를 이용하여 터미널이라는 장치와 연결하여 조작하였고, 아직도 몇몇 은행에서는 이러한 장비들을 볼 수 있다. 터미널이라는 장비는 CRT와 키보드로 구성되어 있으며 Teraterm, 넷텀, 하이퍼터미널과 같은 프로그램들을 모두 '터미널 에뮬레이터'라고 불렸다.

이들은 일반 PC를 터미널 대용으로 사용할 수 있도록 해주는 역할을 한다. 

**이제부터 `터미널`을 통해 `콘솔`과 통신한다 라고 생각해보자**

이해를 쉽게하기 위해 터미널은 콘솔의 부분집합이라고 생각하는 것도 좋다. 아래 커널 컴파일 도움말에 기술된 내용을 보면, 리눅스에서 콘솔과 터미널은 거의 동일한 의미로 사용되고 있다고 볼 수 있기 때문이다.

# 2. 운영체제 비교, 리눅스 vs 윈도우

---

## 2.1 자원의 소모에 대하여

운영체제에는 종류가 굉장히 많지만 가장 대표적이라고 일컬어지는 리눅스와 윈도우를 한 번 비교해보려 한다. 윈도우는 개인 컴퓨터(Personal Computer)에 사용되는 운영체제다. 개인 컴퓨터에 쓰인다는 것은 나홀로 사용자라는 것, 즉 싱글유저 시스템이라는 의미다. 리눅스가 멀티유저 시스템으로 만들어진 것과는 상반된다.

싱글유저 시스템일 때는 보안문제에 대해서 신경을 별로 안 써도 되지만, **멀티유저 시스템일 때는 보안에 특히 신경을 써야 한다.** 이유는 간단하다. 멀티유저 시스템은 그 안에 **내 파일 뿐만 아니라 다른 사람의 파일 또한 존재하기 때문이다.** 내가 다른 사람의 파일을 멋대로 읽고 쓸 수 있다면? 즉 삭제하고 변경할 수 있다면?이라는 물음을 던지면 쉽게 이해가 갈 것이다.

또하나의 이슈는 **메모리관리**이다. 멀티유저시스템에서는 한정된 자원을 모두가 효율적으로 이용하기 때문에 메모리 관리가 상당히 중요하다. 반면 윈도우와 같은 싱글유저 시스템에서는 멀티유저 시스템만큼 메모리관리에 신경을 덜 써도 된다.

> 싱글유저 시스템 : 내돈 내고 구입한 컴퓨터(장비)인데, 내가 아니면 누가써? 나만 쓸거야!

위의 한 문장이 싱글유저 시스템의 기본 철학이다. 그래서 PC(Personal Computer)는 일찌감치 윈도우 운영체제와 함께 결합된다. 윈도우는 GUI(Graphic User Interface)를 채택하고 있기 때문이다. 일반 사용자 입장에서 화면에 일일이 키보드로 명령어를 입력하는 것보다 마우스 몇 번 클릭해서 프로그램을 실행하고 관리하는 것이 훨씬 편하기 때문이다.

반면 **리눅스, 유닉스 등의 멀티유저시스템 CLI(Command Line Interface)를 채택**한다. 윈도우에서는 내가 사용할 수 있는 유틸리티(프로그램, 커맨드와 동의어)가 아이콘으로 보기 좋게 화면에 표시되는데, 리눅스는 `man`이라는 명령어로 내가 사용할 수 있는 커맨드가 무엇인지 알아내거나 사전에 알고 있어야 원활한 사용이 가능하다.

![image-20200806151357994](https://user-images.githubusercontent.com/58545240/90211939-20003300-de2d-11ea-861f-bd781d54e613.png)

**윈도우는 사용자에게 편리한 인터페이스를 제공하는 대신, 동일 작업 대비 훨씬 많은 자원을 요구한다.** 리눅스는 사용자에게 다소 불편한 인터페이스를 제공하는 대신, 화면에 표시되는 Char1(1단위당 1바이트)만큼의 자원을 쓰는 등 효율적인 자원 사용을 가능케한다.

---

## 2.2 보안 이슈에 대하여

윈도우와는 다르게 리눅스에서는 보안이 매우 중요하다. 위에서 언급한 상황을 다시 생각해보자 **만일 한 프로세스가 다른 프로세스의 정보를 함부로 I/O하려고 하면 어떨까? 즉, 다른 프로세스의 파일을 삭제하거나 내용을 바꿀 수 있다면?** 그것만큼 끔찍한 일도 없을 것이다. 어제 힘들게 작성한 보고서를 자신의 가장 친한 친구가 실수로 삭제했다고만 생각해도 화가 치미는데, 모르는사람이 그런다면..

이러한 보안문제를 해결하기 위해 리눅스에서는 사전방지(Prevent)하기 위해 많은 노력을 하게 된다. 사후처리가 아닌 사전방지(Prevent)하는 이유 또한 간단하다. 내 파일이 삭제된 이후에 복구하는 뻘짓보다는 사전에 그런 행위를 못하게 막는 것이 훨씬 효율적이고 철학적으로도 옳다.

우리가 시스템을 설계하는 입장이라고 생각해보자. 알다시피 한 시스템에서 CPU는 모든 프로그램이 공통적으로 사용하는 공용자원이다. **메모리에 존재하는 프로그램은 여러가지고 하나의 프로그램만이 한 번의 순간에 CPU를 온전히 차지한다.**

- **CPU는 한 순간에 하나의 연산밖에 못한다.**

즉, 한 순간에 CPU를 사용하는 건 오직 하나의 프로그램 뿐이다. 여기서 순간이란 건, 기술력에 따라서 달라지는데 현재는 1초에 40억번의 연산이 가능한 CPU들이 등장하고 있으니, 최근기술로서 한 순간을 해석해보자면 나노세컨드 정도는 될 것으로 생각된다.

위의 내용들을 바탕으로 아래의 그림을 샆펴보자.

```bash
# Note
1초에 40억번의 연산이 가능하다는 말은 1초에 CPU는 40억번의 순간을 함축하고 있다는 뜻이다. CPU는 1초에 40억번의 순간이 존재하고 그 순간마다 연산(모든 프로그램을 관리)하기 때문에 우리가 여러 프로그램들을 한꺼번에 켜놓고 작업해도 끊김없이 동시에 처리가 되는 것 처럼 보인다.
```

![image-20200806152422710](https://user-images.githubusercontent.com/58545240/90211946-24c4e700-de2d-11ea-954b-78b29d3995c7.png)

CPU가 Bob의 터미널과 연결된 쉘에게 양도되었다고 하자. 즉 시스템이 Bob의 터미널과 연결된 쉘에 CPU를 넘겨준 것이다. 쉘도 하나의 프로그램이므로, 쉘에 버그가 생겼다고 가정해보자. 본래 쉘은 Bob이 접근 가능한 디스크 영역, 메모리 영역에만 데이터를 읽고 써야하는데, 디스크의 Sector Address를 잘못 계산해서 Dan의 영역에 데이터를 썼다고 가정해보자.

즉 Dan이 저장해놓은 파일들이 다 엉망이 됐다는 뜻이다. 시스템은 이미 CPU를 Bob과 연결되어 있는 쉘에게 양도를 해놨고... 이 쉘은 잘못을 저질렀다. 해당 프로그램이 오류로 엉뚱한곳에 데이터를 읽거나 쓰는 상황이 발생했는데 어떻게 이러한 상황을 예방할 수 있을까?

![image-20200806152712710](https://user-images.githubusercontent.com/58545240/90211983-3a3a1100-de2d-11ea-816b-6fe968ba238b.png)

위 그림에도 써져 있지만 리눅스 시스템 설계자가 고안한 해답은 바로 **"I/O instruction 하지마!!"**이다. 만약 쉘을 포함한 다른 일반 프로그램이 I/O를 수행하는 순간 CPU를 사용권한을 바로 뺏긴다.

쉘(sh)입장에서는 한편으론 억울할 수 있다. **"나는 CPU를 사용할 수 있고, 내역할은 I/O를 하는건데 그럼 뭘 하라는거야?"**라고 쉘은 반박한다. 자신의 임무를 수행하려 했더니 억울하게 CPU를 뺏겨버리니 말이다.

그래서 리눅스 시스템 설계자는 **"그럼 니가 I/O할 때 커널에게 부탁을 해. I/O하게 해달라고"** 라고 쉘에게 말한다. 즉, 현재 리눅스 시스템의 작동은 **I/O Instruction을 할 때는 커널이 갖고 있는 function에 부탁을 하는방식**으로 되어있다. 이 부탁하는 과정을 `***System Call***`이라 한다.

커널은 해당 입출력 명령이 합법적인 것인지 검사한 후에 I/O를 대신 진행해준다. 이런 특별한 구조를 구축하기 위해 윈도우나 개인 컴퓨터에는 없는 개념을 하드웨어에 도입한다. 아래의 그림을 살펴보자.

![image-20200806153057063](https://user-images.githubusercontent.com/58545240/90211992-3efec500-de2d-11ea-9043-bcd00b84e08d.png)

바로 **CPU에 하나의 Binary Bit를 도입한 것**이다. 위 그림에 빨간색 네모칸이 바로 그 bit(비트)다. 이 bit를 우리는 **mode bit**라 칭한다. 비트라는건 알다시피 2개의 데이터(0과 1)만을 저장할 수 있다. 0은 유저모드를 의미하고 1은 커널모드를 뜻한다. 우선 우리가 메모리와 CPU에 대해서 알고 있는 개념을 다시 한번 정리해보자. 그림과 함께보자.

- CPU의 Control Unit 파트에 **PC는 Program Counter라고 하는 특수 레지스터다. PC에서는 이번에 수행해야 할 instruction(명령)의 주소**를 메모리로 보낸다. **MAR은 Memory Address Register**이며 **Address Bus**라고도 이해할 수 있는데 MAR이 읽어들인 주소에 **해당하는 데이터를 MBR(Memory Buffer Register)에 담아 IR(Instruction Register)에 보낸다.**
- IR로 들어온 **Instruction(명령)은 위 그림의 우측 하단과 같은 구조**를 지닌다. **op-code**에는 수행해야 할 명령이 적혀있고 그 옆에는 **operands**라고 하는 명령 인수들이 적혀 있다. i와 j 주소에 있는 값을 더하라는 의미 정도로 해석할 수 있다.

![image-20200806153516001](https://user-images.githubusercontent.com/58545240/90212001-458d3c80-de2d-11ea-8fe5-d2ac9d36c192.png)

- Op-Code를 읽어들여 명령어를 처리하는데, **i와 j에 해당하는 메모리주소에 담겨 있는 값을 알아내기 위해 또 다시 메모리에 접근해서 연산을 진행한다.**

### >> mode bit(모드 비트) 추가설명

CPU에는 한 바이너리 비트인 **모드 비트**라는 개념이 존재하고, **그것은 0 또는 1의값**을 갖는다. 커널 모드에서 동작하느냐 유저모드에서 동작하냐를 정해주는 이 바이너리 비트를 우리는 일단 0이면 유저모드, 1이면 커널모드로 동작한다고 생각해보자.

![image-20200806153729243](https://user-images.githubusercontent.com/58545240/90212014-49b95a00-de2d-11ea-83c2-4e53254c2fa7.png)

만약 모드 비트가 `커널모드(1)`로 되어있다면 **CPU는 어떠한 영역의 메모리라도 접근할 수 있다.**

커널모드가 아닌 `유저모드(0)`로 되어있다면 **모든 메모리에 접근(Access)는 불가능하고 자신의 Address Space만 접근가능하다.**

또한 `커널모드(1)`일 경우에는 **모든 instruction이나 op-code를 수행(execute)할 수 있지만**

`유저모드(0)`라면 **I/O instruction이나 special register accesses와 관련된 instruction은 금지**된다.

즉, **유저모드에서는 타인에게 큰 영향을 줄 수 있는 instruction은 모두 거부되는 것이다.**

```bash
# Note
`special register accesses`란 스택 포인터(SP)나 프로그램 카운터(PC)와 같은 특별한 레지스터에 대해 값을 읽는 등위 행위를 말한다.
```

![image-20200806154109939](https://user-images.githubusercontent.com/58545240/90212030-4e7e0e00-de2d-11ea-86a8-b3bd4a75dca9.png)

---

## 2.3 커널모드와 유저모드

인천 공항 보안 검색대를 생각해보자. 일반 사람들이라면 보안 검색대를 반드시 통과해야겠지만, 자신이 대통령이나 국가원수에 해당하는 중책이고 매우 바쁜일이 있다면 보안 검색대를 거치지 않고 바로 공항을 나설 수 있다.

CPU는 항상 메모리에서 address를 메모리에 건넨다. 원하는 instruction이 있다면 해당 instruction의 주소를 Program Counter가 메모리로 보낸다. 이 과정이 CPU가 메모리에게 **"이 instruction을 수행해야 하니 메모리에 관련된 코드를 나에게 보내다오"**라고 말하는 과정이며, instruction이 오면 그걸 실행(execute)하는 과정에서 또 관련 매개변수(operands)에 관련된 주소를 보내고 관련 정보를 받아온다.

이처럼 CPU는 계속 메모리에게 address를 보내는 작업을 진행하는 것인데, CPU가 메모리로 주소를 보낼 때는 그 당시 모드 비트가 어떤 것으로 되어 있느냐가 정말 중요하다. 먼저 아래의 그림을 살펴보자.

![image-20200806160700319](https://user-images.githubusercontent.com/58545240/90212037-52aa2b80-de2d-11ea-9742-c13a81686514.png)

위의 그림은 **빨간색 모드 비트가 유저모드였을 경우**를 상정하고 설명을 한다. 앞에서 인천 공항 검색대로 예시를 들었는데, 운영체제에 빗대서 다시 정리해보자. 먼저 첫 번째 보안 검색(address)를 진행한다.

CPU와 메모리 사이에는 **MMU(Memory Management Unit)**이라는 하드웨어가 존재한다. 이 MMU의 역할은 CPU로부터 메모리로 가는 **address를 조사**하는 것이다. 즉, CPU가 메모리에게 넘기는 address 정보가 올바른지 판명하는 것이다. **"CPU야. 너가 지금 보낸 주소가 너가 할 수 있는 접근 메모리 범위를 벗어나지는 않았니?"**라고 말한다.

첫 번째 검사를 무사히 통과하면 **Instruction Fetch**, 즉 instruction을 가져온다. 위에서 우리는 instruction,의 구조가 op-code, operands로 구성된다는 것을 확인했는데, 바로 이 **op-code를 보고** 아 이것이 덧셈이구나, 뺄셈이구나, 곱셈이구나 등을 확인하게 된다. 이게 바로 2번째 검사다.

명령어(op-code)를 확인해 봤는데, 만약 이것이 **privileged op-code(I/O와 같은 중요한 역할을 하는 실행)**을 시도하려 한다라고 판단되면 **그 순간 바로 CPU를 뺏겨버린다.**

이렇게 작동을 한다면 위에서 언급한 보안이슈를 만족할 수 있다. illegal access가 사전에 예방(prevent)이 되고 차단이 될 수 있다.

![image-20200806161142471](https://user-images.githubusercontent.com/58545240/90212045-576edf80-de2d-11ea-8499-e3e0bbe9a336.png)

정리하자면 모드 비트가 유저모드일 때, CPU가 접근하는 메모리 주소가 실행 중인 프로그램의 범위 밖이거나 I/O instruction등의 금지된 실행을 하려고 한다면 CPU를 운영체제로부터 박탈당한다. 반대로 커널모드였을 경우는 위에서 언급한 검증 절차를 전혀 밟지 않아도 된다.

```bash
# Note
커널모드는 어떠한 메모리 영역도 접근 가능하며 어떠한 연산도 시행할 수 있는 특권을 가지고 있다. `오직 커널만이.`
```

# 3. printf("Hello World!")의 진실

---

그러면 이제 우리가 한 가지 궁금한 점이 있다. 우리는 프로그램을 만들 때 소스코드에 입출력과 관련된 함수를 작성한다. `printf()`를 사용하거나 `get()`등 디스크에 접근해서 값을 읽어오거나 화면에 문자를 출력하는 함수를 사용한다. 우리가 작성한 코드와 프로그램은 유저모드에서 아무런 제약없이 사용할 수 있었는데, 왜 I/O가 금지되었다고 말을 하는 것일까?

정답은 **“소스코드에서만 그렇게 보인다”**이다. 소스코드에서는 개발자가 입출력을 관리하는 것처럼 보이지만, 소스코드를 컴파일 한 후에 바이너리 파일을 열어보면 I/O와 관련된 instruction은 전혀 존재하지 않는다.

![image-20200806161339721](https://user-images.githubusercontent.com/58545240/90212051-5b9afd00-de2d-11ea-9d51-1d5f744eb570.png)

위에서 언급했듯이 I/O를 하고 싶으면 **커널이 가지고 있는 function**을 호출하는 방법밖에는 없다. 즉 커널에 부탁을 해야한다는 말이다. 그 행위를 우리는 위에서 **시스템 콜(System call)**이라고 했었다.

그런데 의문점이 하나 있다. 함수를 호출한다는 건, 특정 기능을 호출한다는 건 내 프로그램 안에 있는 함수를 호출하는 개념인 것인데, 어떻게 다른 프로그램(커널)의 함수를 호출하지?

그 원리는 우리가 입출력 함수를 담은 소스코드를 컴파일 했을 때, 해당 부분(입출력 등)이 등장하면**’change CPU protection mode to Kernel’**명령어를 수행하는 것이다. CPU의 모드비트를 바꾸는 것이다.

소스파일에 **privileged instruction**이 등장하면 바이너리파일에서 **chmodk**로 변환된다는 건 이제 이해할 수 있을 것이다. 그렇다면 이제 하드웨어가 이 명령어를 어떻게 실행시켜가는지 이해해보자.

---

## 3.1 chmodk를 실행했을 때 하드웨어에서 벌어지는 일들

![image-20200806161542626](https://user-images.githubusercontent.com/58545240/90212090-6fdefa00-de2d-11ea-8f68-158be5b1ea75.png)

소스파일에 입출력 파트에 컴파일러가 컴파일 시에 **chmodk**를 넣어둔 후 발생하는 일은 첫 번째로 유저로부터 CPU를 뺏는다. 더 이상 유저모드에서 실행(run)할 수 없게 만드는 것이다. 그걸 우린 **“trap에 걸린다”**라고 한다.

*트랩은 인터럽트랑 비슷한 개념으로 글 최하단 부분에 자세하게 설명을 적어놓았다.*

트랩에 걸린 후에 트랩핸들러 루틴(트랩을 처리하는 루틴)으로 진입하는데, 해당 루틴은 커널 안에서 처리된다. **chmodk**명령어를 처리하기 직전에 우리는 시스템 콜과 관련된 parameter를 사전에 약속된 곳에 기록을 해둔다. 왜냐하면 이후 트랩이 명령어를 처리할 때 유저가 어떤 처리(write, read, open, close)를 하려 했는지를 알아야 하기 때문이다.

I/O와 관련된 디스크와 관련 섹터에**”A프로그램에 B작업을 요청합니다.”**라는 관련 parameter를 적어두는 것이다. 그럼 트랩핸들러가 그 정보를 확인한 후,**”아 너는 I/O를 하고 싶고 그 중에서도 read를 하고 싶구나”**처럼 확인을 하는 것이다.

이제 유저가 뭘 처리하고 싶어하는지 알았으니 그냥 진행하면 되는 것일까? 아니다.**유저가 해당 디스크나 메모리 영역에 권한(read, write, execute 중 하나)이 있는지도**확인해야한다.

```bash
# Note
커널 function은 라이브러리 function과 다르다. 라이브러리에 있는 함수를 우리가 호출할 때는, 해당 코드가 그대로 우리 소스코드 안에 Copy & Paste 되는반면 커널의 함수를 호출한다는 건 커널에게 부탁을 하고 커널이 해당 함수를 수행해주는 개념이다. 커널이 메모리에 항상 상주해 있어야 하는 이유 중 하나이기도 하다. 유저가 어떤 함수를 요구할지 모르기 때문에 항시 대기해야 하는 것이다.
```

![image-20200806161633797](https://user-images.githubusercontent.com/58545240/90212103-753c4480-de2d-11ea-9bea-5ee94d3c2b91.png)

위의 검증 과정들을 거친 후 read/write 등의 작업이 끝나면 트랩으로 돌아가고, 해당 트랩에서 다시 유저모드로 return된다. 그 때 비로소 유저는 자신이 처리한 작업이 제대로 완료됐는지를 확인할 수가 있다.

![image-20200806161656290](https://user-images.githubusercontent.com/58545240/90212110-79686200-de2d-11ea-9586-6642f37c5b16.png)

과정을 한 번 더 간략하게 도식화한 것이 위 그림이다. 소스파일을 컴파일해서 바이너리파일에 **chmodk**를 껴넣고, 그로 인해 **트랩이 발생**한다. 커널 안의 트랩 핸들러는 적절한 검증절차를 거친 후에 유저모드에서 요구했던 작업을 진행한다. 그 후 다시 유저모드로 돌아온다.

![image-20200806161714782](https://user-images.githubusercontent.com/58545240/90212117-7cfbe900-de2d-11ea-81f5-e04129e0ea1b.png)

커널모드와 유저모드 사이의 모드가 바뀌는 과정을 계속해서 반복한다. 프로그램 내에 더이상 커널에 요구할 것이 없을 때까지. 모든 프로그램은 다 유저모드로 run하다가 커널 모드로 run하는 걸 반복한다.

자, 그런데 run을 한다는 건 유저 모드 혹은 커널 모드 안에 있는 function을 사용한다는 것인데 function들을 계속 call 했다가 return하고 call 했다가 돌아오고 하는 과정을 반복한다는 것이다.

function에는 보통 local variable(지역 변수)들이 있기 마련이고, 해당 지역 변수들이 어디 저장되는지 생각해보자. **일단 이 지역변수들은 메모리에 언제부터 언제까지 존재할까?** 그렇다. 함수가 호출되고 리턴되기까지 존재한다. 따라서 미리 메모리에 담아두는 비효율적인 방법보다는 임시적으로 메모리에 담아뒀다가 삭제하기 용이한 자료구조를 택해야 하는데, 그것이 바로 **스택(Stack)**이다.

![image-20200806161745028](https://user-images.githubusercontent.com/58545240/90212136-85542400-de2d-11ea-991a-e51e3bff812b.png)

function이 호출되면 해당 function의 local variable(지역변수)들이 스택에 push(삽입)된다. 지역변수 뿐만 아니라 함수가 끝나고 돌아갈 주소(return address)등도 함께 push(삽입)된다. 어떤 프로그램이나 유저모드와 커널모드를 오가기를 반복한다는 점을 우리는 알고 있다.

**유저모드에서 유저만의 function들을 실행하고 리턴하기 위해서 유저모드에도 스택이 필요한 것이고, 커널모드도 마찬가지로 자신만의 function을 실행하고 리턴하기 때문에 스택이 필요하다.**

# 4. 중간 정리

---

![image-20200806161821757](https://user-images.githubusercontent.com/58545240/90212150-8c7b3200-de2d-11ea-992a-ecbeee8422a6.png)

결국 프로그램의 실행은 **유저모드와 커널모드를 Alternating하는 것**으로 볼 수 있다. 위 그림을 쭈욱 따라가면서 1강에서 했던 내용들을 다시 상기해보자. 유저모드가 자신만의 코드를 수행하다가 **커널에게 부탁할 일이 생기면 System call을 한다.**

그렇게 되면 커널모드로 진입하여 **Kernel a.out**이 진행되면서 커널이 일을 처리한 후 다시 유저모드로 돌아와서 작업을 진행한다. 이 과정은 프로그램이 종료할 때까지 반복된다.

마지막으로 인터페이스에 대한 언급으로 1강 강의노트를 마치고자 한다. 커맨드는 유틸리티의 또 다른 이름이고 유틸리티는**disk resident program**이다(function이 아니다). 운영체제의 interface를 살펴보면, **커맨드는 키보드 interface**고 **system call이나 library call은 function interface**이다.

리눅스 운영체제를 사용하게 된다면, **man**명령어를 정말 많이 사용하게 될텐데 각종 명령어를 모를 때 `man <command>`방식으로 원하는 명령에 대한 상세 정보를 확인할 수 있다. 명령어 우측 괄호 안에 있는 숫자가 해당 명령어가 **커맨드, 시스템 콜, 라이브러리 함수**인지를 분간해준다.

![image-20200806161850869](https://user-images.githubusercontent.com/58545240/90212155-94d36d00-de2d-11ea-876c-8094b21058de.png)

# 5. 인터럽트와 트랩

---

인터럽트는 시스템 내에서 하드웨어가 생성한 흐름 변경이다. 인터럽트 원인을 처리하기 위해 인터럽트 처리기가 사용된다. 제어는 인터럽트된 컨텍스트 및 명령으로 리턴된다. 트랩은 소프트웨어가 생성한 인터럽트다. 장치 폴링의 필요성을 없애기 위해 인터럽트를 사용하여 I/O의 완료를 알릴 수 있다. 트랩을 사용하여 운영 체제 루틴을 호출하거나 산술 오류를 포착 할 수 있다.

**인터럽트는 하드웨어 인터럽트**이며 **트랩은 소프트웨어 호출 인터럽트**이다. 하드웨어 인터럽트 발생은 일반적으로 다른 하드웨어 인터럽트를 비활성화하지만 트랩에는 해당되지 않는다. 트랩이 제공 될 때까지 하드웨어 인터럽트를 허용하지 않으려면 명시 적으로 인터럽트 플래그를 지워야한다. 일반적으로 컴퓨터의 인터럽트 플래그는 트랩이 아닌 (하드웨어) 인터럽트에 영향을준다. 즉,이 플래그를 지우더라도 트랩을 방지 할 수는 없다. 트랩과 달리 인터럽트는 CPU의 이전 상태를 유지해야한다.

**트랩은 일반적으로 소프트웨어 인터럽트 라고하는 특별한 종류의 인터럽트**다. 인터럽트는 하드웨어 인터럽트(하드웨어 장치의 인터럽트)와 소프트웨어 인터럽트(트랩)를 모두 포괄하는보다 일반적인 용어다.

---

> 지금부터는 1장에서 간단하게 배운 **System Call**에 대해 더 자세히 다룰 것 이다. **System Call**이란 멀티유저 시스템에서 한 프로세스가 다른 프로세스에 I/O로 함부로 접근해 데이터를 망치는 일을 사전방지(Prevent)하기 위해 나온 방법이다. 정리해보면 쉘이 I/O를 사용하려고 하는 순간 커널은 CPU를 빼았는다. **I/O를 하고 싶으면 커널이 가지고 있는 function에 부탁해**라는 매커니즘을 System Call이라 하고 우리는 이 내용을 1장에서 배웠다. 이번 2장에서는 System Call의 구체적인 동작 방식을 살펴본다.

---

# 6. 시스템 콜(System Call)

---

**시스템 콜(System Call)은 것은 정확히 언제 일어나는 것일까?** 우리가 I/O관련 function을 하려고 하면 그때 바로 일어나는 것일까? 이것을 알아보기 위해 먼저 밑에 그림을 보자.

![image-20200806163106579](https://user-images.githubusercontent.com/58545240/90212182-aae12d80-de2d-11ea-970d-a9c08662e841.png)

*리눅스 명령어는 옆에 붙은 숫자에 따라* ***커맨드(1), 시스템 콜(2), 라이브러리 함수(3)****로 구분된다.*

위 그림의 좌측을 보면 유저 영역 안에 내가(유저)가 작성한 코드 `my code`가 있다. 이 코드에서 `printf()`를 호출(call) 하는데 이 `printf()` 코드는 내가 작성한 게 아니라 `library function`이다. C언어를 배울 때 `#include <stdio.h>`를 하는 이유를 생각해보면 금방 이해할 것이다. 그럼 이제 `printf()`가 내가 작성한 코드 `my code`에 들어오는데 `printf()`는 출력 즉, I/O를 해 줘야 한다. 1장에서 배웠듯, 멀티 유저 시스템에서 I/O는 오직 커널만 할 수 있기 때문에 **I/O를 하는 모든 library function은 무조건 System Call을 사용해야 한다. 커널에게 부탁한다는 뜻이다.**

시스템 콜을 하게 되면 **Wrapper Routine**이라는 공간에 가게 되고 이 공간에는 왜 커널로 가게 되는지 알려주는 정보들을 담고 있는 **Prepare parameter**와 CPU의 모드 비트를 커널로 바꾸는 **chmodk**가 들어있다.

**chmodk**가 실행되면서 프로그램은 런타임 중 트랩에 걸려 커널 영역으로 가게 된다. 커널에서는 `Prepare parameter`에 담겨있는 내용을 보고 적절한 **System call function**으로 처리를 해준다.

*커널 안에 있는 **모든 System call function의 이름은 `sys_`로 시작**한다. 리눅스의 naming convention(명명 규칙)이니 알아두길 바란다*

---

## 6.1 Wrapper Routine

트랩으로 넘어갈 내용들을 준비하고 실질적으로 트랩을 일으키는 공간인 `Wrapper Routine`에 대해 조금 더 알아보자.

![image-20200806163401735](https://user-images.githubusercontent.com/58545240/90212189-af0d4b00-de2d-11ea-82d2-20a237895692.png)

`Wrapper Routine`에서 (인텔의 경우) `$0x80`등 의미 없는 문자들을 이용해 Machine Instruction을 주어 트랩을 발동한다. 그런데 위에서 트랩을 일으키기 전에 `Prepare parameter`들을 준비하게 되는데 그 중에 가장 중요한 것은 바로 **system call number**라는 것이다. 이 `system call number`는 커널이 가지고 있는 **system call function의 시작 주소를 담고있는 Array(배열)의 Index 번호**로 사용이 된다.

`system call number`의 예를 들어 보면 다음과 같다. `file`과 관련된 `system call`에는 `open, close, read, write`등이 있는데 open은 0번, close는 2번, read는 3, write는 4번 등 call number을 이용해 Array의 Index 위치에 접근을 한다.

지금까지의 과정을 순차적으로 정리해 보면 아래와 같다.

1. 컴파일러(gcc)가 유저가 짠 코드를 보고 라이브러리(`printf()`)를 호출한다.
2. 라이브러리에서 시스템 콜(`write`)을 호출한다. 위 그림의 write(2)의 2는 시스템 콜을 의미하는 숫자일 뿐 매개변수와 같은 의미는 없다.
3. Wrapper Routine에서 `write`에 대응하는 `system call number`가 나오고 트랩을 건다.
4. 커널이 `system call number`을 가지고 `system call function table`에 접근해 `function`의 시작 주소에 접근한다.

```bash
# Note
여기서 하나 알아둬야 할 점은 이렇게 system call number를 지정한 컴파일러와 그 system call number를 받고 system call function table에서 function을 찾는 운영체제의 번호가 서로 일치해야 한다는 점이다. 이러한 번호들은 컴파일러를 쓰는 회사에서 결정을 한다. 실례로 만약 다른 회사의 플랫폼으로 시스템을 옮기면 소스파일들을 다시 컴파일을 해줘야 system call number가 얽혀서 오동작하는 오류를 방지할 수 있다.
```

마지막으로 아래 그림에 나온 예시를 통해 시스템 콜의 과정을 자세히 살펴보자.

![image-20200806163451277](https://user-images.githubusercontent.com/58545240/90212194-b3d1ff00-de2d-11ea-9245-b3c083c5ad91.png)

- 유저 프로그램이 시스템 콜을 호출한다.
- Machine Instruction이 트랩을 발동한다.
- 하드웨어가 유저 모드에서 커널 모드로 `mode bit`를 바꾼다.
- 하드웨어가 `sys_call()`이라는 커널안의 트랩 핸들러(Trap Handler)로 가게 된다.
- 이런 핸들러는 커널안의 `assembly function`을 수행한다.
- 지금까지 유저 프로그램에서 진행했던 단계를 저장을 한다. (커널 쪽 일이 다 끝나면 시스템 콜을 호출 했던 곳으로 돌아가서 다시 진행을 해야하기 때문에 저장하는 것이다.)
- 시스템 콜 번호가 커널 안에 `sys_call table`에 있는 번호에 맞는 번호인지 확인한다.
- 맞다면 `system call function`의 주소를 가져온다.
- 그리고 `system call function`을 불러 작업한다.
- (만약 진행 과정 중 디버깅이 필요하다면 디버거를 실행시킨다.)
- 다시 시스템 콜 호출했던 유저의 영역으로 돌아가고 mode bit를 유저 모드로 전환한다.

---

## 6.2 Kernel System Call Function

스마트폰 어플리케이션으로 찍은 사진을 볼 수 있는 갤러리 어플리케이션을 만들었다고 생각해보자. 갤러리 어플은 사용자가 자신이 촬영하여 폰에 저장한 사진을 볼 수 있게끔 해준다.

어플리케이션을 제작할 때 소스코드에는 분명 스마트폰에 저장된 사진을 읽어오는 기능이 있을 것이다. 이 기능은 `library`함수를 사용하여 구현했을 것이고, 실제 동작할 때 `library`는 I/O를 하기위해 System Call을 호출할 것이다. 커널에게 부탁한다는 매커니즘이 시스템 콜이라는 점을 다시한 번 떠올리자.

```bash
# Note
별도의 함수를 만들어서 스마트폰에 저장된 파일을 읽어오는 것보다는 라이브러리로 구현된 소스코드를 사용하는 것이 훨씬 효율적이다. 만약 코드를 직접 만든다고 해도 시스템 콜을 적절히 배합해서 원하는 동작을 하게끔 구현해야하는데 굳이 이렇게 할 필요가…
```

커널에서는 유저가 원하는 사진 파일을 시스템 콜을 호출한 유저 영역으로 넘겨줘야 할 것이다. 때로는 커널이 유저 영역으로부터 데이터를 가져와야 하는 경우도 있을 것이다. 즉 어플리케이션이 제대로 동작하기 위해서는 유저 프로그램과 커널 프로그램이라는 서로 독립된 프로그램 사이에 데이터를 주고 받을 수 있는 수단이 반드시 필요하다.

![image-20200806163607107](https://user-images.githubusercontent.com/58545240/90212198-b896b300-de2d-11ea-86f0-3a2be15d99b5.png)

**그러한 기능들은 오직 커널만이 가지고 있다.** 리눅스는 멀티 유저 시스템이고 시스템의 보안을 위해서 오직 커널만이 모든 메모리에 접근이 가능하다. 좀 더 자세히 살펴보면, 커널이 유저에게 데이터를 보내줄 수는 있어도 **유저가 커널로부터 데이터를 읽어 올 수는 없고** 커널이 유저한테서 데이터를 읽어올 수는 있어도 **유저가 커널한테 데이터를 보낼 수는 없다.** 모든 I/O는 커널을 통해서만이 이루어 진다.

```bash
# Note
이쯤 되면 유저는 거의 커널의 노예라고 할 수 있다. 모든 중요한 행위는 커널에게 부탁해야한다. 감히 컴퓨터에 직접적으로 데이터를 쓴다거나 읽어온다든가 하는 행위는 절대 할 수 없다.
유저가 요청하는 데이터의 바이트의 수는 커널이 디스크에서 받아오는 것처럼 일정한 바이트의 단위가 아닌 4바이트, 7바이트 등 여러가지가 될 수 있기 때문에 커널에는 유저가 원하는 바이트 만큼 넘겨주는 기능 등이 존재한다.
```

---

## 6.3 System Call Number

그럼 커널에 대해 더 자세히 알아보기에 앞서 트랩전에 정해지는 시스템 콜 번호에 대해 구체적으로 알아보고 가자.

![image-20200806163714470](https://user-images.githubusercontent.com/58545240/90212209-bcc2d080-de2d-11ea-94ca-031ca7d1f0c7.png)

`System call number`는 커널의 `system call table`의 인덱스 번호로 사용되어 `system call function`의 주소의 시작값을 불러오는 용도로 사용된다. `System call number`는 컴파일러와 OS를 제작한 회사에서 정하며 이렇게 정해진 번호는 변경 할 수 없다.

그렇다면 리눅스에 자신만의 `시스템 콜(System Call)`을 만들 수는 없을까? `sys_write()`나`sys_read()`처럼 내가 특정 기능 수행하는 시스템 콜을 정의하고 사용할 순 없을까? 물론 직접 만들 수 있다!

![image-20200806163738941](https://user-images.githubusercontent.com/58545240/90212214-c3514800-de2d-11ea-8859-42924d150bae.png)

시스템 콜을 만들기 전에 먼저 새로운 시스템 콜을 만드는 것의 장점을 살펴보자. 우리는 새로운 시스템 콜을 만들 때 우리가 원하는 특정 기능만을 위한 코드를 작성할 수 있다. 즉 기존에 존재하는 시스템 콜 보다 간단하고 성능 또한 좋게 만들 수 있다.

```bash
# Note
예를 들어, 여러분은 컴퓨터 화면에 특정 알파벳만을 출력하는 기능을 새로 정의할 수 있을 것이고 이는 알파벳 뿐만 아니라 숫자, 기호 등을 출력해줄 수 있는 기존의 printf() 함수보다 훨씬 코드도 간결하고 효율적일 것이다.
```

분명 시스템 콜을 직접 만들어 사용하면 성능도 좋고 기존 시스템 콜보다 간결할 수 있다는 장점이 존재하지만 이보다 훨씬 큰 단점이 존재한다. 새로운 시스템 콜을 만들게 되면, 그 시스템 콜만의 새로운 `system call number`가 필요하게 된다.

이렇게 새로 제작할 때마다 `system call number`를 정의하게 되면 새로만든 시스템 콜은 그것을 제작한 플랫폼에서만 사용할 수 있다. 즉 다른 플랫폼에서 본인이 만든 시스템 콜(예를 들어 99번)을 호출하는 것은 불가능하다. 다른 플랫폼에는 99번에 해당하는 시스템 콜이 존재하지 않거나 다른 시스템 콜일 수 있기 때문이다. **플랫폼 의존적**이라는 치명적인 단점 때문에 보통 시스템 콜을 직접 만들어서 사용하는 일은 거의 없다.

또한 한번 만든 시스템 콜은 **추가만 가능하고 변경은 불가능**하기 때문에 나중에 수정을 하는 것도 불가능하다. 그렇다면 새로운 시스템 콜은 만드는 건 아예 하지 말아야 할까? 다행히도 방법은 있다.

![image-20200806163811969](https://user-images.githubusercontent.com/58545240/90212220-c815fc00-de2d-11ea-9e65-fd202bb923bd.png)

그 방법은 바로 기존에 있던 시스템 콜인 `read`나 `write`에 있는 **파일 디스크립터(File Descriptor)**을 활용하는 것이다. 파일 디스크립터는 뒤에 다루겠지만 먼저 간단히 설명을 하자면 **운영체제가 만든 파일이나 소켓을 편하게 부르기 위해서 부여한 숫자**이다.

파일 디스크립터는 보통 적은 숫자만이 활용이 되고 있어 보통은 잘 쓰지 않는 999번 등에 본인의 파일 디스크립터를 지정하고 사용하면 커널안에 내장된 시스템 콜에 영향을 주지 않고도 사용할 수 있다. 훨씬 안전한 방법이다.

`Robert M. Love`의 책에서도 권장하는 방식이고 전 세계 모든 유닉스 사용자들이 이러한 방식을 사용하고 있다고 한다.

# 7. Process Management

---

시스템 콜에 대한 내용은 이 정도로 정리하고, **Process Management**에 대한 내용으로 넘어가자. Process Management는 커널이 하는 아주 중요한 임무 중 하나로서 반드시 짚고 넘어가야 할 부분 중 하나다.

---

## 7.1 OS Kernel

1강에서 우리는 운영체제가 어떤 역할을 하는지를 배웠다. 운영체제는 **하드웨어 자원을 관리**하고 **프로그램들을 지원**해주는 역할을 한다.

![image-20200806163848933](https://user-images.githubusercontent.com/58545240/90212233-ccdab000-de2d-11ea-923c-bd53d75748dd.png)

이와 마찬가지로 **운영체제의 핵심**인 커널 또한 같은 역할을 한다. 위 그림을 살펴보자. 커널은 위로는 프로그램들을 지원하고 밑으로는 하드웨어(CPU, Memory, Disk, TTY)를 관리하는 데이터와 기능들을 가지고 있는 프로그램이다.

```bash
# Note
실제로 위 아래 개념이 존재하는 것은 아니고 유저 프로그램과 하드웨어의 중간다리 역할을 한다는 점을 보여주기 위해 그림과 설명이 저렇게 제공된 것이다.
```

효율적인 하드웨어 관리와 유저 프로그램을 지원하기 위해 커널은 자체적인 **Internal Data Structure**을 가지고 있다.

![image-20200806163915750](https://user-images.githubusercontent.com/58545240/90212272-e3810700-de2d-11ea-90ae-6d7701e64552.png)

먼저 하드웨어 관리를 위한 **Data Structure**안에는 **각 하드웨어에 대한 정보**가 담겨있다. 예를 들어 Memory 하드웨어에 관한 Data Structure `mem`에는 이 **Memory의 크기가 어느정도이며 어디서부터 어디까지 메모리가 사용되고 있는지 등** 관리를 위해 필요한 내용들이 담겨있다.

하드웨어 뿐만 아니라 프로세스들을 관리하기 위한 Data Structure또한 존재한다. 우리는 이러한 Data Structure를 **PCB(Process Control Block)**이라 부른다. 즉 **프로세스를 지원하고 관리하기 위한 정보들이 담겨있는 데이터 구조체**이다.

위에서 설명한 프로세스와 하드웨어를 관리하기 위한 데이터가 담겨있는 데이터 구조체를 통틀어 **메타데이터(metadata)**라고 부른다.

```bash
# Note
컴퓨터공학을 공부하다보면 정말 많이 만나는 용어 중 하나가 메타데이터다. 데이터를 관리하기 위한 데이터라고 생각하면 이해가 편하다. 도서관에 수많은 책들이 존재하는데, 책들을 관리하기 위해서는 효율적인 전산 시스템이 필요하듯 메타데이터 또한 시스템에 있어 필수적인 요소다.
```

**그렇다면 프로세스를 관리하기 위한 metadata에는 어떤 정보들이 있을까?**



![image-20200806163949014](https://user-images.githubusercontent.com/58545240/90212279-e845bb00-de2d-11ea-8dff-3fb1bad23ef9.png)

`metadata`에는 다음과 같은 내용등이 담겨있다.

- PID(프로세스 식별자)
- 프로세스의 우선순위
- 대기 현상 (디스크를 읽고 쓰는 등 입출력 작업에는 waiting이 일어난다.)
- 프로세스의 상태 (동작 중인지, 수면 중인지)
- 디스크 내 이미지의 위치
- 메모리 내 이미지의 위치(메모리 안에 코드가 저장되어 있는 위치)
- 열린 파일들(유닉스에서 **파일은 바이트의 연속이고 각종 디바이스 또한 전부 파일로 취급**한다. 참고로 **제일 먼저 오픈하는 파일은 키보드와 스크린 파일**이다.)
- 현재 프로세스가 실행되고 있는 환경에 대한 정보
- 터미널
- 상태 백터 저장 공간 (`state vector save area`라는 용어 자체에 친숙해지는 것이 좋다.)

```bash
# Note
- 만약 프로세스 A가 CPU를 점유하고 있다가 디스크에 용무가 생겨 디스크에게 갔는데 디스크가 먼저 들어 온 일을 처리하고 있었다면 기다림(waiting)을 신청하고 디스크가 작업을 끝내기를 기다린다. 인간 세계에서는 대기 시간이 고작 몇 초도 안걸리는 작업이라고 생각할 수 있지만 이 정도의 시간은 CPU 입장에서는 몇억, 몇 천억년의 시간이기에 A가 기다리는 동안 A가 점유하던 CPU를 다른 프로세스에게 주게 되는데 이때 A가 하고 있던 작업 내용을 A의 PCB(Process Control Block)에 저장을 한다.

- 이때 이 저장 공간을 state vector save area라고 한다. state vector save area는 Register들을 저장하고 있는 공간이다. Register라는 건 State of Flipflop(0과 1)이 32개가 모여있는 집합이다. 프로세스의 상태들을 저장한다고 이해하면 된다.
```

- 부모, 자식 프로세스
- 실행 시간

**이처럼 metadata에는 프로세스와 하드웨어를 관리하는데 있어 필요한 모든 정보를 담고 있다.**

다음으로 `state vector save area`를 자세히 살펴보자. 먼저 앞서 예시로 들었던 **프로세스 A의 기다림(waiting) 신청**이라는 개념에 대해 자세히 설명하겠다. 도대체 기다린다는 건 뭐고 커널 내에서 어떻게 동작하는 걸까?

![image-20200806164033632](https://user-images.githubusercontent.com/58545240/90212287-ed0a6f00-de2d-11ea-84a9-a8beebd34070.png)

우리는 은행에 가서 일을 처리하려 할 때 이미 창구에 다른 사람이 먼저 일을 보고 있으면 **번호표**를 뽑고 기다린다. 이처럼 프로세스 또한 본인이 사용하고 싶은 하드웨어가 이미 다른 프로세스에 의해 사용되고 있으면 대기표를 뽑고 기다려야 한다.

프로그램적으로 위 과정을 이해해보면 다음과 같다. **프로세스가 자신의 PCB에 사용하고 싶은 하드웨어에 대한 링크를 걸어놓고 Waiting Queue(대기열)에 들어가게 된다.** 만약 본인 앞에 다른 프로세스가 똑같은 하드웨어를 사용하려고 이미 `Waiting Queue`에 있는 상황이라면 먼저 기다리고 있던 프로세스의 뒷 순서로 `Waiting Queue`에 들어간다.

이런 `Waiting Queue`중 **CPU에 링크를 걸어놓고 기다리는 것을 ready queue**라고 **하고 디스크에 링크를 걸어놓고 기다리는 것을Disk I/O queue(또는 Disk wait queue)라고 한다.**

# 8. Child Process 생성하기

---

컴퓨터를 부팅하면 제일 먼저 **커널 프로세스**가 로드된다. 그리고 이 커널은 터미널이 켜질때 마다 그에 해당하는 **Shell**, 즉 **Child Process**를 만든다. Shell은 사용자의 입력을 기다리고 입력이 들어오면 그에 따른 작업을 수행해주는 프로그램이다. 사용자가 `Mail`이라고 입력하면 `Mail`이라는 `Child Process`가 생성 된다. 이처럼 프로세스들이 진행될 때는 자식 프로세스(Child Process)가 생성되면서 진행된다. 따라서 커널을 공부할 때 Child Process는 반드시 알아야하는 개념이다. 지금부터의 설명은 아래 그림과 함께 살펴보도록 한다.

![image-20200806164057432](https://user-images.githubusercontent.com/58545240/90212296-f4317d00-de2d-11ea-8789-5b6639473b3a.png)

```bash
# Note
잠시 여기서 프로그램과 프로세스의 차이에 대해 알아보자. 프로그램과 프로세스의 차이는 명확하다. 프로그램은 보조 기억 장치에서 실행이 되기만을 기다리는 정적인 데이터의 집합이고, 프로그램이 명령어와 데이터와 함께 메모리에 적재되면 프로세스가 되는 것 이다. 즉, 프로세스란 실행 중인 프로그램을 뜻한다.
```

1강에서 배웠듯 프로그램에는 `User Stack`과 `Kernel Stack`이 존재한다. `User Stack`은 프로그램에서 `function`을 사용할 때 사용된다. `Kernel Stack`은 유저 모드에서 시스템 콜을 통해 `커널의 function`들을 사용할 때 필요한 자료구조로 프로그램 실행에 필요한 Local Variable들을 저장하기 위한 공간이다. 만약 자료구조를 가변적인 Stack구조로 사용하지 않고 늘 공간을 확보해둔다면, 프로그램 크기가 엄청 커지고 운영 비용만 비싸질 것이다.

지금부터는 `Child Process`를 생성하기 위한 과정들을 살펴볼 것이다. `Child Process` 생성을 위해서는 먼저 Process의 정보가 들어있는 `PCB(Process Control Block)`를 만들고 그 PCB에 해당하는 Process를 만들어 줘야한다.

진행 순서는 아래와 같다.

1. **PCB 공간을 만들어 준다.** 초기값으로 **Parent Process의 PCB를 복사**해온다. Parent가 사용하던 Resource(터미널, 키보드, 스크립트)를 자식 프로세스도 사용하게 되는 것이다. `Parent Process`의 실행 환경이 `Child Process`의 실행 환경이 된다.
2. `Child Process`가 들어갈 수 있는 메모리 공간을 확보하여 초기값을 지정한다. 이를 위해 커널은 Memory의 Data Structure에 가서 빈 메모리 공간을 찾아 공간을 지정해준다. 지정된 공간에 Child Process의 값들을 넣기 전에 먼저 `Parent Process`의 **image를 똑같이 복사**를 해준다. 이 이유는 후에 등장한다. 프로세스 처리과정을 간편화하기 위해 복사한다고 일단 기억해두자.
3. 디스크로부터 `Child Process`에 **새로운 image를 로드한다.**
4. 새로 생긴 `Child Process`의 `PCB`를 **CPU의 ready queue에 등록**하여 CPU를 사용 할 수 있게끔 준비해준다. (아직까지 CPU는 `Parent Process`가 사용 중 이기 때문이다.)

이러한 4가지 과정을 시스템 콜의 용어로 정리하면 **두가지**로 정리할 수 있는데,

1. 1번과 2번의 과정을 **Fork**라고 부른다. (Parent와 동일한 것을 만든다.)
2. 3번과 4번의 과정을 **Exec**이라고 부른다. (디스크로 부터 새 이미지를 읽어온다.)

---

## 8.1 Fork

일단 Fork(포크)에 대해 알아보기 전에 Fork는 **한번 호출하면 두번 리턴한다**라는 개념으로 기억하자. 지금은 이해가지 않더라도 일단 이 사실을 받아들이고 설명을 읽어보자.

두번의 리턴 중 첫번째 리턴은 `Parent Process`가 본인이 가지고 있는 Process 상태를 그대로 `Child Process`에 복사하고 CPU의 `ready queue`에 `Child Process`를 등록 시켜놓고 다시 `Parent Process`로 리턴하는 과정이다.

```bash
# Note
단순히 함수를 호출한 후 리턴해서 그 다음 실행흐름으로 위치했다는 의미다. 프로그램적으로 너무나도 당연한 과정이다.
```

그 후 `ready queue`에 등록되어 대기중이었던 `Child Process`가 CPU를 점유하게 된다. `Child Process`가 실행되는데, `Child Process`는 만들어질 당시 `Parent Process`와 동일한 `PCB(Process Control Block)` 즉, 같은 `State Vector`를 가지고 생성되었기 때문에 **Fork를 호출하고 난 바로 그 다음 진행 시점**에서 실행된다.

즉 `Child Process`는 `Parent Process`가 가지고 있는 정보들 뿐만 아니라 **프로그램 진행 상황까지 완전히 똑같은 상태**를 가지게 되고 이런 현상 때문에 **Child Process 또한 Fork에서 리턴**하게 된다.

그렇기 때문에 한번 Fork를 해서 두번 돌아온다는 표현이 생긴 것 이다. 단, 운영체제가 이런 두 가지의 return으로 일어나는 혼동을 막기위해 리턴하는 값은 다르게 해준다. 지금까지 설명한 과정을 아래 그림과 함께 살펴보자.

![image-20200806164202055](https://user-images.githubusercontent.com/58545240/90212306-fa275e00-de2d-11ea-9895-c1910f4457ff.png)

Fork가 두번 리턴되는데 한번은 `Parent Process`로, 한번은 `Child Process`으로 리턴한다. 그리고 리턴할 때의 값은 **pid 값**이다. `pid`는 Process Id라는 의미로 이는 유닉스 시스템에서 각 프로세스에게 할당하는 고유 식별값이다. `pid`값이 0이라면 가면 `Child Process`를 의미하고 그게 아니라면 현재 실행 중인 프로세스는 `Parent Process`다.

```bash
# Note
fork()는 두 번 리턴하는데, 각 리턴값은 다음과 같다. Child Process에게는 0값을 리턴하고 Parent Process에게는 Child Process의 pid(process id)를 리턴한다.
```

아래의 프로그램을 리뷰하면서 내용을 정리해보자.

![image-20200806164234130](https://user-images.githubusercontent.com/58545240/90212312-fdbae500-de2d-11ea-99c9-1a38ffb4bf53.png)

`fork.c`라는 이름을 가진 소스파일이고, fork를 호출하는 프로그램이다. 프로그램의 출력 결과를 예상 해보고 확인하면서 지금까지 배운 `fork`를 리뷰해본다.

`fork()`를 호출하면 위 그림에 나와 있는 코드가 그대로 복사되어 `Child Process`에게 할당된다. **하나 더 생성**이 되는 것이다. 그럼 `Parent Process`와 `Child Process`는 서로 같은 코드와 상태를 가지고 있게 되는 것이다.

`fork()`호출의 리턴값 `pid`의 값에 따라 `Child Process`가 실행되거나 `Parent Process`가 실행된다. 결국엔 둘 다 실행되겠지만 둘 중 누가먼저 실행되는지 위 코드에서는 정확히 파악하기 어렵다. 보통은 부모 프로세스가 먼저 실행된다.

---

> 이번 강의에서는 System Call이 일어나는 절차에 대해 System Call Wrapper Routine, System Call Number 등을 배웠고 커널이 프로세스들과 하드웨어들을 관리하기 위해 정보를 모아둔 Data Structure인 metadata에 대해서 배웠다. 또 마지막으로 Child Process의 생성과정 중 Fork에 대해 간략히 알아보았다. 다음 3강에는 Fork를 좀 더 자세히 살펴보고 Exec에 대해 설명한다. 

---

# 9. 주요 시스템 콜 동작 원리

---

> 2강에서 설명했던 `fork()`의 작동 원리에 대해서 이어서 설명한다. `fork()`뿐만아니라 이번 3강에서는 **다양한 시스템 콜**에 대해 학습한다. 또한 **데몬(Daemon)과 서버(Server)**에 대해서도 간단히 학습할 것이다.
>
> 시작하기 앞서 이번 강의에서 등장할 그림들에 오류가 있다는 점을 언급하고 싶다. 오류가 있는 부분은 별도로 빨간색으로 마크해서 원래 있어야할 곳으로 표식을 해놓거나 중간 중간 어떤 부분에 오류가 있는지를 언급을 했으니 부디 설명을 읽으면서 헷갈리지 않길 바란다.

---

## 9.1 Fork(2)의 동작 원리

![image-20200811161310555](https://user-images.githubusercontent.com/58545240/90212319-027f9900-de2e-11ea-9714-4e1599ec9c50.png)

그림에서 수정된 사안이 하나 있는데, **printf(“I am parent!\n”)**부분이 else 구문에 속해야하는 것이 맞다. 이점을 주의해서 아래 설명을 보자.

위의 소스코드로 동작하고 있는 프로그램이 **쉘(Shell) 프로그램**이라고 해보자. 쉘 프로그램은 사용자로부터 입력을 기다리고 입력된 명령을 토대로 프로그램을 실행하는 교통 정리 프로그램이라고 우리는 배웠다. 쉘이 시작되면 명령어를 입력할 수 있는 **터미널 혹은 프롬프트 창**이 등장할 것이고 쉘은 터미널 혹은 프롬프트 창에 사용자로의 명령이 입력되기를 기다리고 있다. 아래와 같은 화면을 생각하면 된다.

![image-20200811161331469](https://user-images.githubusercontent.com/58545240/90212329-06abb680-de2e-11ea-878a-dd197246b191.png)

우리가 쉘에 Microsoft의 Word 프로그램을 실행시키는 **word**라는 명령을 터미널에 입력했다고 해보자. 입력된 명령어를 받은 쉘은 가장먼저 **fork( )**를 진행한다. **fork( )**를 호출하면 자식 프로세스가 생성되면서 부모 프로세스와 완전히 동일한 **소스코드(image)** 갖게된다. 코드 뿐만 아니라 부모 프로세스의 **PCB(Process Control Block)**도 그대로 물려 받는다. (PCB에 대해서는 [2강 강의노트](https://medium.com/pocs/리눅스-커널-운영체제-강의노트-2-78406a13c5c9)에서 자세히 다뤘으니, 기억이 나지 않는다면 2강 강의노트에서 PCB 키워드로 검색을 해서 확인하길 바란다.)

```bash
# Note
fork()는 두번 리턴된다. 한 번의 리턴은 자식 프로세스에게 0값을 리턴하고 나머지 한 번은 부모 프로세스에게 자식 프로세스의 프로세스 아이디값을 리턴한다.
```

아직은 부모 프로세스가 **CPU를 점유**하고 있기에 **fork( )**로부터 리턴된 **pid값**은 자식 프로세스의 **pid**값이고 부모 프로세스는작업`printf("I am parent!n")`을 마저 진행한다. 자식 프로세스의 pid값을 리턴 받음으로써 부모 프로세스는 자식 프로세스를 알고 통제할 수 있는 것이다. 부모 프로세스로부터 복제되어 생성된 자식 프로세스는 현재 **ready queue**에서 **CPU**가 자신에게 할당되기를 기다리는 중이다.

앞서 2강에서 **fork( )**는 **두 번 리턴된다**고 설명한 바 있다. 첫 번째 리턴에서는 **자식의 pid(Process Id)**를 리턴하므로 if 조건문을 건너 띄고 else 구문으로 넘어간다. else구문으로 넘어가면 **printf(“I am parent!n”);**가 실행되고 모니터 화면에는 **I am parent**가 나타나게 될 것이다. 작업을 다 마친 부모 프로세스는 종료가 된다.

이후 **CPU**의 점유권은 자식 프로세스에게 넘어가게 된다. 이론상 **ready queue**에 대기하고 있던 다른 프로그램들이 없었다고 가정한다면, 부모 프로세스가 끝남과 동시에 자식 프로세스는 **CPU**를 쥐게 된다.

자식 프로세스는 어떻게 동작할까? 위에서 `fork()`가 실행되면서 부모의 코드(이미지) 뿐만 아니라 **PCB를 통째로 복사**했기 때문에 다음에 어디서부터 실행해야할지 알려주는 **PC(Program Counter)와 SP(Stack Pointer) 등 또한 복사**되었다. 즉 PCB에 존재하는 **State Vector Save Area영역 (이하 state vector로 서술함)**에 있는 `PC`와 `SP`등을 복사했기 때문에 자식 프로세스의 코드가 실행될 때는 맨 처음부터 실행되는 것이 아니라 `fork()` 중간에서부터 다시 진행하게 되어 있다.

대부분의 프로그램은 초기 실행될 때 `main()`부터 시작한다. **PCB에 그렇게 명시되어 초기화가 되기 때문**이다. 하지만 지금 다루고 있는 자식 프로세스의 경우는 PCB에서 가리키고 있는 다음 실행주소(Program Counter)가 `fork()`에 있었기 때문에, **자식프로세스는** `**fork()**`**중간 영역부터 진행**한다. (중간 영역이라는 건 `fork()` 함수가 한창 진행중일 때 복사가 일어났으므로, 그 진행중이었던 파트부터 다시 진행된다는 의미로 해석하면 된다.)

자식 프로세스가 `fork()`에서 리턴되면, **자식 프로세스 코드 안의 pid 변수는 0의 값(자식 프로세스 pid는 보통 0)**을 가지기 때문에 **I am Child \n**이 화면에 출력되게 된다. 지금까지 다룬 내용을 다시한 번 정리하면서 아래 그림을 살펴보자.

![image-20200811161413429](https://user-images.githubusercontent.com/58545240/90212367-1aefb380-de2e-11ea-87a5-5f3f39525cd0.png)

위에서 수정했던 것과 마찬가지로 일단, `printf("I am Parentn")`는 else문에 속해 있어야 하는 것을 염두하고 살펴보면, 첫 번째 출력값인 **I am Parent는 일단 부모 프로세스가 시행한 작업**이다. 그리고 부모프로세스가 끝나면서 자식 프로세스가 CPU를 점유하게 되면서 `fork()`로부터 리턴 값을 받아 if문 조건을 만족하게 되고, `printf("I am Child n")`를 실행하게 된다. 따라서 **I am Child는 자식 프로세스가 시행한 작업**이라고 할 수 있다. if문 끝단에 있는 `execlp` 구문 같은 경우는 바로 아래에서 이어서 설명한다.

## 9.2 Exec(2) 동작 원리

![image-20200811161524397](https://user-images.githubusercontent.com/58545240/90212413-365abe80-de2e-11ea-888c-b02f81c1466e.png)

**exec(2) 시스템 콜**에 대해 알아보기 전 몇 가지 배경지식에 대해 먼저 짚어보고자 한다. 위 그림을 보면서 함께 설명을 따라가보자. 먼저 `exec()`에 매개변수를 살펴보면, `/bin`이 보인다. `/bin`은 **바이너리(binary) 파일만 모아둔 폴더(directory)**를 의미한다. 그 폴더 안에는 바이너리 프로그램들이 수 십개가 존재하고 있는데, 그 바이너리 프로그램 마다 원래는 `a.out`의 형식으로 되어 있지만 **그 이름을 각자의 프로그램 제작사의 입맛에 맞게끔 설정해 놓았다(ls, cat, hwp, ppt 등).**

코드를 살펴 보면 자식 프로세스 차례가 왔을 때 `I am child!` 부분의 출력문을 출력하고, **execlp(exec 계열 함수)를 실행**하게 되어 있다. `exec` 시스템 콜은 현재 돌아가고 있는 프로세스 위에 **자신의 프로세스로 완전히 덮어씌어(over write) 버린다.** 덮어쓴 후 **exec 매개변수로 왔던 그 프로그램의 main( )으로 가는 것**이 `exec`의 작동 원리다.

새로운 프로세스가 생기는 것이 아니기 때문에, **pid(Process Id)는 변하지 않는다.** 다만 **프로세스를 구성하는 코드(기계어 코드)와 데이터, 힙, 그리고 스택 영역의 값들이 exec으로 발생하는 새로운 프로그램의 것으로 바뀌게 된다.**

![image-20200811161548608](https://user-images.githubusercontent.com/58545240/90212423-3a86dc00-de2e-11ea-97fc-905ef470ebbd.png)

설명은 위와 동일하다. **exec은 자신의 프로세스를 현재 진행 중인 프로세스 위에 덮어 써버린다.** 덮어 씀과 동시에 **date의 main( )으로 넘어가는 것**이고, 그 쪽에서 날짜를 출력해주는 작업을 진행한다. 그래서 유닉스나 리눅스에서는 **프로세스의 생성이 fork( )하고 exec( )을 하는 두 스텝으로 존재한다.**

**fork( )는 image(= 소스코드)와 PCB를 전부 복사**하는데, `exec()`의 경우에는 현재 image에 새로운**실행(execute)코드를 디스크로부터 바이너리 파일 형태로 가져온 후**에**현재 image에 덮어 씌우기(over write)를 진행**하고 **자신 프로세스의 main( )으로 진행하는 것**이다. 한마디로 기존의 작업하던 것을 자신의 프로그램으로 갈아 치우고 자신의 프로그램을 가동시키는 행위라고 할 수 있다.

## 9.3 Wait(2) 동작 원리

![image-20200811161612801](https://user-images.githubusercontent.com/58545240/90212429-3f4b9000-de2e-11ea-914d-0c98398c1270.png)

시스템 콜은 결국 **커널모드로 진입하는 것**을 뜻한다. 위 그림을 보면서 **wait( )**에 대해 알아보자. 어떤 프로그램이 `wait()`를 호출하면 **해당 프로그램의 CPU 사용권한을 박탈**한다. 위 그림의 본문 첫 줄에 등장하는 것처럼 **프로세스 P_A로부터 CPU 사용 권한을 박탈**한다(**preempt**).

![image-20200811161628504](https://user-images.githubusercontent.com/58545240/90212434-4377ad80-de2e-11ea-892f-27e1319290f4.png)

임의의 프로세스 A(위 그림에서 P_A 라고 표현되어 있음)가 `wait(2)` 시스템 콜을 호출하면 **커널모드(K)의 트랩 핸들러(Trap Handler)에 진입**하여 **wait( ) 시스템 콜 실행**을 하게 되는데, 이때 **시스템콜을 호출한 프로세스로부터 CPU를 뺐는다(preempt).**

풀어쓰자면, 커널은 보통 자신의 작업을 다 하고 나면 호출한 프로세스의 유저 모드로 돌아가야 하는데, 유저모드로 돌아가지 않는다.

커널이 아닌 프로그램은 자신의 주소(address)에 한정되서 read, jump 등을 수 할 수 있지만 **커널은 어디로든 가고 jmp(점프)할 수 있기 때문에** `ready queue`에 가서 준비된 프로세스 중 **우선순위가 가장 높은 프로그램의 PCB를 찾아서 PC(Program Counter)를 알아낸 후에 PC(프로그램 카운터)가 가리키 쪽으로 가는 것(jmp)이다.** 이 과정이 **preempt**라 부른다.

![image-20200811161645908](https://user-images.githubusercontent.com/58545240/90212437-470b3480-de2e-11ea-9b0e-4298f2acfe68.png)

그 아래의 그림을 살펴보자. 이번에는 **부모 프로세스에 초점**을 맞춰서 살펴보자. `fork( )`후에 if문을 통과한 후에 else문에서 부모 프로세스는 자신의 일을 수행한다. 모든 일을 마친 후 소스코드의 마지막으로 가보니 **wait( ) 시스템 콜을 호출**하고 있다.

![image-20200811161700182](https://user-images.githubusercontent.com/58545240/90212450-4c687f00-de2e-11ea-9eab-3092917c442c.png)

**wait( ) 시스템 콜을 호출하면, 부모 프로세스는 잠들게 된다.자식 프로세스가 끝날 때까지** 잠을 잔다(sleep). CPU는 자식 프로세스에게 넘어가고 자식프로세스는 자신이 할 일을 수행한다. 자식이 하는 일 중에 `execlp("/bin/date"...)`라는 명령어가 마지막으로 있으니 해당 명령어를 마지막으로 수행하고 자식 프로세스는 중료한다.

자식 프로세스가 종료했을 때 **CPU는**자식 프로세스로부터 **부모 프로세스를 찾는다.** 그 후 CPU는 **부모 프로세스를 대기명단(ready queue)에 등록**시킨다. 이후 **부모가 CPU 점유권을 받았을 때! 그 때가 바로 wait( ) 시스템 콜이 끝나는 지점**이다. 부모는 이후 자신의 남은 일이 있었다면 해당 작업을 진행하게 된다.

```bash
# Note
비유를 들자면, 메일 프로그램을 들 수 있다. 메일 프로그램을 이용하는 목적은 상대에게 메일을 보내는 것이므로 우리는 ‘메일 쓰기’를 클릭할 것이고, 곧 텍스트를 입력할 수 있는 에디터가 나타난다. 여기서 메일은 부모프로세스고 텍스트 에디터는 자식 프로세스라고 할 수 있는데, 우리가 메일 쓰기를 마치면 자식 프로세스(텍스트 에디터)가 종료하면서 부모 프로세스(메일 프로그램)가 다시 등장하게 된다.
```

## 9.4 Exit(2) 동작 원리

메인함수 `main()`가 끝날 때는 **반드시 exit(2) 시스템 콜이 존재**한다. 설령 우리가 소스 프로그램을 작성할 때, `exit()`을 직접 기입하지 않았더라도 **컴파일러가 알아서 main( ) { }의 마지막에 exit(2) 시스템 콜을 삽입**하게 되어 있다. 아래 그림을 살펴보자.

![image-20200811161746093](https://user-images.githubusercontent.com/58545240/90212455-512d3300-de2e-11ea-8838-db8002b33a4b.png)

자식 프로세스(`pid: 0`)의 작업 중 `execlp("/bin/date", ...)`가 있고 위에서 배웠듯이 `exec(2)`계열의 시스템 콜(`exec, execv, execlp ...`)이 실행되면서 현재 있는 프로세스 위에 **인자로 주어진 프로세스(date)를 덮어 씌어버린다.** 그리고 곧장 해당 프로세스의 `main()`을 실행시키게 된다. 원래 저 노란 박스(main 함수가 들어 있는)에는 `exit()`이라는 소스코드가 존재하지 않았다. 하지만 컴파일러가 컴파일을 할 때 삽입을 해줬고, 실제 만들어진 이진파일(binary file)을 열어 보면, `exit(2)`에 해당하는 코드가 들어있게 된다.

![image-20200811161805188](https://user-images.githubusercontent.com/58545240/90212483-66a25d00-de2e-11ea-8104-4b0817d44a11.png)

위 그림에는 `exit(2)`의 작동 원리가 좀 더 상세하게 적혀 있다. 이후 들어오는 신호들을 전부 무시해버리고, 파일들이 열려 있다면 파일들을 닫는다. 또한 메모리 영역에서 해당 프로세스가 차지하고 있는 부분(image)을 해제(deallocate) 해버리고, 부모 프로세스에게 통보한다. 그리고 `exit(2)`을 호출한 프로세스의 상태를 좀비(ZOMBIE)상태로 설정한다. (좀비 상태라는 건 다음 강의에서 다루게 된다.)

커널에서 일어나는 동작으로는 **먼저 exit(2)을 호출한 프로세스의 CPU를 빼았고, ready queue에 있던 다른 프로세스에게 CPU를 넘겨**준다. 이 과정을 **스케쥴링(scheduling)**한다고 표현하는데, 실제로 **exit(2)을 호출하게 되면 커널 안의 schedule( ) 함수가 호출**된다. 스케쥴 함수 관련 설명은 글의 마지막 3번 부분에서 다룬다.

# 10. 시스템 콜 요약 정리 (Summary)

---

![image-20200811161833210](https://user-images.githubusercontent.com/58545240/90212494-6ace7a80-de2e-11ea-8346-e799499a1e72.png)

지금까지 우리는 프로세스를 위한 4가지 시스템 콜에 대해 살펴 보았다. `fork()`는 부모 프로세스와 아주 유사한 자식 프로세스를 만들어 내고, `exec()`은 진행 중인 프로세스 위에 새로운 프로세스 이미지를 덮어 씌운 후 `main()`으로 가게 된다. `wait()`은 이 시스템 콜을 호출한 프로세스를 잠들게 하는 것이고, `exit()`은 가지고 있던 모든 자원(resource)을 반환하고 부모 프로세스에게 알려주는 역할을 한다.

# 11. Context Switch (유저 모드와 커널 모드 사이의 전환)

---

지금부터 설명하는 내용은, 설명과 함께 그림을 봐야 이해가 잘되니 이 점 꼭 유의해서 설명을 차근차근 살펴보자.

![image-20200811161900993](https://user-images.githubusercontent.com/58545240/90212496-6efa9800-de2e-11ea-918c-aac3bc4ddf95.png)

1. 유저가 쉘이 띄어 준 프롬프트에 **명령어(“ls”)를 입력**한다. 쉘 입장에서 이 프로세스를 실행시키기 위해서 **fork( )를 실행**한다. 여기서 **쉘은 부모 프로세스**가 되고 **새롭게 생기는 프로세스는 자식 프로세스**가 된다. `fork()`가 동작하면서 쉘의 **PCB와 쉘의 a.out(코드)을 그대로 복사**한다. 그러나 **CPU는 아직 쉘에게 할당**되어 있기 때문에 `ls`가 실행되거나 하진 않는다.
2. **부모 프로세스 쉘이 wait( )을 호출**하게 되고 **쉘은 잠들게 된다.** 잠들면서 **부모 프로세스는 CPU의 대기 리스트(queue)에 들어가게 된다.**
3. 자식 프로세스는 부모 프로세스와 똑같은 코드 및 상태를 가지고 있으므로 **fork( ) 중간에서 동작**하게 된다. `fork()`로부터 리턴된 값은 자식 프로세스를 뜻하는 `pid`값 0으로 **자식프로세스는 execlp("/bin/ls" ...)를 실행**하게 된다.
4. **디스크로부터** `**ls**`**를 로드**한다. 자식 프로세스가 **기존의 부모 프로세스(쉘)로부터 그대로 복사해왔던 이미지 위에 그대로 덮어**씌운다(overwrite). 덮어 씌운 후 `**ls**`**의 메인 코드로 가서 코드를 실행**한다(`ls`가 실행된다).
5. `ls`가 끝나면**exit(2) 시스템 콜**을 하게 되어 있고, `exit(2)`을 호출함으로써 **다시 커널모드로 들어와서 커널은 CPU를 다른 프로세스에게 할당**하게 된다. 이 때 **wait(2) 시스템 콜이 끝난 것**으로 인지를 하게 된다.
6. (그림에는 7번으로 되어 있음) 높은 우선순위를 가지고 기다리고 있던 프로세스가 없다면, **기존의 부모 프로세스(쉘)는 다시 동작**하게 된다.

![image-20200811161919872](https://user-images.githubusercontent.com/58545240/90212500-7326b580-de2e-11ea-8046-0d7987978fb9.png)

위 그림은 **쉘의 유저모드와 커널모드를 왔다 갔다 하는 것을 시간 순서로 도식화**해놓은 것이다. 위에 해당하는 부분은 가볍게 훑어보는 것으로 아래의 그림으로 넘어가 보자.

![image-20200811161937216](https://user-images.githubusercontent.com/58545240/90212508-77eb6980-de2e-11ea-8638-acc206229787.png)

Kernel의 경우 **하드웨어를 위한 자료구조**, 즉 테이블이 하나 존재한다. 그 자료구조를 위 그림에서는 **struct CPU**라고 표현하고 있다. 위 그림의 상황을 보자면, 먼저 CPU가 P1을 실행시키고 있다(**파란 글씨로 P1 was running on CPU**). 그리고 **P1이 wait(2) 시스템 콜을 호출**한다. 시스템 콜을 호출하면서 **커널은 CPU state vector(PC, SP 등)를 P1의 PCB에 저장**한다.

이렇게 상태 값을 기억하는 이유는 **wait(2) 시스템 콜이 끝났을 때 wait(2)을 호출한 프로세스가 다시 정상적으로 작업을 원활하게 진행하기 위해서**다. 보다시피 **P1과 P2의 PCB는 커널 코드 안에**있다. 위 그림의 Kernel 파트를 보면, 커널에는 2종류의 자료구조가 존재하고 있다. P1과 P2에 해당하는 PCB들을 각각 하나씩 가지고 있는데, **커널 안에는 기본적으로 각 하드웨어 자원들 마다(for each hardware resource) 자료구조**가 존재하고 또한 **각 유저 프로세스 마다(for each user processs) 자료구조가 존재**한다.

P1은 자신의 **state vector에 해당하는 값들을 P1에 대응되는 PCB에 써주고(저장하고)**, CPU는 이제 그 다음 실행해야 할 프로세스에게 자신을 넘겨줘야 한다. CPU는 ready queue를 따라가서 CPU를 쓰겠다고 줄을 서 있는 프로세스들의 PCB를 살펴보고 우선순위가 제일 높은 프로세스를 선택한다. 그 프로세스가 동작하기 위해서는 **그 프로세스에 해당하는 PCB로부터 레지스터 값들을 가져와서 자신이 가지고 있는 PC, SP 등에 저장**해야 한다. **CPU안에 있는 PC(Program Counter)가 P2의 PC로 바뀌었기 때문에 P2의 PC가 가리키고 있는 곳부터 실행(run) 된다.**

![image-20200811162008321](https://user-images.githubusercontent.com/58545240/90212520-7d48b400-de2e-11ea-82bf-665ed9caac34.png)

다음으로 **Context Switch**에서 중요한 역할을 맡고 있는 **schedule 함수**에 대해 살펴보자. `schedule()`은 internal 함수이고, `Kernel a.out`에 알려지지 않는 함수다. **internal 함수랑 정 반대되는 성격을 가진 것이 바로 시스템 콜이며, 시스템 콜은 커널 a.out에 알려지고 커널 밖에서 부를 수 있다.** (커널이 금단의 영역이라면, **시스템 콜은 그 영역에 접근할 수 있는 유일한 방법**이다.) **반면에 schedule( )은 커널 안에서만 부를 수 있는 함수**이다. 즉 유저모드(커널 밖)에서는 요청조차 할 수 없다.

우선 이 `schedule()`은 **다음에 실행될 프로세스를 찾아 선택**한다. 그리고선 **context_switch( )라는 함수를 호출**한다. `schedule()`은 `read(), wait(), exit()`과 같은 함수가 호출한다. `read()`의 경우를 생각해보면, 사실 디스크로부터 데이터를 읽어와 달라는 요청은 CPU 입장에서는 몇 억년 걸리는 일이다. 디스크에 간다고 해서 바로 정보를 읽어올 수 있는 확률은 매우 적기 때문에(다른 프로세스에서도 디스크를 사용중일 수 있기 때문에) 필연적으로 대기하는 시간이 생기게 되는데 이 시간 동안 CPU가 가만히 있을리 없다. 모든 자원은 제때 제때 효율적으로 사용이 되어야 하기 때문에 CPU를 다른 프로세스에 할당 해주어야만 한다. 그래서 `read()`에서도 `switch()` 호출이 일어나는 것이다.

**context_switch( )**를 부르면, **현재 CPU state vector를 은퇴하는 프로세스의 PCB에 쓰고,새로 등장(arising)하는 프로세스로부터 PCB를 로드**하고, **해당 PCB의 PC로부터 다시 프로그램을 진행**하는 작업을 해준다.

즉 `schedule( )`은 **CPU의 임자가 바뀌어야 할 때(read( ), wait( ), exit( ))마다 불리고, 새로운 임자에게 할당해주기 위한 내부 작업을 진행**한다.

# 12. 총정리

---

지금까지 다뤘던 내용들을 총 엮어서 설명을 진행한다. 꽤나 복잡한 그림이 엮여 나오니 설명과 함께 따라오도록 노력해보자. 일단 아래 그림에 분홍색 구간은 커널이다. 커널 안에는 여러가지 시스템 콜이 존재하고 있다. 그리고 이 시스템 콜들은 `context_switch()`와 같은 내부함수와 연관이 있으며 각 하드웨어 자원마다 자료구조가 존재(struct CPU)한다. 아래 그림 또한 그림에 오류가 있는 부분이 있는데, 오류가 난 부분은 설명하면서 함께 나오니 너무 걱정할 필요는 없다.

![image-20200811162036018](https://user-images.githubusercontent.com/58545240/90212525-8174d180-de2e-11ea-92ae-0daaa0ae15d6.png)

1. `fork()`를 진행하면 커널로 진입한다. **커널에서 fork( )는 부모 프로세스와 똑같은 image를 생성**한다.
2. 점선으로 표시된 이유는 **아직 CPU 제어가 부모 프로세스에 있기 때문에** 자식 프로세스로 향하는 선은 점선으로 표시가 되어 있다.
3. 그 다음 `fork()` 작업이 끝나고 리턴한다. 앞서 언급했듯 부모 프로세스에서 `fork()`를 실행했을 때의 결과값과 자식 프로세스가 실행했을 때의 결과값은 다르다고 했다. 일단 **첫번째로 리턴되는 건 부모 프로세스의 PID**가 리턴되므 else문으로 가서 **wait( ) 시스템 콜을 호출**한다.
4. `wait()` 시스템 콜의 요청을 처리하기 위해 **또 다시 커널모드로 진입**한다. **wait( )은 CPU를 잠시 포기하겠다는 의미이기 때문에 context_switch( ) 함수를 실행**한다.
5. 그림을 정정해야 한다. `wait()`에서 `context_switch()`로 가는 것이기 때문에 **5번 화살표의 방향은 반대가 되어야**한다. `context_switch()` 함수가 실행되면서, 먼저 **CPU에 있던 state vector 영역에 해당하는 정보를 부모 프로세스의 PCB에 덮어 쓴다(저장한다).** 이렇게 저장을 해야 후에 자식 프로세스의 작업이 끝나고 돌아왔을 때, 부모 프로세스의 PCB에 저장되어 있는 상태값들을 보고 후에 다시 부모 프로세스로 돌아가서 남은 작업들을 원활하게 처리할 수 있다.
6. 그런데 자식 프로세스가 생겨날 때 애초에 부모프로세스에서 `fork()`가 일어나던 시점에 형성된 것이므로, 자식 프로세스의 `PC(Program Counter)`는 `fork()` 중간을 가리키고 있었을 것이다. 따라서 제어흐름은 6번 화살표를 따라 `fork()`로 가게 되고,**자식 프로세스의 시작은 fork( )에서 시작되는 것**이다.
7. 자식 프로세스에서 실행되고 있는 `fork()`의 리턴 값은 당연히 자식 프로세스의 PID일 것이다. 따라서 자식 프로세스가 실행하기로 되어 있는 `exec()`이 호출된다.
8. **exec( )이 해주는 작업은 하드 디스크에 저장되어 있는 프로그램 코드(유저가 exec 시스템 콜의 매개변수로 준 프로그램)를 불러들여 현재 진행되고 있었던 프로세스 이미지 위에 덮어 씌우는 작업**이다.
9. 따라서 디스크에 유저가 `exec()`시스템 콜에 매개변수로 넘긴 `ls`에 해당하는 프로그램이 **현재 진행중이었던 쉘(자식 프로세스) 위에 덮어 씌어지게 된다.**

![image-20200811162058192](https://user-images.githubusercontent.com/58545240/90212534-876ab280-de2e-11ea-8405-4a890757f4e1.png)

10. 덮어씌어진 후에 **ls 프로그램의 main( )으로 흐름이 넘어간다.**

11. `ls`의 코드가 전부 실행된 후 **exit( )이 호출되면서 흐름은 12번으로 넘어간다.**

12. 소스코드 상에 `exit( )`이 존재하지 않아도 컴파일러가 알아서 삽입을 해주기에, `exit()`을 무사히 실행할 수 있다. `exit()`은 지금까지 **진행중었던 프로세스로부터 CPU를 뺐고 다른 프로세스에게 재할당해 주는 과정**이 있기에 마찬가지로 **context_switch( )를 호출하게 된다.**

13. 자신을 호출한 프로세스로부터 **CPU를 뺐고, ready queue에 가서 CPU를 기다리고 있던 프로세스 중 우선순위가 높은 프로세스를 골라서 해당 프로세스의 PCB 안의 상태값들을 현재 CPU의 레지스터에 복사 붙여넣기(복붙) 한다.**
14. ` ready queue`에 부모 프로세스만 남아있다고 가정한다면, 부모 프로세스가 선택되어 실행될 것이고 부모 프로세스는 `wait()`을 진행하고 있었기 때문에 `wait()` 중간부터 다시 실행된다.
15. 14번까지의 작업이 끝났다면 쉘은 다시 사용자로부터 또다른 명령을 기다리고 있게된다.

## 12.1 용어 정리

총정리인 만큼, 기존의 프로그램과 프로세스 차이에 대해서도 한 번 짚어보고 가도록 한다.

![image-20200811162228948](https://user-images.githubusercontent.com/58545240/90212564-9d787300-de2e-11ea-8ae6-fde355072c02.png)

**프로그램이 실행중일 때** 우리는 프로그램을 **프로세스**라 부른다. `a.out` 형식을 가지고 `main()`함수부터 시작하게 되어 있다. 스케쥴링과 보호의 단위이고, 자원을 할당받는 과정을 수반하고 유저모드와 커널모드를 왔다갔다 하면서 진행된다.

![image-20200811162244670](https://user-images.githubusercontent.com/58545240/90212573-a1a49080-de2e-11ea-8f57-bf6bedc0fec3.png)

유저 영역(user space)의 **text는 instruction(명령문)을 의미**한다. `data`와 `bss`에 대한 설명은 위 그림의 Note파트에 서술되어 있다. 먼저, 두 개의 배열(array)이 존재한다. A라는 배열은 초기값을 할당해줬고 B라는 배열에는 초기값을 주지 않았다. 만약 배열의 크기가 100만 정도에 전역변수로 선언되어 있다면? A 배열처럼 초기값을 할당 해줬다면 디스크에서 백만 개의 셀을 갖고 있어야 한다(사전에 자원이 지급됨). 만약 초기값을 주지 않았다면 디스크에 실제로 존재하진 않고 해당 배열이 실행 중 로드 될 때만 할당되게 된다.

**초기에 값이 할당된 부분을 data**라고 하며 **초기에 할당되지 않은 데이터 부분을 bss**라고 한다. **heap은 동적 메모리 할당**에 쓰여지는 데이터 영역이며 **stack은 함수 호출 등에 사용되는 자료구조**다.

커널 영역(kernel space)에는 **PCB와 stack**이 존재한다. HW(CPU) 쪽에서는 **state vector**가 존재한다. 이런 것들을 합쳐서 우리는 **context라고 부른다.**

## Daemon (데몬) 또는 Server

서버(혹은 데몬)는 무엇일까? 근본적으로 서버는 **a.out(실행 파일)이다**. 다만 조금 특이한 알고리즘을 가지고 있을 뿐이다. 아래 그림을 살펴보자.

![image-20200811162320894](https://user-images.githubusercontent.com/58545240/90212581-a5d0ae00-de2e-11ea-9296-cb7e5a5a6e83.png)

맨 처음 서버 혹은 **데몬이 시작되는 건 부팅 될 때(boot time)**다. **부팅하고 나서 대부분의 시간은 잠들어 있다. 요청이 올 때만 해당 요청을 서비스 해주고 서비스가 끝나면 또 잠들게 된다. 이런 프로그램을 우리는 데몬 또는 서버라고 부른다.** 만약 프린트 서버가 존재한다고 하면, 프린트 서버는 말 그대로 프린트 요청이 올 때만 프린트를 해주고 그 이외에는 잠든다. 네트워크 서버 또한 네트워크 요청(연결, 해제 등)이 올 때만 처리하고 그 이외에는 잠든다.

서버라는 것은 하드웨어의 개념이 아니라 **소프트웨어의 개념**인 것이다. **항상 incoming request가 오는지 안 오는지 지켜보고 있으며 서비스가 올 때만 서비스를 해주게 되어 있다.**

![image-20200811162335435](https://user-images.githubusercontent.com/58545240/90212585-a9fccb80-de2e-11ea-9782-7c8925d23216.png)

리눅스 시스템에서 사용되는 **명령어 ps(Process State)를 살펴보자**. 현재 기기에서 **어떤 프로세스가 작동**하고 있는지를 나타낸다. `-e` 옵션의 경우 시스템 프로세스까지 전부 보여주는 명령어다. 웹서버나 네트워크서버 등의 모든 시스템 프로세스의 상태를 보여주는 명령어다.

![image-20200811162351823](https://user-images.githubusercontent.com/58545240/90212589-ad905280-de2e-11ea-8187-5055f00c9169.png)

보통 데몬이나 서버 프로그램의 경우 이름 뒤에 `d`자가 붙는다. `httpd`는 웹에서의 통신에 사용되는 데몬이고, `ftpd`는 파일전송 서버를 나타내는 등 다양한 서버와 데몬이 존재하고 있다.

---

> 생각만큼 크게 어렵지 않았던 3강이다. 결국 모든 프로그램은 알고리즘을 이해하는 것이 전부가 아닐까 하는 생각이 든다. 애초에 프로그램이란 건 논리의 집합이고, 해당 논리대로 작업을 하는 것이니 그 논리만 파악하고 있으면 그 프로그램을 아는 것이니까. 그나저나 강의 노트를 작성하는 건 생각만큼 쉬운 일이 아니라는 걸 다시한 번 체감한다. 내가 듣고 이해하는 것과 다시 누군가에게 풀어서 설명하는 건 천지차이니까.

---

# 13. 복습

---

> 이번 4번째 강의에서는 `fork()`를 통해 프로세스를 생성해 내는 과정에 대해 더 자세히 알아보는 시간을 갖는다. 또 PCB 내용을 분류해 볼 것이며 `fork()`와는 조금 다른 `clone()`에 대해서도 다룰 예정이다. 이번 강의는 부모 프로세스가 어떻게 자식 프로세스를 어떤 과정을 통해서 만들어 내는지를 확실히 알아야 이해할 수 있기에 먼저 지금까지 배운 내용 중 일부분을 복습을 하고 4강을 진행 할 것이다.

---

지금까지 한 내용들은 아래 등장하는 두개의 그림에 잘 정리 되어있다. 그림에 나와있는 순서들을 머리속에 담아만 둘 수 있다면 앞으로 좀 더 심화적인 내용을 이해할 때 큰 도움이 될 것이다. 그럼 지금부터 그림과 함께 설명을 보도록 하자.

![image-20200811162640825](https://user-images.githubusercontent.com/58545240/90212592-b2550680-de2e-11ea-829e-e785933c30c8.png)

가운데에 보라색 박스로 그려져 있는 커널이 있다. 그리고 좌측에 유저가 작성한 프로그램인 쉘이 있다. 또 여기서 살펴볼 프로세스는 쉘 프로세스로 부모(Parent)와 자식(Child) 두개가 존재한다. 실제로 동작할 때는 훨씬 더 많은 프로세스들이 동작하고 있기 때문에 CPU 자원을 바로바로 받지는 못한다는 점을 알아두고 아래 흐름을 살펴보자.

1. 먼저 우리 프로그램에다 `ls` 명령어를 쳤다고 가정하자. 그러면 프로그램은 `ls`라는 자식 프로세스를 만들려고 할 것이다.
2. 그럼 자식을 만들기 위해 먼저 `fork()`를 실행한다. 이때 이 `fork()`는 쉘에 있는게 아니라 커널안에 있는 것이다. 시스템을 직접적으로 다루는 중요한 동작은 모두 커널이 관리한다. `fork()`는 작성된 프로그램과 똑같은 데이터를 복사해 만들어 줄 것이다. 그림에 표시된 점선은 제어흐름이 넘어간다는 뜻이 아니라 단지 데이터만 복사 된다는 뜻이다.
3. 그렇게 `fork()`를 하고나서 다시 돌아와서 **PID 값**을 비교해 보니 자식 프로세스의 pid값이 리턴되었으므로 현재 부모 프로세스 제어흐름에 있다는 뜻이므로 `else`로 간다. `fork()`는 두번 리턴되는데 한번은 부모 프로세스에게 `fork()`로 만들어진 자식 프로세스의 pid값을 넘겨주고 한번은 자식 프로세스에게 0값을 넘겨준다. 자식 프로세스의 제어 흐름에는 0값이 전달된다.
4. 이렇게 `else`로 들어온 부모 프로세스는 시스템 콜인 `wait()`을 호출한다. 이때 `wait()`를 한 이유는 부모 프로세스가 **CPU**를 포기하고 자식 프로세스에게 CPU를 넘겨주기 위한 것이다. 즉 실행흐름을 자식 프로세스에게 넘겨주기 위함이다.
5. 그러면 `wait()`에서 CPU를 넘겨주기 위해 `context_switch()`를 실행하면서 지금까지 동작했던 부모 프로세스의 `state vector`들을 부모 프로세스의 `PCB(Process Control Block)`에 저장한다. 그 후 **CPU**를 기다리고 있는 프로세스들의 정보가 있는 `ready queue`에 가서 우선순위가 제일 높은 프로세스의 `PCB`를 **CPU**에 연결 시켜 준다. 이때 알아야 할 내용은 커널은 유저마다 **커널 스택**을 하나씩 가지고 있다는 점이다. 현재 커널 스택에는 `wait()`와 관련된 지역 변수들이 먼저 들어가 있다. 그리고 그 위에 `context_switch()`에 관련된 지역 변수들이 저장되어 있다. 부모 프로세스의 `PCB`에는 이러한 정보들이 저장되어 있다.
6. **CPU**를 처음으로 넘겨받은 자식 프로세스는 `return`부터 해야하는 상황에 처해있다. 자식프로세스는 만들어 질때 부모 프로세스의 상태정보를 똑같이 복사해 만들어지기 때문에 `fork()`작업을 마무리 하고 있던 부모프로세스의 상황 또한 그대로 복사 되기 때문이다. 그래서 자식 프로세스는 `fork()`로 `return`을 하게 되면서 `fork()`는 두번 리턴한다는 개념이 생겨난 것이다. 단지 이번에는 자식 프로세스의 실행흐름이라는 점이 다르고 리턴된 `pid`값이 0이고 0값을 토대로 `if`와 `else`중 프로그램 내에서 어떤 제어흐름으로 갈지를 결정하게 된다.
7. 리턴된 `pid`값이 0인것을 보면 자식 프로세스라는 뜻이므로 `if`문 안으로 들어가게 된다. 거기서 `exec()`을 하게 된다.
8. 위 그림에서는 `exec()`에 매개변수가 `ls`인 상황이다. 이 명령어는 매개변수로 넘어온 프로그램을 찾고 해당 프로그램 이미지를 로드한다. 따라서 `exec()`이 실행되면서 디스크에 가서 `ls`를 찾는다.
9. 그 후 자식 프로세스쪽에 디스크에서 찾은 `ls`내용을 덮어씌운다. 이로서 자식 프로세스는 더 이상 부모 프로세스의 복제품이 아닌 자신만의 역할을 하는 프로세스로 된다.

![image-20200811162702431](https://user-images.githubusercontent.com/58545240/90212594-b7b25100-de2e-11ea-8c31-ad8b4cd6518b.png)

10. `exec()`을 통해 디스크에서 `ls`를 가져와 현재 이미지(코드)에 덮어씌우고

11. 자식 프로세스는 자신이 할 일을 진행한다. 할 일이란 `ls`가 하는 작업과 동일하다.

12. 일을 다 하고나면 이제 CPU가 필요 없으니 프로세스를 종료하기 위해 시스템 콜 `exit()`을 호출한다.

13. 그럼 이제 또 CPU를 다른 프로세스를 주기 위해 `context_switch()`를 하게 되고 이때 부모 프로세스의 PCB를 불러온다. 그럼 이때 커널의 스택에는 `wait()`와 그 위에 `context_switch()`가 쌓여있는 상태로 있다. 보라색 커널 영역의 그림에는 스택이 반대로 표현되어 있다. 또한 `wait()`과 `context_switch()`사이에 있는 `exec()`과 `exit()`은 중간에 분명 스택에 쌓이긴 했으나 13번 실행흐름 전에 각각 실행이 끝나면서 스택에서 빠져나가 있는 상태다.

14. 마지막으로 스택의 가장 상위에 위치하고 있는 `context_switch()`에서 `wait()`으로, 그리고 `wait()`에서 다시 부모쪽에서 시스템 콜 `wait()`을 호출한 곳으로 돌아가게 된다.

*이렇게 하면 `fork()`의 과정이 끝이난다. 복습을 통하여 부모의 프로세스가 어떻게 자식 프로세스를 생성하고 자식 프로세스는 어떻게 종료되는지에 대해 알아보았다. 그렇다면 이제 본격적으로 `fork()`를 통해 프로세스를 생성하는 과정을 자세히 알아보자.*

# 14. Process Create

---

부모 프로세스가 자식 프로세스를 만들어 내는 작업을 할 때는 두번의 오버헤드(overhead)가 발생한다. 이 오버헤드들은 `fork()`를 하는 도중 발생하며, 첫번째 오버헤드는 부모 프로세스의 이미지를 자식에게 복사할 때 생기고 두번째는 부모 프로세스의 PCB를 자식 프로세스에 복사하며 생긴다.

작업 과정을 자세히 알아보기 전에 먼저 PCB의 구성에 대해 알아보자.

```bash
# Note
오버헤드: 어떤 처리를 하기 위해 들어가는 간접적인 처리 시간, 메모리 등을 말한다.
A라는 처리를 10초만에 했지만 안전성 고려 때문에 처리가 15초가 걸리는 B의 방식은 오버헤드가 5초가 발생한 것이다. 또한 이러한 B의 방식을 개선해 12초가 걸리면 오버헤드가 3초 단축되었다고 말한다.
```

## 14.1 리눅스의 PCB와 Thread

![image-20200811162926413](https://user-images.githubusercontent.com/58545240/90212619-cdc01180-de2e-11ea-955b-468da4afae14.png)

PCB에는 다양한 정보들이 수 킬로바이트라는 꽤 큰 용량으로 구성되어 있다. 이러한 PCB의 내용을 분류를 하자면 **task basic info**와 프로세스가 오픈한 파일들에 대한 정보가 들어있는 **file**, 프로세스가 접근 중인 file system에 대한 정보인 **fs**가 있으며 프로세스가 사용 중인 터미널 정보 **tty,** 사용 중인 메인 메모리에 대한 정보 **mm**과 여러 신호 정보인 **signals**들로 나눌 수 있다.

리눅스는 이렇게 분류된 요소들을 하나의 구조(Struct)로 묶지 않고 그림 오른쪽에 나와 있는 것 처럼 **6개의 구조**로 나눠 관리한다.

![image-20200811162940267](https://user-images.githubusercontent.com/58545240/90212629-d284c580-de2e-11ea-84a0-3674088a269e.png)

먼저 그림의 왼쪽 상자에 나와있는 것처럼 크게 `task_struct`가 있다. 이건 리눅스가 가지고 있는 `PCB`인데 그 안에는 여러개의 `struct`들에 대한 내용이 있고 그 옆에 보라색으로 `*mm, *tty등`이 있는 것을 알 수 있다. 보라색으로 표시되어 있는 **포인터(\*)**를 따라가면 각각이 가르키는 파일, 메모리를 등을 만나볼 수 있다. 이는 오른쪽 그림에도 나와있는데, 그림을 보면 왼쪽 노란 상자에 `task basic info`가 있고 그곳에서 화살표로 가르키는 곳을 따라가면 각각의 구조(struct)들이 나온다. 이처럼 **리눅스의 PCB**는**1개의 구조가 아닌 6개**의 구조로 나눠져 있다.

그렇다면 리눅스는 어째서 1개가 아닌 6개로 나눠서 관리하는 것일까.

![image-20200811162955337](https://user-images.githubusercontent.com/58545240/90212637-d6184c80-de2e-11ea-83ca-44eb4c0f10cd.png)

위 그림의 왼쪽에 있는 노란 상자들을 보자. **저 6개의 상자들이 있어야 하나의 PCB**이다. 전에 `fork()`를 통해 자식 프로세스를 생성하고 이때 부모 프로세스의 정보를 그대로 복사한다고 했는데 그 정보가 바로 위 그림에 나와 있는 정보다.

그렇다면 부모 프로세스의 노란색 상자 6개 구성요소가 전부 자식 프로세스에게 복사되는 것일까? 만약 그렇게 `fork()`가 동작한다면 `files, fs, tty, mm, signals` 등을 각각 읽고 쓰는데 많은 자원이 사용된다. **모든 걸 복사**해서 자식 프로세스를 만든다고 했을 때 부모 프로세스의 PCB 정보를 `read()`할 때 사용되는 바이트, 자식 프로세스의 PCB에 `write()`할 때 필요한 바이트가 각각 필요하므로 상당한 부하가 걸린다. 따라서 이러한 제작 방식을 **heavy-weight creation**이라 칭한다. 초기 리눅스가 구현될 때는 이런 방식으로 구현되었다고 한다.

그러나 막상 시스템을 만들다보니 부모 프로세스가 가지고 있는 `tty(터미널)`나 `fs(파일 시스템)`는 자식이 가지고 있는 것과 동일한 경우가 많다. 즉 복사하는 게 아니라 공유를 할 수 있다. 자식 프로세스에게는 부모 프로세스가 가지고 있는 `tty`나 `fs`등의 주소만 알려줘서 같은 자원을 공유하는 방식으로 생성되는 것을 **light-weight creation** 칭한다.

자식 프로세스가 부모 프로세스와 다르게 사용할 것들만 **선택적으로 복사**하자라는 아이디어로 구현한 이 방식은 전부 복사할 때 들어가는 하드웨어 자원과 오버헤드를 최소화 시키는 장점이 있다.

자, 그럼 위에서 배운 개념을 바탕으로 좀 더 구체적인 예시를 들고 이해해보자. 게임을 만든다는 상황을 가정해보자.

![image-20200811163014029](https://user-images.githubusercontent.com/58545240/90212643-d9abd380-de2e-11ea-97b9-b47d69b560b4.png)

위 그림을 보면 정 가운데에 검은 네모 상자는 메인 메모리를 뜻한다. 맨 위의 박스들은 여러개의 CPU를 표현하고 있고 CPU마다 각각 프로그램 카운터를 내장하고 있다. 이런 상황에서 `Game XYZ`가 실행되고 있다고 가정하자. 이 게임은 지금 `CPU #0`위에서 실행되고 있고 `Game XYZ` 프로세스를 `CPU #0`의 `PC(프로그램 카운터)`가 가르키고 있다. 그리고 각 `CPU`를 위해서 `PCB`가 좌측에 노란 상자로 존재하고 있다. 각 `PCB`는 6개의 구성요소로 되어 있다.

이런 상황에서 자식 프로세스를 전통적인 방법으로 만들었다고 생각해 보자. 그럼 `a.out`도 복사하고 `PCB`도 똑같이 복사해서 자식 프로세스를 만들 것이다. 이렇게 되면 위에서 말했던 것처럼 오버헤드가 발생한다. 어떻게 하면 오버헤드를 줄여줄 수 있을까?

```bash
# Note
`프로그램 카운터(Program counter,PC)`: 마이크로프로세(중앙 처리 장) 내부에 있는 레지스터 중의 하나로서, 다음에 실행될 명령어의 주소를 가지고 있어 실행할 기계어 코드의 위치를 지정한다. 때문에 명령어 포인터 라고도 한다.

`프로세서 레지스터(Processor Register)`: 컴퓨터의 프로세서 안에서 자료를 보관하는 아주 빠른 기억 장소이다. 일반적으로 현재 계산을 수행중인 값을 저장하는 데 사용된다.

`a.out`: 과거 유닉스 계통 운영 체제에서 사용하던실행 파일과목적 파일형식. assembler out의 약자이다.
```

![image-20200811163107160](https://user-images.githubusercontent.com/58545240/90212649-dd3f5a80-de2e-11ea-8518-cbe39fe60775.png)

오버헤드를 줄이기 위해 고안된 방법이 바로 자식 프로세스를 생성할 때 프로세스로 만들지 않고 **Thread**로 만드는 것이다. **Thread**는 모든 구조를 복사해 오는게 아니라 **CPU관련 정보들을 가지고 있는 Task basic info만 복사해 오는것을 칭한다.** 아래 그림을 보면 자식들은 부모의 구조들을 전부 복사해오지 않고 **Task basic info**만 복사해 와서 나머지는 부모와 공유해 사용한다. 이러한 방식을 light-weight creation이라고 한다.

```bash
# Note
`프로세스`와 `스레드`의 차이: 프로세스는 운영 체제로부터자원을 할당받는 작업의 단위이고 스레드는 프로세스가 할당받은자원을 이용하는 실행의 단위이다.
```

![image-20200811163140061](https://user-images.githubusercontent.com/58545240/90212654-e0d2e180-de2e-11ea-9db9-920535f39bf7.png)

위의 내용들을 정리해보면, 리눅스에서 **Thread**는 `PCB`에서 **Task basic info만 복사**해오고 다른 PCB 데이터는 **공유**를 한다. 덕분에 데이터의 복사는 줄고 자식 프로세스를 만들때의 오버 헤드가 최소화 된다. 그래서 리눅스에서의 **Thread**라는 것은 프로세스를 만들때 **light-weight 방식**으로 만든다고 한다. 그리고 이런 방식은 단순히 복사를 하는 `fork()`가 아닌 `**clone()**`이라는 시스템 콜을 사용한다.

![image-20200811163202711](https://user-images.githubusercontent.com/58545240/90212665-e4666880-de2e-11ea-9da6-cba562ccfa6e.png)

`clone()` 시스템 콜을 살펴보기 위해 위의 그림을 살펴보자. 가운데 `clone()` 시스템 콜이 있다. `clone()`을 호출할 때 부모 프로세스는 바이너리 비트 5개를 매개변수로 넘긴다. 만약 **이 5개의 비트가 전부 11111이면 모든걸 복사하고 00000이면 Task basic info만 복사해오는 제일 light-weight 복사 방식을 하라는 것을 뜻한다.** 이러한 방식으로 생성된 자식 프로세스는 프로세스라 하지 않고 **Thread(스레드)** 칭한다.

여기서 만약 `clone()`의 **바이너리 비트 5개가 clone(11111)이면 모든 걸 복사하는 전통적인 heavy-weight 방식인 fork()를 해달라는 의미와 같은 뜻이 된다.** 이렇게 생성된 자식 프로세스는 프로세스가 된다.

```bash
# Note
Unlike fork(2), these calls allow the child process to share parts of its execution context with the calling process.
```

# 15. Process Copy

---

지금까지는 프로세스가 생성 되는 과정에 대해서 알아보았다. 부모 프로세스가 자식 프로세스를 생성 할 때 두가지 오버 헤드가 생긴다는 것을 배웠다. **첫번째는 PCB를 복사할 때 생기고 두번째는 image를 복사 할 때 생긴다.** **PCB는 하얀색 도화지의 속성(크기, 질감, 모양)이라면 image는 그 도화지 위에 색칠된 그림이라고 볼 수 있다.** 그렇기에 PCB보다는 **image를 복사해 오는 오버 헤드가 더 크게 발생한다.**

부모 프로세스한테 `ls`를 명령하면 바로 자식 프로세스가 자신만의 속성을 갖고 생성되는 것이 아니라, 먼저 부모 프로세스의 상태 정보를 복사하고 그 위에 자식 프로세스가 갖는 속성을 덮어 씌운다. 근데 생각해보면 이런 과정 자체가 너무 비효율적인 과정이라고 생각 할 수도 있다. **어차피 덮어씌울 걸 왜 굳이 부모의 image까지 복사하는 과정이 필요한 것일까.**

물론 위와 같은 완전한 복사가 항상 비효율적이라는 것은 아니다. 어떤 유저는 부모 프로세스가 가지고 있던걸 정확히 똑같이 복사하고 싶어할 수도 있다. 예를 들어 hwp 문서를 키고 또 똑같은 hwp 문서를 새로 키고 싶어하는 경우도 있을 수 있기 때문이다. 그러나 대부분은 이메일을 키고 거기서 이메일을 쓰는 일을 하듯이 부모 프로세스와는 다른 일 처리를 하는 경우가 대부분이다.

그래서 고안된 아이디어는 다음과 같다. 모든 코드를 복사해 오는 것이 아니라 **페이지 매핑 테이블만 복사**해 오는 방법이다. 이 방법을 사용하면 **자식 프로세스는 image를 부모 프로세스로부터 가져오는 것이 아니라 부모 프로세스의 image를 가르키는 페이지 매핑 테이블만 복사해서 가져오게 된다.** 자식 프로세스는 페이지 매핑 테이블을 가지고 execute를 하게 되고 Instruction(실행 명령)을 가져오는 동안에는 부모와 **같은 페이지**를 쓸 수 있게 된다.

```bash
# Note
`페이지 테이블`: 페이징 기법에 사용되는 자료구조로서, 프로세스의 페이지 정보를 저장하고 있는 테이블이다. 테이블 내용은 해당 페이지에 할당된 물리 메모리의 시작 주소를 담고있다.

`페이징 기법`: 컴퓨터가 메인 메모리에서 사용하기 위해 2차 기억 장로부터 데이터를 저장하고 검색하는 메모리 관리 기법
```

그런데 이렇게 같은 페이지를 사용하다 보면 문제가 생기는 경우가 발생한다. 자식과 부모 프로세스 둘 다 페이지에서 `read()`해서 데이터를 읽는건 상관이 없지만, 만약 페이지에 **write()**를 해서 무언가를 페이지에 쓰게 된다면 어떻게 될까.

`write()`를 하는 경우에만 한정해서 그 **페이지만 부모와 자식에게 하나씩 복사**본을 따로 만들어 주면 된다. 이런 과정을 **Copy on Write(COW)**라고 부른다. 아래 그림을 살펴보자.

![image-20200811163304453](https://user-images.githubusercontent.com/58545240/90212683-e9c3b300-de2e-11ea-96e8-4d9391a7534b.png)

**Copy On Write**방식을 이용하면 처음 시작할 때 자식 프로세스는 페이지 테이블만 가지고 있을뿐 독자적인 image라는 것은 없다. **그러나 부모나 자식 프로세스중 하나라도 페이지에 변화를 주면 그 부분만 각각 복사를 하게 된다.** 이런 방식은 image를 약간은 게으르게(lazy) 만들어 주는 방식으로 볼 수도 있을 것이다.

위 그림을 보면서 다음 설명을 같이 따라가 보자. 먼저 부모 프로세스쪽에서 `fork()`를 하게 된다. 부모 프로세스 정보를 전부 복사 하는 것이 아니라 **COW 방식으로 페이지 테이블만 복사해 가져온다.** 그리고 나서 `fork()`를 했던 곳으로 돌아온다. 그리고 나서 `wait()`시스템 콜을 호출해서 CPU를 자식 프로세스에게 넘겨주려 할 것이다. 그럼 CPU가 자식 프로세스에게 넘어가서 자식도 `fork()`로부터 리턴해서 자식 프로세스만의 작업들을 수행할 것이다. 이때 자식이 페이지를 읽어 오는건 상관없지만 **write()를 하게 되면 그 페이지에 대해서만 복사를 한다.**

그런데 보통 자식 프로세스는 `fork()`에서 돌아오면 거의 바로 `exec()`을 하게 된다. 즉 전에 부모 프로세스의 이미지를 복사 했든 안했든 자신만의 이미지(코드)로 싹 갈아엎는다. 여기서 문제가 발생하는데, 부모 프로세스가 `fork()`를 하고나서 돌아오고 나서 문제가 생긴다. 부모 프로세스가 `fork()`에서 돌아와서**바로 wait()를 안하고 다른 일을 처리할 경우 3분의 1 정도는 보통 write()의 기능을 한다.** 그 말인 즉슨 자식 프로세스가 CPU를 점유하기 전에 페이지 테이블에 계속 변화가 발생하게 되는 것이다.

이런 행위는 계속해서 **Copy On Write**를 하게 될 것이고 **이렇게 복사 된 값들은 사실 자식 프로세스가 exec()을 하게 되면 어차피 덮어 씌워지기 때문에 결국에는 의미없는 복사를 하고 있는게 된다.** 그렇다면 어떻게 하는 것이 효율적인 방법일까. 아래 그림을 살펴보자.



![image-20200811163323148](https://user-images.githubusercontent.com/58545240/90212737-0b249f00-de2f-11ea-92a5-d94c8e1ef857.png)

**그림의 위쪽 부분은 위에서 설명한 불필요한 COW가 발생하는 경우를 나타낸 것이다.** `fork()`로 돌아온 부모 프로세스가 자식 프로세스에게 `wait()`으로 CPU를 넘겨줄 때 까지 계속해서 의미없는 COW를 만들어 내고 있는 그림이다.

이를 해결하기 위해서 다음과 같은 새로운 방법을 사용한다.

1. 부모 프로세스가 `fork()`를 호출해서 자식 생성을 끝내고 `fork()`를 했던 곳으로 돌아가려 한다. 즉 커널에서 유저모드로 돌아가려 하고 있다.
2. 이때 `fork()`안에서 **자식 프로세스의 CPU 우선 순위를 확 높여버린다.** 이렇게 우선순위를 높이는 이유는 커널에서 유저모드로 돌아갈때에는 우선순위가 제일 높은 프로세스한테 CPU를 넘겨주기 때문이다.
3. **이렇게 되면 CPU가 부모한테 돌아가는 것이 아니라 자식 프로세스한테 가게 된다.** 이렇게 CPU를 받은 자식 프로세스는 바로 `exec()`을 하게 되고 CPU를 다 쓰게 되면 `exit()`으로 CPU를 다음 순서로 넘겨주게 된다.
4. 그럼 이제 부모 프로세스가 CPU를 받게되고 `fork()`에서 돌아오고 본인이 할 일을 하게된다.

**이러한 방식을 통해 쓸데없이 일어나는 COW를 방지하고 자식 프로세스는 성공적으로 복사된다.**

---

> 4강에서 핵심을 뽑자면 아래의 2가지를 꼽을 수 있겠다.
>
> 1. 리눅스가 PCB를 6개의 구조로 나누어서 관리한다는 것과 PCB를 전부 복사하지 않고 필요한 것들만 복사하는 것을 **Thread(Light Weight Creation)**라고 한다는 점.
> 2. 복사한 페이징 매핑 테이블의 불필요한 Copy On Write를 방지하기 위해 `fork()`에서 돌아올때 부모가 아닌 자식에게 먼저 CPU를 줘 오버 헤드를 막는점.
>
> 고건 교수님께서 친절하게 `fork()`의 과정까지 복습을 해주시고 나서 심화 내용을 들어가 차근차근 이해하기 쉽게 강의를 따라갈 수 있었던 것 같다.

---

# 16. Kernel Thread(커널 스레드)

---

> 이번 5번째 강의에서는 프로세스간 CPU 점유권의 이동이 어떤 매커니즘으로 이루어지는지를 다루게 된다. CPU를 할당해준다는 것은 단순히 프로세스의 우선순위 말고도 고려해야할 것들이 많다. 리눅스 운영체제는 과연 이러한 숙제를 어떻게 풀고 있는지 지금부터 살펴보려 한다.

---

먼저 강의노트 4에서 다뤘던 내용들을 잠시 떠올려보자. **스레드(Thread)가 있고 프로세스(Process)**가 있었다. **프로세스**는 **부모의 것(Task basic info + files, fs, tty, mm, signals)을 전부 그대로 복사**한 것(heavy-weight creation)이고,**반대로 최소한으로 복사(light-weight creation)한 것이 스레드**이다.

또한 커널은 **메모리 상주 프로그램(memory resident program)**이다. `main()`함수가 있는 **평범한 프로그램의 특성 +부팅할 때 부터 메모리에 올라와서 컴퓨터의 전원을 완전히 차단할 때까지 메모리에 상주**한다는 특성을 함께 가지고 있다. 여기까지 떠올렸다면 이제 아래 그림을 보면서 오늘 다룰 주제 중 하나인 **커널 스레드**를 살펴보자.

![image-20200811163518896](https://user-images.githubusercontent.com/58545240/90212742-0f50bc80-de2f-11ea-99cd-54da4747f672.png)

위 그림의 우측 보라색으로 칠해진 부분은 커널영역을 의미한다. 컴퓨터가 **맨 처음 부팅(booting)하면 분명 커널의** `**main()**`**부터** 실행할 것이다. 그 후 커널 프로세스가 동작하던 중 커널 프로세스 내에서 **시스템 콜** `**clone()**`**을 호출**하게 되면 자식 프로세스가 생기는데, 이때 **부모 프로세스가 가르키는 PC(Program Counter) 와 자식 프로세스가 가리키는 PC는 각각 다른 곳**을 가리키고 있을 수 있다.

그래서 만약 CPU 코어가 3개가 있다고 한다면, **각 CPU코어의 PC는** `**main()**`**과** `**f1()**`**과** `**f2()**`**를 가리키고 있을 수 있다**(CPU dispatched for child & starts here). 그렇게 만들어진 **자식 프로세스를 커널 스레드**라 한다. 이는**자식 프로세스들이 커널 코드를 실행(execute)하고 있기 때문**이다. 대부분의 경우 이런 함수들은 **서버(server) 혹은 데몬(daemon)**이다.

서버와 데몬의 알고리즘은 **기본적으로 무한 루프(endless loop)**라고 우린 배웠다. 또한 서버와 데몬은 **대부분의 시간을 자면서(sleep)** 보낸다. 그러다 **요청(request)이 오면 깨어나 그 작업을 처리해주고 또 잔다.** 네트워크와 관련된 작업을 처리하는 데몬이라면 네트워크 서버라고 부를 수 있고, 만약 프린트 요청을 기다리고 있다면 프린트 서버라고 할 수 있다.

결국 커널 스레드는 **커널 프로세스가** **`clone()`**을 호출해서 light weight overhead로 자식을 만들어준 것**이다. 또한 커널 메모리 영역과 코드를 똑같이 접근하고, 커널 코드를 실행한다. 당연하게도 커널 스레드는 커널 영역에만 존재한다. 많은 데몬(웹서버, 프린트 등)들이 커널 스레드이다.

![image-20200811163537777](https://user-images.githubusercontent.com/58545240/90212750-12e44380-de2f-11ea-8ee3-574a1ced591b.png)

위 그림을 보면 CPU가 여러개 있고 **CPU 마다 PC(Program Counter)를 가지고 있다.** 지금은 PC가 커널을 가리키고 있다. 여기서 커널이 `clone()`을 통하여 스레드 2개를 만들어줬다고 가정해보자. **CPU #2에 할당된 것은 프린트 데몬(서버)**이고, **#3에 할당된 것은 PageFault 데몬**이다. 앞 장에서 다뤘듯 **스레드는 위 그림의 우측 노란박스에 해당하는 Task basic info 파트만 복사를 하고 나머진 부모 프로세스와 공유**를 한다.

**Task basic info 안에는 state vector save area가 존재**하기 때문에**각 스레드마다 별도의 Program Counter와 Stack Pointer를 갖고 있을 수 있는 것**이다. 각 스레드가 **각자의 Stack**을 갖고 있기 때문에 **개별적으로 커널 내의 다른 함수들을 호출하면서 실행**될 수가 있다.

# 17. Process State

---

프로세스의 상태에는 **ready, running, waiting이 존재**한다. **running은 프로세스 입장에서는 최상의 상태**이며, running 중 Disk I/O를 요구하는 사건이 발생하면 CPU가 해당 프로세스의 상태를 **waiting**으로 바꾼다. waiting의 경우 시그널의 상황에 따라 2가지의 반응이 있을 수 있는데 이 부분은 중요한 내용은 아니므로 넘어가도록 한다.

![image-20200811163620597](https://user-images.githubusercontent.com/58545240/90212756-17a8f780-de2f-11ea-85f7-bd7bca6b5c98.png)

**I/O가 끝나고 상태는 wait에서 ready**로 넘어가게 된다(CPU는 항상 바쁘다). **I/O가 끝나고 ready list에 참여해서 기다리다 보면 자신의 차례가 올 것**이다. 차례가 오는 것을 **Scheduler dispatches**라고 표현한다. dispatch과정을 상세하게 풀어보자면, `**context_switch()**`**의 동작으로 설명**할 수 있다.

첫번째로 현재 사용중이던 프로세스의 `state vector`를 저장한다. 그리고나서 **run하고 싶은 프로세스의 state vector를 CPU에 로드한 후, Program Counter가 가리키는 곳으로 가는 것이 dispatch의 과정**이다.

CPU가 주어지면 **어느 정도의 시간(time slice)만큼만 동작하고, 시간이 끝나면 다시 ready list로 가는 구조**인데, 일단 프로세스가 성공적으로 `exit()`했다고 가정해보자. 프로세스가 `exit()`을 하게 되면 **zombie 상태**가 된다.

좀비 상태라는 것은 `a.out`도 날라가고 `file`도 전부 `closed`되고 메인 메모리도 다 뺐기고, `PCB`만 남은 상태를 의미한다. 왜 PCB가 남아있는 것일까?

만약 부모 프로세스(parent)가 지금까지 기다리고 있었으면, parent가 깨어나 CPU를 쥐고 실행될 것이다. **이 때 parent는 자신이 잘 동안child가 뭘 했는지, 디스크와 CPU를 얼마나 썼는지, 제대로 끝났는지 등에 대한 내용을 child의 PCB에서 확인한다.**

위와 같은 중요한 정보들이 `PCB`에 있기 때문에 `PCB`는 남겨둬야 한다. **따라서 자식 프로세스의 PCB는 부모 프로세스가 말소시키는 것이 맞다. parent가 말소시킬 때까지는 자식은 zombie상태인 것이다. 따라서 최상위 부모 프로세스는 자식 프로세스들이 사용한 모든 자원을 전부 파악할 수 있어야 한다.**

# 18. Kernel Scheduling (커널 스케쥴링)

---

![image-20200811163649168](https://user-images.githubusercontent.com/58545240/90212766-1bd51500-de2f-11ea-9b70-5cbc30a56da4.png)

리눅스에서는 어떤 프로세스가 다음에 실행될 프로세스일까? 물론 **priority(우선순위)**가 가장 큰 프로세스가 실행될 것이다. 하지만 **time slice를 가지고 있는지**도 반드시 확인해야 한다.

타임 슬라이스(time slice)에 대한 설명은 아래 그림을 보면서 살펴보도록 한다.

![image-20200811163702784](https://user-images.githubusercontent.com/58545240/90212771-20013280-de2f-11ea-9930-d071769b1e64.png)

먼저 위쪽 네모박스 안의 내용을 살펴보자. **CPU가 어떤 프로세스에게 할당될 때는 일종의 시간제한**이 있게 된다. 위 예시에서는 100ms라고 되어 있다. 하지만 안타깝게도 이마저도 못쓰게 되는 경우가 발생할 수 있다. 현재 프로세스보다도 더 급한 작업이 요구되었을 때, CPU를 또다시 빼앗길 수 있다.

위 네모박스 안에서는 **20ms를 사용하다가 CPU를 뺏겨버렸고 남은 80ms는 추후에 다시 CPU를 받았을 때 사용**해야 한다. 여기서 **남은 80ms를 remaining timeslice**라고 한다.

즉, 리눅스에서 프로세스가 CPU를 차지하기 위해서는 **우선순위가 높아야 하고 남은 타임슬라이스가 0보다 커야 한다.** 아래 그림을 보면서 스케쥴링이 정확하게 어떻게 이루어지는지 알아보자.

![image-20200811163720250](https://user-images.githubusercontent.com/58545240/90212774-242d5000-de2f-11ea-84ff-e971a5e9ca36.png)

**먼저 맨 위의 레디큐(Ready Queue)**를 살펴보자. 레디큐에는 **PCB가 여러개 들어 있다. 바로 실행(run)할 수 있는 작업들이 쭈욱 연결**되어 있는 것이다.

이런 구조에서의 **문제점은 멀티 프로세서 시스템이 되서 CPU의 갯수가 증가하게 되면 연결된 머신의 수에 따라 레디큐에 연결되는 PCB도 기하급수적으로 많아질 것**이라는 데 있다. 500개의 프로세스가 돌고 있다고 가정한다면, `context_switch()`를 실행할 때마다 이 500개에 해당하는 레디큐의 내용을 전부 뒤져서 우선순위가 높은 프로세스를 골라내야 하는데, 탐색 작업이 상당히 비효율적(시간이 오래걸림)이라는 문제가 생긴다.

그래서 이번에는 **위 그림의 중간에 있는 구조(큐를 두 개로 나눈 구조)로 설계**를 해 보았다. 하나의 레디큐로 두는 것은 탐색하는데 비효율적이라고 생각으로부터 나온 설계다. **높은 우선순위와 낮은 우선순위를 가지는 두 개의 큐를 제작**했다. 하나의 큐 보다는 확실히 효율적이겠지만, 이마저도 만족스럽지 않기에 **위 그림의 맨 아래와 같은 설계(여러 개의 큐를 두는 구조)**를 해봤다. **좀 더 분류를 세분화해서 여러 개의 큐**를 만들었다. 각 큐는 `PCB`를 가리키는 포인터로 이루어져 있다.

하지만 주황색으로 구현된 큐를 보니 중간 중간 비어 있는 큐(2번과 4번 큐)도 보인다. 이러한 큐까지 탐색할 필요는 없으니 좀 더 효율적으로 탐색을 해볼 수 있지 않을까? 라는 생각을 할 수 있다.

따라서 아래 그림과 같이 **해당 우선순위에 해당하는 큐가 비었는지 안 비었는지를 체크할 수 있는 비트 배열을**하나 더 두게 된다.

![image-20200811163734972](https://user-images.githubusercontent.com/58545240/90212784-27c0d700-de2f-11ea-82e6-67bdfef4d452.png)

위 그림의 좌측에는 바이너리 배열이 하나 있는데, **0은 해당 인덱스의 포인터를 따라가면 해당 인덱스의 큐가 비어 있다**는 뜻이고 **1이라면 해당 인덱스의 큐에 내용물이 있다**는 뜻이다. **유닉스에서는 이 바이너리 배열을 비트맵**이라 부른다.

이렇게 비트로된 배열을 사용함으로써 **탐색 속도가 향상**될 수 있다. 이 비트맵의 인덱스가 얼마나 존재하는지가 시스템에서 다루는 난이도가 얼마나 세분화되어 있는지를 나타낸다. 132개의 인덱스가 존재한다고 가정한다면, 우선순위가 132개로 나뉘어진다는 뜻이다. **우측에는 큐로 이루어진 배열**이 그 구현체다. **해당 큐의 각 내용물은 PCB**로 이루어져 있다.

위 그림의 맨 밑을 보면 **구조체가 하나 존재**한다. 그 구조체 **안에는 비트맵과 큐 배열**이 존재한다. 멀티 프로세서 시스템에서 CPU가 10개 있다고 한다면, 이 구조체가 **각 CPU마다 존재**한다고 생각하면 된다. 비트맵과 큐 배열을 함께 포함하고 있는 구조체가 바로 **priority array (우선순위 배열)**이다. 아래 그림을 살펴보자.

![image-20200811163749527](https://user-images.githubusercontent.com/58545240/90212828-4030f180-de2f-11ea-8523-694eaaec6e95.png)

CPU 스케쥴러가 `context_switch()`가 일어날 때마다, 레디큐에서 우선순위가 가장 높은 프로세스를 뽑아야 하는데 이때 해야 할 작업은 **비트맵을 스캔하고 0이 아닌 항목이 있다면, 해당 난이도의 큐를 찾아내서 큐의 작업내역을 순회하며 실행**하는 것이다. 0이 아닌 비트맵이 있다면 포인터를 따라가서 해당 큐의 작업 내역을 실행하면 된다. 아래 그림을 살펴보자.

![image-20200811163802908](https://user-images.githubusercontent.com/58545240/90212830-43c47880-de2f-11ea-900c-069185b1e45e.png)

**레디 큐라는 것은 ready to run a cpu**를 의미한다. 즉 **바로 실행될 수 있는 프로세스들을 모아둔 큐**이다. 하지만 만약 레디 큐에 있던 프로세스가 자신에게 **할당된 타임 슬라이스를 다 썼다면어떻게 해야할까? 타임 슬라이스를 다 사용한 프로세스는 위의 그림에서 나타난노란색 영역(Expired array)으로 빠지게 된다.** 다음 타임 슬라이스를 배정받기 전까지는 레디 큐에 있을 자격이 없기 때문에 **Expired array**에서 대기하게 된다. **이 두개의 array는 각 CPU마다 존재한다.**

![image-20200811163820776](https://user-images.githubusercontent.com/58545240/90212835-46bf6900-de2f-11ea-9447-2917cf6e3eb0.png)

좀 더 정확하게 설명을 하자면, **Active 영역에 있는 작업들이 모두 처리된 후 Expired 영역으로 가게 된 프로세스들은 일률적으로 (한 번에) 타임 슬라이스를 배정**받게 된다. 그 뒤 **Expired 영역이 Active 영역으로, 기존의 Active 영역은 Expired 영역으로 변환(interchange)된다.** 이 변환 과정은 단순히 **서로가 가리키는 포인터가 바뀌는 작업을 의미**하는데 구현 코드는 아래 그림의 맨 아래 네모박스에 나와있다.

![image-20200811163837483](https://user-images.githubusercontent.com/58545240/90212842-4a52f000-de2f-11ea-95db-39672a7d9505.png)

# 19. Kernel Preemption

---

![image-20200811163854332](https://user-images.githubusercontent.com/58545240/90212850-4de67700-de2f-11ea-8586-a0b861cbd1c5.png)

## 19.1 Mutual Exclusion — 상호 배제

컴퓨터 시스템을 얘기할 때 가장 중요한 파트 중 하나가 **상호 배제 문제**이다. 시스템이 정상적으로 작동하기 위해서는 이 상호 배제 개념은 반드시 필요하다.

설명을 진행하기 전, 먼저 `X++`이라는 연산이 정확하게 어떻게 이루어져 있는지부터 이해하고 가자. 우리가 보통 프로그래밍 언어를 사용할 때 `X++`과 같은 단항연산자를 사용하면, 하나의 명령만으로 덧셈이 정상적으로 이루어지는 것 같지만 실제로 동작하는 기계 입장에서 이 단항연산 과정은 3단계로 나누어진다. 그 과정은 아래와 같다.

1. `X`를 저장소로부터 읽어서 `CPU 레지스터`로 읽어들인다.
2. `CPU` 안에서 `ALU` 연산을 진행한다.
3. `CPU` 로부터 나온 결과를 다시 저장소에 쓴다.

먼저 우리가 흔히 사용하는 **X와 같은 변수 또는 데이터는 항상 저장소(storage)에 존재**한다. `CPU`안에는 저장시킬 수 있는 용량이 얼마 없고 또 비싸기 때문에 `CPU`안에 많은 변수와 데이터를 저장할 수는 없다. 때문에 우리가 흔히 알고 있는 컴퓨터 시스템에서 **데이터는 저장소에 저장되고 연산은 CPU에서 이루어지게 되는 것**이다. 이 개념을 먼저 숙지한 후 아래의 그림과 함께 설명을 보자.

![image-20200811163924928](https://user-images.githubusercontent.com/58545240/90212857-5179fe00-de2f-11ea-868a-2e0dc93f91b9.png)

위 그림을 보면 두 개의 프로세스가 존재하고, 이 두 프로세스는 **한 개의 변수 X를 공유**하고 있다. 프로세스 A가 먼저 `X`에 대한 `X++`를 연산한 후 메모리에 저장하고, 그 다음 우측의 프로세스 B가 변수 `X`를 레지스터로 읽어들여 연산처리를 한 후 디스크에 저장한다. 이렇게만 한다면, `X`라는 변수는 정상적으로 `11 -> 12 -> 13`순으로 디스크에 저장될 것이다.

그런데 만약 이 두 프로세스가 `X`를 동시에 읽어들여 연산할 경우에는 문제가 발생하게 된다. A가 `X`를 읽은 후에 `++`연산을 진행하는 도중 B가 `X`를 읽어들였다고 가정해보자. A는 아직 더한 값을 디스크에 쓰지 않았기 때문에 아직 `X`는 초기값 `11` 그대로다. 따라서 B가 읽어들인 `X`값은 `11`이다.

여기서 A가 값을 `12`로 증가시킨 후 디스크에 `12`를 기록했다고 해도, 결국 B또한 `12`로 증가시킨 후에 기록하기 때문에 덧셈은 한 번밖에 일어나지 않는다. **두 번의 덧셈연산이 제대로 동작하지 않은 것이다.** 이처럼 프로세스 간 **공유된 변수를 접근하는 부분을 Critical Section**이라 부른다.

크리티컬 섹션에는 하나의 프로세스만 접근해야만 한다. 그래야 위와 같은 오류가 발생하지 않는다. **이러한 원칙이 바로 상호 배제(Mutual Exclusion)의 원칙**이다. 크리티컬 세션은 하나의 프로세스만 접근이 가능하다. 또 그래야만 한다.

유닉스는 지난 40년간 이러한 문제를 어떻게 해결했을까? 그 방법은 매우 단순하게도, **커널모드인 경우에는 CPU를 뺐지 않고 유저모드일 경우에만 CPU를 뺐는다. 커널에 있을 때는 CPU preemption을 고려하지 않아도 된다.**

하지만 이러한 설계에는 문제가 있다. **중요한 작업이 도중에 발생했다고 해도, Kernel 모드이기 때문에 CPU를 다른 곳에 할당하지 못한다면 리얼타임시스템(real-time system)과 같이 빠른 처리와 빠른 전환이 어려워진다.** 커널이 작업중임에도 CPU를 가져올 수 있어야 진정한 리얼타임시스템이 가능하다. 이 부분을 어떻게 해결할지가 아래 그림에 나와있다.

![image-20200811163941105](https://user-images.githubusercontent.com/58545240/90212865-550d8500-de2f-11ea-962c-b63c39455b6a.png)

위 그림의 좌측 상단을 보면, `쉘(sh)`이 `read()` 시스템 콜을 호출하고 있다. 함수 안에 변수들이 쓰이면서 스택에 이러한 변수들이 담기게 된다. 우측의 `mail` 프로그램에서도 `send()` 시스템 콜을 호출하고, 이 호출 또한 지역변수를 자신만의 스택에 담는다. **그러나 둘은 같은 공용 변수에 접근하고 있다. 어떻게 하면 두 프로세스가 원활하게 동작이 가능할까?**

리눅스에서는 **공용 변수에 접근할 때만 따로 lock을 건다**. 접근이 끝났다면 unlock을 한다. **즉, lock이 되어 있다면 커널모드이건 아니건 CPU를 뺐는 일은 발생하지 않는다.** 그러나 **unlock이라면 크리티컬 세션이 아닌 것이므로 커널 모드임에도 CPU를 다른 프로세스에게 할당하는 것이 가능**하다. 리얼타임시스템을 고려한 리눅스에서의 설계다.

![image-20200811163959603](https://user-images.githubusercontent.com/58545240/90212875-5939a280-de2f-11ea-80a2-f87c6a494dc5.png)

위의 `preempt_count`가 바로 `lock의 갯수`이다. CPU를 뺏으러 왔을 때, **preempt_count가 0이면 공용 변수에 접근하는 프로세스가 하나도 없다는 것이기 때문에 CPU를 뺏을 수 있다.** `need_resched`의 경우는 리얼타임 시스템이 CPU를 다른 프로세스에 할당해주기 위해 왔는데, `preempt_count`가 `0`이 아니여서 뺐을 수는 없으니 "지금 다른 우선순위 높은 프로세스가 CPU를 기다리고 있어!"라는 표시를 해주는 용도로 사용한다. 따라서 **preempt_count 가 0으로 되고 need_resched 플래그가 세트되어 있다면, 현재 공용변수에 접근하고 있는 것이 없으니 CPU를 다른 프로세스에게 할당해도 좋다는 의미가 된다.**

---

> **프로세스 스케쥴링을 할 때 우선순위와 타임슬라이스, 그리고 크리티컬 영역까지 고려한다는 점을 알게 되었다.** 커널을 설계하는 개발자들이 얼마나 치밀하고 효율적으로 설계하기 위해 노력했는지를 간접적으로나마 알 수 있었다. 커널과 같은 하나의 훌륭하고 완벽한 프로그램을 만드는 그 날까지 노력해야겠다.

---

# 20. 타이머와 시간 관리

---

> 지난 5강에선 **Timeslice 라는 CPU에게 주어지는 사용시간과 CPU의 사용 순서를 관리하는 커널 스케쥴링에 대하여 공부를 했다.** 이번 시간에는 그 시간의 단위에 대한 공부와 **여러 개의 인터럽트가 일어났을 때의 관리방법**에 대하여 공부를 할 것이다.

---

우리는 시계가 돌아갈때 나는 소리를 째깍째깍 거린다고 표현을 하며 이는 영어로 Tick Tack이라고 표현이 된다. 이때 일초에 1000번 **째깍거리면** 1000 헤르츠(Hertz, HZ)라고 하고 이는 1 밀리세컨드(1 Millisecond)가 된다. 이러한 표현들은 물리학에서 사용하는 표현들이고 `#define HZ 1000`이라는 표현을 사용하면 **1초에 1000번 인터럽트**가 걸리는 설정으로 된다. 대부분의 경우에는 `100`을 걸어 놓는다.

![image-20200811164119341](https://user-images.githubusercontent.com/58545240/90212901-6e163600-de2f-11ea-9fb3-a21e714b9356.png)

**시스템이 켜진(부팅 된) 이후**에 몇 번 tick을 했는지 기록한 것을 우리는 **Jiffies**라고 표현을 한다. 이는 전역 변수이며 카운터의 역할을 한다. 이러한 `Jiffies`를 설정된 `HZ`로 나누면 몇 초가 흘렀는지 알 수 있다.

```bash
# Note
예시: Jiffies가 24000이고 HZ가 100으로 설정되어 있었다면 24000/100 즉 시스템이 부팅된 이후 240초가 흘렀다는 것을 알 수 있다.
```

위에서 우리는 `HZ`의 단위로 인터럽트를 걸기위해 이러한 시간 개념을 도입했다는 것을 살펴봤. 그렇다면 **왜 1초에 100번씩이나 인터럽트가 걸려야 하는 것 일까. 왜 I/O 인터럽트처럼 어떤 입력이나 할 일이 생겼을 때만 인터럽트를 하면 되는 것 아닐까**라는 의문을 품을 수 있을 것이다.

시스템에 시간 단위를 도입한 이유는 **먼저 특정 시간마다 반복이 필요한 일들을 처리하려면 시스템이시간의 개념을 알아야 하기 때문이다.** 사실 가장 중요한 이유는 **스케쥴링**에 필요하다는 점이다. 프로세스들은 CPU를 사용할 수 있는 시간인 **Timeslice**를 배정받게 된다.

이때 얼마만큼의 시간이 지났는지 파악해 다음 작업에 `CPU`를 넘겨주는 등의 역할을 할때 시간의 단위를 사용하게 된다. 즉, 하나의 작업이 다 끝날때까지 다음 작업이 기다리는 것이 아닌 `**HZ**`**의 단위로 계속해서 프로세스들이 돌아가며 작업을 할 수 있게 해주기 위해 계속해서 특정 시간마다 인터럽트를 걸어주는 것 이다.**

```bash
# Note
첫번째 이유에 대한 예시: 만약 안구건조증이 있는 사람이라면 2시간 마다 하던 일을 멈추고 반복적으로 약을 넣어줘야 한다.
두번째 이유에 대한 예시: 학교에서 하루종일 한 과목만 할 수는 없기에 수업 시간 종소리를 통해 다음 수업을 진행한다.
```

그렇다면 이런 인터럽트의 횟수를 `100`에서 `1000`번을 하게 `HZ`를 설정하면 좋은 것 일까? 꼭 그렇지만은 않다. 물론 인터럽트 하는 주기가 늘어났기에 반응을 해주는 횟수는 증가할 수 있다. **그러나 많은 인터럽트를 하면서 오버헤드가 증가하기 때문에 현재 쓰는 장치들은 대부분 100 HZ라는 적절한 숫자로 설정해 준 것이다.**

![image-20200811164153528](https://user-images.githubusercontent.com/58545240/90212915-77070780-de2f-11ea-9a78-76b3106cd8ea.png)

이러한 시스템의 시간은 크게 **Timer 와 Real-Time Clock(RTC)** 2가지로 나눌 수 있다. 먼저 **Timer**는 **주기적으로 CPU에게 인터럽트**를 거는 역할을 한다. 이러한 **Timer**는 프로그램적으로 인터럽트를 걸 수 있게 설정할 수도 있다.

**Real-Time Clock**은 **현실 세계의 시간**을 표현하며 PC의 전원을 꺼놔도 보조 베터리를 통해 계속해서 현재 시간을 측정 한다. 후에 다시 PC를 켰을때 시스템은 **Real-Time Clock**의 시간을 보고 현재 시간을 표시한다. 그렇다면 이제 이런 타이머 인터럽트를 관리하는 핸들러의 실제 구현을 아래 그림과 함께 알아보자.

![image-20200811164205969](https://user-images.githubusercontent.com/58545240/90212920-7bcbbb80-de2f-11ea-9347-6320b3659bb4.png)

먼저 인터럽트가 걸리면 `dotimer(struct ptregs *regs)`로 들어와 `Jiffies`를 하나 증가시키게 된다. 이때 `Jiffies`는 시스템이 켜진 이후 매번 카운트를 하는 역할을 함으로 `64비트`라는 매우 큰 크기로 정해준다. 그 후에 `update_process_times(user_mode(regs))`를 통해 몇번이나 인터럽트가 걸렸는지 업데이트를 하는데 이때 인터럽트가 걸린 순간에 User 모드였는지 Kernel 모드였는지 같이 기록을 한다.

이러한 타이머는 프로그램으로 특정 시간으로 설정 할 수도 있으며 지연 시킬 수도 있다는 점 정도만 알아두고 이런 타이머에 의해 걸리는 인터럽트에 대해 자세히 알아보자

# 21. 인터럽트

---

**인터럽트란 CPU가 프로그램을 실행하고 있을 때, 입출력 하드웨어나 예외상황 등이 발생해 작업을 처리가 필요할 경우에 커널에게 처리해 달라고 요청하는 것**이다. 그럼 먼저 하드웨어적 문제중에서 CPU에 대해 알아보자

## 21.1 CPU

CPU는 먼저 인스트럭션을 가져온다. 그리고 그 인스트럭션을 분석하고 실행을 한다. 이렇게 실행을 하기 위해선 데이터를 읽어와야 하는 경우도 종종 있게 된다. 또한 연산을 처리 했다면 처리된 결과를 반환해준다. 인스트럭션이 다 끝났으면 그 다음 인스트럭션을 가져오기 위해 프로그램 카운터를 증가시킨다(보통은 `4 byte` 정도 증가시킨다).

![image-20200811164234316](https://user-images.githubusercontent.com/58545240/90212928-7f5f4280-de2f-11ea-842e-2ea9b3af277b.png)

이런 일련의 작업을 하던 중 **Disk가 인터럽트를 걸었다**고 생각해보자. 그러면 **Interrupt Request Bit** 한 비트를 설정을 한다. 이 비트가 걸려있으면 작업을 계속 돌지 않고 **프로그램 카운터에 인터럽트 핸들러의 새로운 주소를 저장**한다. 그리고 다시 진행을 해서 인스트럭션을 가져오면 이제 아까 저장했던 인스트럭션을 가져와 점프를 한 효과를 보게된다. 물론 이런 인터럽트가 걸리지 않게끔 설정할 수도 있다. 예를 들어 컴퓨터를 부팅할 때의 경우엔 `Interrupt Request Bit` 설정을 `disable` 시켜서 인터럽트 당하지 않게 만들 수 있다.

위의 경우는 디스크 하나만 인터럽트를 걸었을 경우에 대한 설명이었고, 아래 그림은 인터럽트를 거는 장치들이 많을 때의 상황을 설명한다.

![image-20200811164249731](https://user-images.githubusercontent.com/58545240/90212934-82f2c980-de2f-11ea-9ab9-dc12775e6de3.png)

여러개의 장치가 인터럽트를 거는 것을 통제하기 위해 **Interrupt Controller**라는 개념을 도입했다. 우리는 이런 **Interrupt Controller**를 **PIC(Programmable Interrupt Controller)**라고 부른다. 여기서 `Programmable`이 붙는 이유는 소프트웨어적으로 관리가 가능하기 때문에 프로그램이 가능하다는 뜻이 붙은 것이다.

## 21.2 PIC

![image-20200811164310341](https://user-images.githubusercontent.com/58545240/90212940-86865080-de2f-11ea-9b4d-a0761e124f45.png)

PIC에서는 여러개의 장치들이 인터럽트 요청을 한다. 이때 이런 요청들을 한 장치들은 **Interrupt Request Line(IRQ Line)**에 연결이 된다. 이렇게 요청이 들어온 요청 라인들은 **Mask Register**을 통해서 `0`일 경우 차단이 되고 `0`이 아닐 경우엔 그 다음 단계로 통과가 된다. 이러한 `Masking`은 소프트웨어적으로 차단할 장치등을 설정할 수 있다. 그 후에**Interrupt Request Register**에서 `Masking`이 되지 않은 장치들만 요청을 할 수 있게 설정해 주는 단계를 처진다.

이렇게 설정이 된 요청들 중 우선순위가 제일 높은 요청을 **Priority Register**에서 받고 지금 인터럽트 요청이 진행 중 이라고**In Service Register**에 등록하고 **INTR**, 즉 `CPU`에게 인터럽트 요청을 한다. 이때 어떤 `IRQ Line`에서 요청이 들어온건지를 **Vector**에 넣어 보낸다. 그렇게 요청을 보내고 나서 `CPU`가 요청을 처리 했다는 **ACK**신호를 보낼 때 까지 다른 `PIC`와 장치들은 차단되어 있다. 이러한 요청 처리 단계를 위해 CPU는 빠르게 일들을 처리해 줘야 한다.

![image-20200811164325640](https://user-images.githubusercontent.com/58545240/90212949-8b4b0480-de2f-11ea-8972-797efa46b0fb.png)

지금까지 설명한 요청 단계를 정리해 보면 위의 그림과 같이 나온다. `PIC`가 `INTR`과 `Vector`을 `CPU`에게 보내고 그럼 `Interrupt Request Bit`을 설정하고 그럼 그걸 본 `CPU`는 그 요청을 처리할 새로운 공간을 만들어 일들 처리후 `ACK`를 다시 `PIC`에게 보낸다. 이러는 동안 요청을 보낸 `PIC`은 `ACK`가 올때까지 다른 요청들을 차단하고 기다리고 있는다.

## 21.3 SMP & AMP

이번에는 여러개의 요청들이 멀티프로세싱 환경에서는 어떤 방법으로 처리되는지 알아보자.

![image-20200811164348476](https://user-images.githubusercontent.com/58545240/90212953-90a84f00-de2f-11ea-97dc-c7adfc158b21.png)

먼저 `CPU` 2개가 있고 이 `CPU`가 `bus`에 달려있다고 하자. 그리고 모든 I/O 장치들은 **multi APIC(Advanced Pic)**에 달려있다. `APIC`이란 멀티 프로세서를 위한 `PIC`이다. 또 다른 `APIC`은 각각의 `CPU`에 달려있는데 이때의 조그만한 `PIC`들은 **Local APIC**이라 하며 여기에는 정기적으로 인터럽트를 걸어주는 **Timer**만 달려있다.

![image-20200811164401220](https://user-images.githubusercontent.com/58545240/90212974-9f8f0180-de2f-11ea-9456-d41dcb60e905.png)

여기서 잠시 컴퓨터 구조에 대한 얘기를 해보자. `CPU`가 `0000000~7777XXX`번 메모리를 메모리 관리 유닛에게 보내면 위 그림의 좌측 **Memory** 쪽으로 달려간다. 그러나 `7777XXX ~ 7777777`번 메모리를 유닛에게 보내면 **I/O Interface**쪽으로 버스를 타고 달려가게 되어있다.

`I/O 버스`들에는 각각 I/O 장치들이 연결이 되어있는데 이런 장치들은 컴퓨터 뒤에 보면 있는 각종 연결장치, 즉 **I/O Interface Card**들로 연결이 되어있다. 이런 `I/O Interface Card`의 구조는 `Address, Data`와 보조 레지스터인 `Control, Status` 등으로 구성되어 있다. 그 외에 장치를 만든 회사 이름, 모델 아이디 등등이 들어있으며 어느 카드에 꽂혀있는 장치인지 등에 대한 정보도 들어있다. 이러한 인터페이스를 바탕으로 위 그림의 오른쪽 처럼 각종 장치들이 인터페이스 카드들에 연결이 되어 있는 것이다.

![image-20200811164413347](https://user-images.githubusercontent.com/58545240/90212980-a3228880-de2f-11ea-8cdb-eb2b51275b9d.png)

위 그림은 방금 설명한 과정을 **SMP(Symmetric Multiprocessing), 즉 대칭형 멀티 프로세싱 방식**에서 처리하는 과정을 나타내는 그림이다. `SMP`란 **두 개 이상의 동일한 프로세서가 하나의 메모리, I/O 디바이스, 인터럽트 등의 자원을 공유하여 단일 시스템 버스를 통해 각각의 프로세서는 다른 프로그램을 실행하고 다른 데이터를 처리하는 시스템**을 말한다.

즉, **두 개 이상의 프로세서가 하나의 컴퓨터 시스템을 공유**하도록 연결되어 있으며, **각각의 프로세서가 독립적으로 자신의 작업을 처리하는 방식**이다. 이러한 방식에서 디바이스가 I/O 인터페이스 카드에 연결이 되어 요청을 보내면 `APIC`이 받아 처리를 하는데 `CPU`간의 차이가 없는 대칭형이기 때문에 어느 `CPU`에 요청을 전달하는지는 다음과 같은 두 가지 방식을 사용한다.

첫번째 방식은 **Static Distribution방식으로** 정적으로 정해진 곳에 보낸다. 이 경우에는 이미 만들어진 `Static Table`을 통해 결정을 하게 된다.

두번째 방식은 **Dynamic Distribution 방식**으로 동적으로 결정을 하는데 이때 **동적 IRQ 분배 알고리즘**을 통해 보낼 곳을 정한다. 이 알고리즘의 목표는 **우선순위가 제일 낮은 프로세스를 돌리고 있는 CPU에게 IRQ를 주는것**이다.

그렇다면 프로세스를 실행하고 있는 경우에는 어떻게 처리를 해야 할까. 이를 해결 하기 위해 모든 프로세스에 카운터를 둔다. 이 **카운터의 값이 가장 큰 CPU**가 `IRQ`를 받게 되는데 이때 카운터를 `0`으로 낮춰주고 `IRQ`를 받지 않은 다른 모든 `CPU`의 카운터는 증가시킨다. 이렇게 카운터를 증가시킴으로서 **나중에 어떤 CPU가 IRQ를 제일 적게 받아 제일 처리를 많이 안했는지 분별할 수 있는 척도로 사용을 하게된다.**

![image-20200811164429791](https://user-images.githubusercontent.com/58545240/90212987-a6b60f80-de2f-11ea-818f-f342324da2c7.png)

그럼 이제 **Asymmetric Multiprocessing(AMP), 비대칭형 멀티 프로세싱에선 어떤 방식을 사용하는지 알아보자.** `AMP`란 **두개 이상의 각각의 프로세서가 자신만의 다른 특정 기능을 수행하는 방식**을 말한다.

예를 들어 하나의 프로세서가 메인 운영체제를 실행하도록 하고 다른 프로세서는 I/O 기능을 전용으로 수행하는 형식 등을 말한다. 이런 구조에서 `IRQ`를 처리하는 방식이 위 그림에 나와있다. 각각의 `CPU`가 다른 작업들을 하고있다. 이때 **Master CPU**는 **본인만의 메모리**를 가지고 있는데 여기 **OS 커널**이 들어있다.

즉, **Master CPU만 I/O 인스트럭션을 가지고 있는 구조**이기에 다른 `CPU`가 I/O를 하려면 `Master CPU`에게 신호를 보내야 한다. 이러한 **주종 관계라는 간단한 구조 덕분에 디자인이 쉽다.**

그러나 `Master CPU`에게 시스템 콜 요청이 많아지면 과부화가 쉽게 걸리며 `Master CPU`가 망가지면 아무것도 할 수 없기 때문에 문제가 생긴다. 과거에는 `AMP` 방식만 사용하다 처리 방식이 발전이 되어 `SMP`로 바뀌었다.

---

>Timer와 장치들의 인터럽트 및 처리방법에 대해 공부하였다.

---

