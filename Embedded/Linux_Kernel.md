# **LINUX 커널 :black_nib:**

---

>리눅스 커널을 한 사람이 전부 아는 것은 불가능하다. 커널 관련 두터운원서를 75~80권 정도는 읽어야 "아, 한 번씩은 훑어봤다"라고 말할 수 있을 정도다. `IBM`같은 대형회사에서도 리눅스를 다루는 사람만 250명 정도 있다고 한다. 250명의 사람이 방대한 커널에서 각자 분야를 맡아서 일을 처리한다.
>
>따라서 "모든걸 다 알아야지~!"라는 마음가짐 보다는 **"커널과 운영체제가 어떤식으로 동작하는지 개략적으로 이해해보자"**라는 수준의 마음가짐으로 임할것이다.

---

# **1. 운영체제란?**

운영체제(Operating System)이란 하드웨어 자원들(cpu, memory, disk, tty)을 관리하고 프로그램들을 지원(support)해주는 것이다. 

(아래 그림을 살펴보면 Operating System의 아래에 하드웨어들이 있고 그 위로는 프로그램들이 있다.)

![image-20200806143341661](https://user-images.githubusercontent.com/58545240/90211904-05c65500-de2d-11ea-8d7a-b2c26e1b2c12.png)

다르게 표현하면, **하드웨어를 감추고 겉으로 다른 프로그램들을 지원해준다**라고 생각할 수 있다.

하드웨어를 감춘다는 건, 프로그램을 사용하는 사람이 편하게 쓸 수 있게 각종 기반 작업을 지원한다는 것으로 이해할 수 있다.

```bash
# Note
일반적으로 우리는 파워포인트나 워드를 쓸 때 프로그램이 cpu와 memory와 어떻게 소통하는지 등에 대해 따로 신경쓰지 않는다.
이는 다 운영체제 덕분이다.
```

---

## 1.1 프로그램이란?

코딩을 해봤다면 `main()`함수의 존재에 대해서 알 것이다. 프로그램이란 `main()`함수를 포함하여 다른 다양한 기능들을 하는 함수들이 모인 존재라고 생각할 수있다. 함수들이 적혀 있는 소스코드 파일을 컴파일 하면 프로그램이 된다는 것을 우리는 알고 있다.

`*.c`파일을 컴파일해서 `a.out` 혹은 `*.exe`등이 생성되고 우리는 이걸 프로그램이라 부른다.

여기서 한 가지 생각해볼 점이 있다. **왜 대부분의 프로그램은 분리되어 있는가?**에 대한 점이다. Microsoft사를 예로 들어보자. Microsoft사는 Word, PowerPoint 등 많은 프로그램을 보유하고 있다. Office관련 프로그램을 통틀어 우리는 Microsoft Office라고 부르기도 하는데, 왜 Microsoft사에서는 **왜 하나의 Office프로그램이 아니라 여러 프로그램(Word, PowerPoint)으로 분할해놨을까?**

사업적인 목적일 수 있겠으나 본질적으로는 **하나의 커다란 프로그램으로 운영할 경우 발생하는 비효율성 때문**이다. 거대한 프로그램은 실행할 때 부팅 시간도 오래 걸리고 메모리 사용에 있어서도 심각한 비효율성을 초래한다. 이런 여러가지 불편한 점이 있기에 여러가지 프로그램으로 분할해 놓은 것이다.

위와같은 이유로 리눅스 운영체제 또한 **Kernel, Shell, Utility 등 여러가지 프로그램으로 나뉘어져 있다.**

---

## 1.2 커널(Kernel)이란?

`Kernel`은 본질적으로 프로그램이다. 우리가 흔히 아는 `main()`으로 시작하는 프로그램 말이다.

하지만 다른 모든 프로그램과는 다르게 커널만이 가지고 있는 특별한 점이 있다. **그것은 바로 'Memory Resident'라는 점이다.** 메모리에 항상 상주해있는 것이 바로 커널이다.

![image-20200806144052100](https://user-images.githubusercontent.com/58545240/90211909-0c54cc80-de2d-11ea-9086-63628e64fb0d.png)

커널이 아닌 다른 프로그램들은 메모리에 있어도 되고 없어도 된다. **'Disk Resident'**라고 표현한다. 필요할때마다 그 때 그 때 메모리에 로딩해서 사용하면 된다는 의미이다.

**커널은 'Memory Resident'특징을 제외하곤 아주 평범한 C Program이다.** 커널을제외한 다른 프로그램들을 우리는 `Utility`라고 하는데 위에서 언급했듯 disk resident하다. 항상 현 주소가 disk라는 의미다. 유저가 필요할 때 요청을 하면 그 때 메모리에 올라오는(로딩되는) 것이다. 그런 의미에서 **Utility**를 우리는 **Command**라고도 칭한다. **Utility와 Command를 동의어로 생각하고 공부**해보자.

---

## 1.3 쉘(Shell)이란?

우리 디스크에는 수십 수백개의 프로그램들이 존재한다. **이 프로그램들이 언제 메모리에 로딩되고 언제 메모리에서 해제되는지 누가 관리해줄까?** 관리해주는 프로그램이 꼭 필요하지 않을까?

위와 같은 필요에 의해 탄생한 것이 쉘이다. **많은 프로그램들의 메모리 교통정리를 해주는 역할을 한다.** 유틸리티 중 하나로 쉘의 1차적인 임무는 **'Job Control'**이다. `Utility`, `Command`, `Job`을 동의어로 생각하고 공부해보자.

---

## 1.4 파일(file)이란?(유닉스 한정)

유닉스(Unix)에서 파일은 **sequence of bytes**를 의미한다. 말 그대로 바이트들의 배열이란 뜻이다. 모든 함수, 명령어들은 결국 기계어로 해석하면 0과 1의 나열에 불과하다.

특히 유닉스, 리눅스에서는 **I/O device**도 file로 취급한다. 당장 이해가 안되겠지만, 입출력 기계들(하드디스크, USB, 키보드 등)을 파일로 취급한다고 알고있어라.

```bash
# Note
유닉스, 리눅스에서 입출력 기계들은 `/dev/hd0`, `/dev/hd1`, `/dev/tty2` 등 파일로서 다뤄지고 입출력 기계와 1:1로 대응된다.
```

---

## 1.5 커널과 쉘, 그리고 유틸리티의 관계

아래의 이미지를 살펴보면, 좌측에 메모리 그리고 우측에 디스크가 있다. 커널과 쉘, 그리고 유틸리티들이 디스크와 메모리에서 어떻게 작동하는지 살펴보자.

![image-20200806144925170](https://user-images.githubusercontent.com/58545240/90211922-11b21700-de2d-11ea-9473-5599df267ffd.png)

맨 처음 시스템을 부팅하면 제일 먼저 메인 메모리에 `Kernel(a.out)`이 올라온다. 커널 실행파일이 메모리에 로딩된다는 말이다. 리눅스는 **멀티 유저 시스템으로 하나의 시스템에 다양한 유저가 들어온다**는 사실을 상기하고 아래의 그림을 살펴보자.

![image-20200806145025885](https://user-images.githubusercontent.com/58545240/90211933-1aa2e880-de2d-11ea-9dd2-02a9643ed66c.png)

유저가 터미널에 전원을 키면 그 터미널 위에서 쉘(shell)이란 프로그램을 메인 메모리에 올라온다. 그 후 쉘은 유저가 키보드로 커맨드를 입력하기 기다린다. 유저가 커맨드를 입력하면, 쉘은 커맨드에 대응하는 유틸리티를 디스크로부터 가져와서 실행시킨다. **각 유저로부터 전원이 들어올 때마다 이 3개의 프로그램(커널, 쉘, 유틸리티)의 관계가 형성됨을 알 수 있다.**

그런데 키보드로 커맨드를 입력하길 기다리고 그에 대응하는 유틸리티를 디스크로부터 가져와 실행시킨다는 것이 무슨말일까? 이 말의 의미를 이해하기 위해서는 **리눅스가 멀티유저 시스템이라는 점을 상기해야한다.** 더 자세한 설명 고고

위 내용들을 다시한번 정리하면 이렇다.

- 커널은 운영체제이며 항상 메모리에 상주해 있다. 나머지 프로그램들은 전부 유틸리티이며 디스크에 상주한다.
- 유틸리티는 항상 디스크에 있다가 필요할 때마다 메모리에 올라오고 사용하지 않을 때는 다시 내려간다. 유틸리티를 우리는 커맨드라고도 칭하며 커맨드들이 교통정리를 하는 것이 쉘의 역할이다
- 쉘한테 우리가 ppt라고 커맨드를 입력하면 쉘은 `child process`로 ppt를 생성한다. `child process`도 추후에 나오는 개념이니 일단 넘어가자

---

## 1.6 터미널은 뭐고 콘솔은 뭘까?

> 이 파트는 *KLDP의 게시물*을 보고 참조하여 작성하였다.

콘솔은 전통적으로 보면 계기판과 입력장치의 모음과 비슷한 것들의 집합으로 컴퓨터를 조작하기 위한 조작부라고 생각하면 된다.

옛날 대형컴퓨터는 `serial(rs232, 422, 485)`를 이용하여 터미널이라는 장치와 연결하여 조작하였고, 아직도 몇몇 은행에서는 이러한 장비들을 볼 수 있다. 터미널이라는 장비는 CRT와 키보드로 구성되어 있으며 Teraterm, 넷텀, 하이퍼터미널과 같은 프로그램들을 모두 '터미널 에뮬레이터'라고 불렸다.

이들은 일반 PC를 터미널 대용으로 사용할 수 있도록 해주는 역할을 한다. 

**이제부터 `터미널`을 통해 `콘솔`과 통신한다 라고 생각해보자**

이해를 쉽게하기 위해 터미널은 콘솔의 부분집합이라고 생각하는 것도 좋다. 아래 커널 컴파일 도움말에 기술된 내용을 보면, 리눅스에서 콘솔과 터미널은 거의 동일한 의미로 사용되고 있다고 볼 수 있기 때문이다.

# **2. 운영체제 비교, 리눅스 vs 윈도우**

---

## 2.1 자원의 소모에 대하여

운영체제에는 종류가 굉장히 많지만 가장 대표적이라고 일컬어지는 리눅스와 윈도우를 한 번 비교해보려 한다. 윈도우는 개인 컴퓨터(Personal Computer)에 사용되는 운영체제다. 개인 컴퓨터에 쓰인다는 것은 나홀로 사용자라는 것, 즉 싱글유저 시스템이라는 의미다. 리눅스가 멀티유저 시스템으로 만들어진 것과는 상반된다.

싱글유저 시스템일 때는 보안문제에 대해서 신경을 별로 안 써도 되지만, **멀티유저 시스템일 때는 보안에 특히 신경을 써야 한다.** 이유는 간단하다. 멀티유저 시스템은 그 안에 **내 파일 뿐만 아니라 다른 사람의 파일 또한 존재하기 때문이다.** 내가 다른 사람의 파일을 멋대로 읽고 쓸 수 있다면? 즉 삭제하고 변경할 수 있다면?이라는 물음을 던지면 쉽게 이해가 갈 것이다.

또하나의 이슈는 **메모리관리**이다. 멀티유저시스템에서는 한정된 자원을 모두가 효율적으로 이용하기 때문에 메모리 관리가 상당히 중요하다. 반면 윈도우와 같은 싱글유저 시스템에서는 멀티유저 시스템만큼 메모리관리에 신경을 덜 써도 된다.

> 싱글유저 시스템 : 내돈 내고 구입한 컴퓨터(장비)인데, 내가 아니면 누가써? 나만 쓸거야!

위의 한 문장이 싱글유저 시스템의 기본 철학이다. 그래서 PC(Personal Computer)는 일찌감치 윈도우 운영체제와 함께 결합된다. 윈도우는 GUI(Graphic User Interface)를 채택하고 있기 때문이다. 일반 사용자 입장에서 화면에 일일이 키보드로 명령어를 입력하는 것보다 마우스 몇 번 클릭해서 프로그램을 실행하고 관리하는 것이 훨씬 편하기 때문이다.

반면 **리눅스, 유닉스 등의 멀티유저시스템 CLI(Command Line Interface)를 채택**한다. 윈도우에서는 내가 사용할 수 있는 유틸리티(프로그램, 커맨드와 동의어)가 아이콘으로 보기 좋게 화면에 표시되는데, 리눅스는 `man`이라는 명령어로 내가 사용할 수 있는 커맨드가 무엇인지 알아내거나 사전에 알고 있어야 원활한 사용이 가능하다.

![image-20200806151357994](https://user-images.githubusercontent.com/58545240/90211939-20003300-de2d-11ea-861f-bd781d54e613.png)

**윈도우는 사용자에게 편리한 인터페이스를 제공하는 대신, 동일 작업 대비 훨씬 많은 자원을 요구한다.** 리눅스는 사용자에게 다소 불편한 인터페이스를 제공하는 대신, 화면에 표시되는 Char1(1단위당 1바이트)만큼의 자원을 쓰는 등 효율적인 자원 사용을 가능케한다.

---

## 2.2 보안 이슈에 대하여

윈도우와는 다르게 리눅스에서는 보안이 매우 중요하다. 위에서 언급한 상황을 다시 생각해보자 **만일 한 프로세스가 다른 프로세스의 정보를 함부로 I/O하려고 하면 어떨까? 즉, 다른 프로세스의 파일을 삭제하거나 내용을 바꿀 수 있다면?** 그것만큼 끔찍한 일도 없을 것이다. 어제 힘들게 작성한 보고서를 자신의 가장 친한 친구가 실수로 삭제했다고만 생각해도 화가 치미는데, 모르는사람이 그런다면..

이러한 보안문제를 해결하기 위해 리눅스에서는 사전방지(Prevent)하기 위해 많은 노력을 하게 된다. 사후처리가 아닌 사전방지(Prevent)하는 이유 또한 간단하다. 내 파일이 삭제된 이후에 복구하는 뻘짓보다는 사전에 그런 행위를 못하게 막는 것이 훨씬 효율적이고 철학적으로도 옳다.

우리가 시스템을 설계하는 입장이라고 생각해보자. 알다시피 한 시스템에서 CPU는 모든 프로그램이 공통적으로 사용하는 공용자원이다. **메모리에 존재하는 프로그램은 여러가지고 하나의 프로그램만이 한 번의 순간에 CPU를 온전히 차지한다.**

- **CPU는 한 순간에 하나의 연산밖에 못한다.**

즉, 한 순간에 CPU를 사용하는 건 오직 하나의 프로그램 뿐이다. 여기서 순간이란 건, 기술력에 따라서 달라지는데 현재는 1초에 40억번의 연산이 가능한 CPU들이 등장하고 있으니, 최근기술로서 한 순간을 해석해보자면 나노세컨드 정도는 될 것으로 생각된다.

위의 내용들을 바탕으로 아래의 그림을 살펴보자.

```bash
# Note
1초에 40억번의 연산이 가능하다는 말은 1초에 CPU는 40억번의 순간을 함축하고 있다는 뜻이다. CPU는 1초에 40억번의 순간이 존재하고 그 순간마다 연산(모든 프로그램을 관리)하기 때문에 우리가 여러 프로그램들을 한꺼번에 켜놓고 작업해도 끊김없이 동시에 처리가 되는 것 처럼 보인다.
```

![image-20200806152422710](https://user-images.githubusercontent.com/58545240/90211946-24c4e700-de2d-11ea-954b-78b29d3995c7.png)

CPU가 Bob의 터미널과 연결된 쉘에게 양도되었다고 하자. 즉 시스템이 Bob의 터미널과 연결된 쉘에 CPU를 넘겨준 것이다. 쉘도 하나의 프로그램이므로, 쉘에 버그가 생겼다고 가정해보자. 본래 쉘은 Bob이 접근 가능한 디스크 영역, 메모리 영역에만 데이터를 읽고 써야하는데, 디스크의 Sector Address를 잘못 계산해서 Dan의 영역에 데이터를 썼다고 가정해보자.

즉 Dan이 저장해놓은 파일들이 다 엉망이 됐다는 뜻이다. 시스템은 이미 CPU를 Bob과 연결되어 있는 쉘에게 양도를 해놨고... 이 쉘은 잘못을 저질렀다. 해당 프로그램이 오류로 엉뚱한곳에 데이터를 읽거나 쓰는 상황이 발생했는데 어떻게 이러한 상황을 예방할 수 있을까?

![image-20200806152712710](https://user-images.githubusercontent.com/58545240/90211983-3a3a1100-de2d-11ea-816b-6fe968ba238b.png)

위 그림에도 써져 있지만 리눅스 시스템 설계자가 고안한 해답은 바로 **"I/O instruction 하지마!!"**이다. 만약 쉘을 포함한 다른 일반 프로그램이 I/O를 수행하는 순간 CPU를 사용권한을 바로 뺏긴다.

쉘(sh)입장에서는 한편으론 억울할 수 있다. **"나는 CPU를 사용할 수 있고, 내역할은 I/O를 하는건데 그럼 뭘 하라는거야?"**라고 쉘은 반박한다. 자신의 임무를 수행하려 했더니 억울하게 CPU를 뺏겨버리니 말이다.

그래서 리눅스 시스템 설계자는 **"그럼 니가 I/O할 때 커널에게 부탁을 해. I/O하게 해달라고"** 라고 쉘에게 말한다. 즉, 현재 리눅스 시스템의 작동은 **I/O Instruction을 할 때는 커널이 갖고 있는 function에 부탁을 하는방식**으로 되어있다. 이 부탁하는 과정을 `***System Call***`이라 한다.

커널은 해당 입출력 명령이 합법적인 것인지 검사한 후에 I/O를 대신 진행해준다. 이런 특별한 구조를 구축하기 위해 윈도우나 개인 컴퓨터에는 없는 개념을 하드웨어에 도입한다. 아래의 그림을 살펴보자.

![image-20200806153057063](https://user-images.githubusercontent.com/58545240/90211992-3efec500-de2d-11ea-9043-bcd00b84e08d.png)

바로 **CPU에 하나의 Binary Bit를 도입한 것**이다. 위 그림에 빨간색 네모칸이 바로 그 bit(비트)다. 이 bit를 우리는 **mode bit**라 칭한다. 비트라는건 알다시피 2개의 데이터(0과 1)만을 저장할 수 있다. 0은 유저모드를 의미하고 1은 커널모드를 뜻한다. 우선 우리가 메모리와 CPU에 대해서 알고 있는 개념을 다시 한번 정리해보자. 그림과 함께보자.

- CPU의 Control Unit 파트에 **PC는 Program Counter라고 하는 특수 레지스터다. PC에서는 이번에 수행해야 할 instruction(명령)의 주소**를 메모리로 보낸다. **MAR은 Memory Address Register**이며 **Address Bus**라고도 이해할 수 있는데 MAR이 읽어들인 주소에 **해당하는 데이터를 MBR(Memory Buffer Register)에 담아 IR(Instruction Register)에 보낸다.**
- IR로 들어온 **Instruction(명령)은 위 그림의 우측 하단과 같은 구조**를 지닌다. **op-code**에는 수행해야 할 명령이 적혀있고 그 옆에는 **operands**라고 하는 명령 인수들이 적혀 있다. i와 j 주소에 있는 값을 더하라는 의미 정도로 해석할 수 있다.

![image-20200806153516001](https://user-images.githubusercontent.com/58545240/90212001-458d3c80-de2d-11ea-8fe5-d2ac9d36c192.png)

- Op-Code를 읽어들여 명령어를 처리하는데, **i와 j에 해당하는 메모리주소에 담겨 있는 값을 알아내기 위해 또 다시 메모리에 접근해서 연산을 진행한다.**

### >> mode bit(모드 비트) 추가설명

CPU에는 한 바이너리 비트인 **모드 비트**라는 개념이 존재하고, **그것은 0 또는 1의값**을 갖는다. 커널 모드에서 동작하느냐 유저모드에서 동작하냐를 정해주는 이 바이너리 비트를 우리는 일단 0이면 유저모드, 1이면 커널모드로 동작한다고 생각해보자.

![image-20200806153729243](https://user-images.githubusercontent.com/58545240/90212014-49b95a00-de2d-11ea-83c2-4e53254c2fa7.png)

만약 모드 비트가 `커널모드(1)`로 되어있다면 **CPU는 어떠한 영역의 메모리라도 접근할 수 있다.**

커널모드가 아닌 `유저모드(0)`로 되어있다면 **모든 메모리에 접근(Access)는 불가능하고 자신의 Address Space만 접근가능하다.**

또한 `커널모드(1)`일 경우에는 **모든 instruction이나 op-code를 수행(execute)할 수 있지만**

`유저모드(0)`라면 **I/O instruction이나 special register accesses와 관련된 instruction은 금지**된다.

즉, **유저모드에서는 타인에게 큰 영향을 줄 수 있는 instruction은 모두 거부되는 것이다.**

```bash
# Note
`special register accesses`란 스택 포인터(SP)나 프로그램 카운터(PC)와 같은 특별한 레지스터에 대해 값을 읽는 등위 행위를 말한다.
```

![image-20200806154109939](https://user-images.githubusercontent.com/58545240/90212030-4e7e0e00-de2d-11ea-86a8-b3bd4a75dca9.png)

---

## 2.3 커널모드와 유저모드

인천 공항 보안 검색대를 생각해보자. 일반 사람들이라면 보안 검색대를 반드시 통과해야겠지만, 자신이 대통령이나 국가원수에 해당하는 중책이고 매우 바쁜일이 있다면 보안 검색대를 거치지 않고 바로 공항을 나설 수 있다.

CPU는 항상 메모리에서 address를 메모리에 건넨다. 원하는 instruction이 있다면 해당 instruction의 주소를 Program Counter가 메모리로 보낸다. 이 과정이 CPU가 메모리에게 **"이 instruction을 수행해야 하니 메모리에 관련된 코드를 나에게 보내다오"**라고 말하는 과정이며, instruction이 오면 그걸 실행(execute)하는 과정에서 또 관련 매개변수(operands)에 관련된 주소를 보내고 관련 정보를 받아온다.

이처럼 CPU는 계속 메모리에게 address를 보내는 작업을 진행하는 것인데, CPU가 메모리로 주소를 보낼 때는 그 당시 모드 비트가 어떤 것으로 되어 있느냐가 정말 중요하다. 먼저 아래의 그림을 살펴보자.

![image-20200806160700319](https://user-images.githubusercontent.com/58545240/90212037-52aa2b80-de2d-11ea-9742-c13a81686514.png)

위의 그림은 **빨간색 모드 비트가 유저모드였을 경우**를 상정하고 설명을 한다. 앞에서 인천 공항 검색대로 예시를 들었는데, 운영체제에 빗대서 다시 정리해보자. 먼저 첫 번째 보안 검색(address)를 진행한다.

CPU와 메모리 사이에는 **MMU(Memory Management Unit)**이라는 하드웨어가 존재한다. 이 MMU의 역할은 CPU로부터 메모리로 가는 **address를 조사**하는 것이다. 즉, CPU가 메모리에게 넘기는 address 정보가 올바른지 판명하는 것이다. **"CPU야. 너가 지금 보낸 주소가 너가 할 수 있는 접근 메모리 범위를 벗어나지는 않았니?"**라고 말한다.

첫 번째 검사를 무사히 통과하면 **Instruction Fetch**, 즉 instruction을 가져온다. 위에서 우리는 instruction,의 구조가 op-code, operands로 구성된다는 것을 확인했는데, 바로 이 **op-code를 보고** 아 이것이 덧셈이구나, 뺄셈이구나, 곱셈이구나 등을 확인하게 된다. 이게 바로 2번째 검사다.

명령어(op-code)를 확인해 봤는데, 만약 이것이 **privileged op-code(I/O와 같은 중요한 역할을 하는 실행)**을 시도하려 한다라고 판단되면 **그 순간 바로 CPU를 뺏겨버린다.**

이렇게 작동을 한다면 위에서 언급한 보안이슈를 만족할 수 있다. illegal access가 사전에 예방(prevent)이 되고 차단이 될 수 있다.

![image-20200806161142471](https://user-images.githubusercontent.com/58545240/90212045-576edf80-de2d-11ea-8499-e3e0bbe9a336.png)

정리하자면 모드 비트가 유저모드일 때, CPU가 접근하는 메모리 주소가 실행 중인 프로그램의 범위 밖이거나 I/O instruction등의 금지된 실행을 하려고 한다면 CPU를 운영체제로부터 박탈당한다. 반대로 커널모드였을 경우는 위에서 언급한 검증 절차를 전혀 밟지 않아도 된다.

```bash
# Note
커널모드는 어떠한 메모리 영역도 접근 가능하며 어떠한 연산도 시행할 수 있는 특권을 가지고 있다. `오직 커널만이.`
```

# **3. printf("Hello World!")의 진실**

---

그러면 이제 우리가 한 가지 궁금한 점이 있다. 우리는 프로그램을 만들 때 소스코드에 입출력과 관련된 함수를 작성한다. `printf()`를 사용하거나 `get()`등 디스크에 접근해서 값을 읽어오거나 화면에 문자를 출력하는 함수를 사용한다. 우리가 작성한 코드와 프로그램은 유저모드에서 아무런 제약없이 사용할 수 있었는데, 왜 I/O가 금지되었다고 말을 하는 것일까?

정답은 **“소스코드에서만 그렇게 보인다”**이다. 소스코드에서는 개발자가 입출력을 관리하는 것처럼 보이지만, 소스코드를 컴파일 한 후에 바이너리 파일을 열어보면 I/O와 관련된 instruction은 전혀 존재하지 않는다.

![image-20200806161339721](https://user-images.githubusercontent.com/58545240/90212051-5b9afd00-de2d-11ea-9d51-1d5f744eb570.png)

위에서 언급했듯이 I/O를 하고 싶으면 **커널이 가지고 있는 function**을 호출하는 방법밖에는 없다. 즉 커널에 부탁을 해야한다는 말이다. 그 행위를 우리는 위에서 **시스템 콜(System call)**이라고 했었다.

그런데 의문점이 하나 있다. 함수를 호출한다는 건, 특정 기능을 호출한다는 건 내 프로그램 안에 있는 함수를 호출하는 개념인 것인데, 어떻게 다른 프로그램(커널)의 함수를 호출하지?

그 원리는 우리가 입출력 함수를 담은 소스코드를 컴파일 했을 때, 해당 부분(입출력 등)이 등장하면**’change CPU protection mode to Kernel’**명령어를 수행하는 것이다. CPU의 모드비트를 바꾸는 것이다.

소스파일에 **privileged instruction**이 등장하면 바이너리파일에서 **chmodk**로 변환된다는 건 이제 이해할 수 있을 것이다. 그렇다면 이제 하드웨어가 이 명령어를 어떻게 실행시켜가는지 이해해보자.

---

## 3.1 chmodk를 실행했을 때 하드웨어에서 벌어지는 일들

![image-20200806161542626](https://user-images.githubusercontent.com/58545240/90212090-6fdefa00-de2d-11ea-8f68-158be5b1ea75.png)

소스파일에 입출력 파트에 컴파일러가 컴파일 시에 **chmodk**를 넣어둔 후 발생하는 일은 첫 번째로 유저로부터 CPU를 뺏는다. 더 이상 유저모드에서 실행(run)할 수 없게 만드는 것이다. 그걸 우린 **“trap에 걸린다”**라고 한다.

*트랩은 인터럽트랑 비슷한 개념으로 글 최하단 부분에 자세하게 설명을 적어놓았다.*

트랩에 걸린 후에 트랩핸들러 루틴(트랩을 처리하는 루틴)으로 진입하는데, 해당 루틴은 커널 안에서 처리된다. **chmodk**명령어를 처리하기 직전에 우리는 시스템 콜과 관련된 parameter를 사전에 약속된 곳에 기록을 해둔다. 왜냐하면 이후 트랩이 명령어를 처리할 때 유저가 어떤 처리(write, read, open, close)를 하려 했는지를 알아야 하기 때문이다.

I/O와 관련된 디스크와 관련 섹터에**”A프로그램에 B작업을 요청합니다.”**라는 관련 parameter를 적어두는 것이다. 그럼 트랩핸들러가 그 정보를 확인한 후,**”아 너는 I/O를 하고 싶고 그 중에서도 read를 하고 싶구나”**처럼 확인을 하는 것이다.

이제 유저가 뭘 처리하고 싶어하는지 알았으니 그냥 진행하면 되는 것일까? 아니다.**유저가 해당 디스크나 메모리 영역에 권한(read, write, execute 중 하나)이 있는지도**확인해야한다.

```bash
# Note
커널 function은 라이브러리 function과 다르다. 라이브러리에 있는 함수를 우리가 호출할 때는, 해당 코드가 그대로 우리 소스코드 안에 Copy & Paste 되는반면 커널의 함수를 호출한다는 건 커널에게 부탁을 하고 커널이 해당 함수를 수행해주는 개념이다. 커널이 메모리에 항상 상주해 있어야 하는 이유 중 하나이기도 하다. 유저가 어떤 함수를 요구할지 모르기 때문에 항시 대기해야 하는 것이다.
```

![image-20200806161633797](https://user-images.githubusercontent.com/58545240/90212103-753c4480-de2d-11ea-9bea-5ee94d3c2b91.png)

위의 검증 과정들을 거친 후 read/write 등의 작업이 끝나면 트랩으로 돌아가고, 해당 트랩에서 다시 유저모드로 return된다. 그 때 비로소 유저는 자신이 처리한 작업이 제대로 완료됐는지를 확인할 수가 있다.

![image-20200806161656290](https://user-images.githubusercontent.com/58545240/90212110-79686200-de2d-11ea-9586-6642f37c5b16.png)

과정을 한 번 더 간략하게 도식화한 것이 위 그림이다. 소스파일을 컴파일해서 바이너리파일에 **chmodk**를 껴넣고, 그로 인해 **트랩이 발생**한다. 커널 안의 트랩 핸들러는 적절한 검증절차를 거친 후에 유저모드에서 요구했던 작업을 진행한다. 그 후 다시 유저모드로 돌아온다.

![image-20200806161714782](https://user-images.githubusercontent.com/58545240/90212117-7cfbe900-de2d-11ea-81f5-e04129e0ea1b.png)

커널모드와 유저모드 사이의 모드가 바뀌는 과정을 계속해서 반복한다. 프로그램 내에 더이상 커널에 요구할 것이 없을 때까지. 모든 프로그램은 다 유저모드로 run하다가 커널 모드로 run하는 걸 반복한다.

자, 그런데 run을 한다는 건 유저 모드 혹은 커널 모드 안에 있는 function을 사용한다는 것인데 function들을 계속 call 했다가 return하고 call 했다가 돌아오고 하는 과정을 반복한다는 것이다.

function에는 보통 local variable(지역 변수)들이 있기 마련이고, 해당 지역 변수들이 어디 저장되는지 생각해보자. **일단 이 지역변수들은 메모리에 언제부터 언제까지 존재할까?** 그렇다. 함수가 호출되고 리턴되기까지 존재한다. 따라서 미리 메모리에 담아두는 비효율적인 방법보다는 임시적으로 메모리에 담아뒀다가 삭제하기 용이한 자료구조를 택해야 하는데, 그것이 바로 **스택(Stack)**이다.

![image-20200806161745028](https://user-images.githubusercontent.com/58545240/90212136-85542400-de2d-11ea-991a-e51e3bff812b.png)

function이 호출되면 해당 function의 local variable(지역변수)들이 스택에 push(삽입)된다. 지역변수 뿐만 아니라 함수가 끝나고 돌아갈 주소(return address)등도 함께 push(삽입)된다. 어떤 프로그램이나 유저모드와 커널모드를 오가기를 반복한다는 점을 우리는 알고 있다.

**유저모드에서 유저만의 function들을 실행하고 리턴하기 위해서 유저모드에도 스택이 필요한 것이고, 커널모드도 마찬가지로 자신만의 function을 실행하고 리턴하기 때문에 스택이 필요하다.**

# **4. 중간 정리**

---

![image-20200806161821757](https://user-images.githubusercontent.com/58545240/90212150-8c7b3200-de2d-11ea-992a-ecbeee8422a6.png)

결국 프로그램의 실행은 **유저모드와 커널모드를 Alternating하는 것**으로 볼 수 있다. 위 그림을 쭈욱 따라가면서 1강에서 했던 내용들을 다시 상기해보자. 유저모드가 자신만의 코드를 수행하다가 **커널에게 부탁할 일이 생기면 System call을 한다.**

그렇게 되면 커널모드로 진입하여 **Kernel a.out**이 진행되면서 커널이 일을 처리한 후 다시 유저모드로 돌아와서 작업을 진행한다. 이 과정은 프로그램이 종료할 때까지 반복된다.

마지막으로 인터페이스에 대한 언급으로 1강 강의노트를 마치고자 한다. 커맨드는 유틸리티의 또 다른 이름이고 유틸리티는**disk resident program**이다(function이 아니다). 운영체제의 interface를 살펴보면, **커맨드는 키보드 interface**고 **system call이나 library call은 function interface**이다.

리눅스 운영체제를 사용하게 된다면, **man**명령어를 정말 많이 사용하게 될텐데 각종 명령어를 모를 때 `man <command>`방식으로 원하는 명령에 대한 상세 정보를 확인할 수 있다. 명령어 우측 괄호 안에 있는 숫자가 해당 명령어가 **커맨드, 시스템 콜, 라이브러리 함수**인지를 분간해준다.

![image-20200806161850869](https://user-images.githubusercontent.com/58545240/90212155-94d36d00-de2d-11ea-876c-8094b21058de.png)

# **5. 인터럽트와 트랩**

---

인터럽트는 시스템 내에서 하드웨어가 생성한 흐름 변경이다. 인터럽트 원인을 처리하기 위해 인터럽트 처리기가 사용된다. 제어는 인터럽트된 컨텍스트 및 명령으로 리턴된다. 트랩은 소프트웨어가 생성한 인터럽트다. 장치 폴링의 필요성을 없애기 위해 인터럽트를 사용하여 I/O의 완료를 알릴 수 있다. 트랩을 사용하여 운영 체제 루틴을 호출하거나 산술 오류를 포착 할 수 있다.

**인터럽트는 하드웨어 인터럽트**이며 **트랩은 소프트웨어 호출 인터럽트**이다. 하드웨어 인터럽트 발생은 일반적으로 다른 하드웨어 인터럽트를 비활성화하지만 트랩에는 해당되지 않는다. 트랩이 제공 될 때까지 하드웨어 인터럽트를 허용하지 않으려면 명시 적으로 인터럽트 플래그를 지워야한다. 일반적으로 컴퓨터의 인터럽트 플래그는 트랩이 아닌 (하드웨어) 인터럽트에 영향을준다. 즉,이 플래그를 지우더라도 트랩을 방지 할 수는 없다. 트랩과 달리 인터럽트는 CPU의 이전 상태를 유지해야한다.

**트랩은 일반적으로 소프트웨어 인터럽트 라고하는 특별한 종류의 인터럽트**다. 인터럽트는 하드웨어 인터럽트(하드웨어 장치의 인터럽트)와 소프트웨어 인터럽트(트랩)를 모두 포괄하는보다 일반적인 용어다.

---

> 지금부터는 1장에서 간단하게 배운 **System Call**에 대해 더 자세히 다룰 것 이다. **System Call**이란 멀티유저 시스템에서 한 프로세스가 다른 프로세스에 I/O로 함부로 접근해 데이터를 망치는 일을 사전방지(Prevent)하기 위해 나온 방법이다. 정리해보면 쉘이 I/O를 사용하려고 하는 순간 커널은 CPU를 빼았는다. **I/O를 하고 싶으면 커널이 가지고 있는 function에 부탁해**라는 매커니즘을 System Call이라 하고 우리는 이 내용을 1장에서 배웠다. 이번 2장에서는 System Call의 구체적인 동작 방식을 살펴본다.

---

# **6. 시스템 콜(System Call)**

---

**시스템 콜(System Call)은 것은 정확히 언제 일어나는 것일까?** 우리가 I/O관련 function을 하려고 하면 그때 바로 일어나는 것일까? 이것을 알아보기 위해 먼저 밑에 그림을 보자.

![image-20200806163106579](https://user-images.githubusercontent.com/58545240/90212182-aae12d80-de2d-11ea-970d-a9c08662e841.png)

*리눅스 명령어는 옆에 붙은 숫자에 따라* ***커맨드(1), 시스템 콜(2), 라이브러리 함수(3)****로 구분된다.*

위 그림의 좌측을 보면 유저 영역 안에 내가(유저)가 작성한 코드 `my code`가 있다. 이 코드에서 `printf()`를 호출(call) 하는데 이 `printf()` 코드는 내가 작성한 게 아니라 `library function`이다. C언어를 배울 때 `#include <stdio.h>`를 하는 이유를 생각해보면 금방 이해할 것이다. 그럼 이제 `printf()`가 내가 작성한 코드 `my code`에 들어오는데 `printf()`는 출력 즉, I/O를 해 줘야 한다. 1장에서 배웠듯, 멀티 유저 시스템에서 I/O는 오직 커널만 할 수 있기 때문에 **I/O를 하는 모든 library function은 무조건 System Call을 사용해야 한다. 커널에게 부탁한다는 뜻이다.**

시스템 콜을 하게 되면 **Wrapper Routine**이라는 공간에 가게 되고 이 공간에는 왜 커널로 가게 되는지 알려주는 정보들을 담고 있는 **Prepare parameter**와 CPU의 모드 비트를 커널로 바꾸는 **chmodk**가 들어있다.

**chmodk**가 실행되면서 프로그램은 런타임 중 트랩에 걸려 커널 영역으로 가게 된다. 커널에서는 `Prepare parameter`에 담겨있는 내용을 보고 적절한 **System call function**으로 처리를 해준다.

*커널 안에 있는 **모든 System call function의 이름은 `sys_`로 시작**한다. 리눅스의 naming convention(명명 규칙)이니 알아두길 바란다*

---

## 6.1 Wrapper Routine

트랩으로 넘어갈 내용들을 준비하고 실질적으로 트랩을 일으키는 공간인 `Wrapper Routine`에 대해 조금 더 알아보자.

![image-20200806163401735](https://user-images.githubusercontent.com/58545240/90212189-af0d4b00-de2d-11ea-82d2-20a237895692.png)

`Wrapper Routine`에서 (인텔의 경우) `$0x80`등 의미 없는 문자들을 이용해 Machine Instruction을 주어 트랩을 발동한다. 그런데 위에서 트랩을 일으키기 전에 `Prepare parameter`들을 준비하게 되는데 그 중에 가장 중요한 것은 바로 **system call number**라는 것이다. 이 `system call number`는 커널이 가지고 있는 **system call function의 시작 주소를 담고있는 Array(배열)의 Index 번호**로 사용이 된다.

`system call number`의 예를 들어 보면 다음과 같다. `file`과 관련된 `system call`에는 `open, close, read, write`등이 있는데 open은 0번, close는 2번, read는 3, write는 4번 등 call number을 이용해 Array의 Index 위치에 접근을 한다.

지금까지의 과정을 순차적으로 정리해 보면 아래와 같다.

1. 컴파일러(gcc)가 유저가 짠 코드를 보고 라이브러리(`printf()`)를 호출한다.
2. 라이브러리에서 시스템 콜(`write`)을 호출한다. 위 그림의 write(2)의 2는 시스템 콜을 의미하는 숫자일 뿐 매개변수와 같은 의미는 없다.
3. Wrapper Routine에서 `write`에 대응하는 `system call number`가 나오고 트랩을 건다.
4. 커널이 `system call number`을 가지고 `system call function table`에 접근해 `function`의 시작 주소에 접근한다.

```bash
# Note
여기서 하나 알아둬야 할 점은 이렇게 system call number를 지정한 컴파일러와 그 system call number를 받고 system call function table에서 function을 찾는 운영체제의 번호가 서로 일치해야 한다는 점이다. 이러한 번호들은 컴파일러를 쓰는 회사에서 결정을 한다. 실례로 만약 다른 회사의 플랫폼으로 시스템을 옮기면 소스파일들을 다시 컴파일을 해줘야 system call number가 얽혀서 오동작하는 오류를 방지할 수 있다.
```

마지막으로 아래 그림에 나온 예시를 통해 시스템 콜의 과정을 자세히 살펴보자.

![image-20200806163451277](https://user-images.githubusercontent.com/58545240/90212194-b3d1ff00-de2d-11ea-9245-b3c083c5ad91.png)

- 유저 프로그램이 시스템 콜을 호출한다.
- Machine Instruction이 트랩을 발동한다.
- 하드웨어가 유저 모드에서 커널 모드로 `mode bit`를 바꾼다.
- 하드웨어가 `sys_call()`이라는 커널안의 트랩 핸들러(Trap Handler)로 가게 된다.
- 이런 핸들러는 커널안의 `assembly function`을 수행한다.
- 지금까지 유저 프로그램에서 진행했던 단계를 저장을 한다. (커널 쪽 일이 다 끝나면 시스템 콜을 호출 했던 곳으로 돌아가서 다시 진행을 해야하기 때문에 저장하는 것이다.)
- 시스템 콜 번호가 커널 안에 `sys_call table`에 있는 번호에 맞는 번호인지 확인한다.
- 맞다면 `system call function`의 주소를 가져온다.
- 그리고 `system call function`을 불러 작업한다.
- (만약 진행 과정 중 디버깅이 필요하다면 디버거를 실행시킨다.)
- 다시 시스템 콜 호출했던 유저의 영역으로 돌아가고 mode bit를 유저 모드로 전환한다.

---

## 6.2 Kernel System Call Function

스마트폰 어플리케이션으로 찍은 사진을 볼 수 있는 갤러리 어플리케이션을 만들었다고 생각해보자. 갤러리 어플은 사용자가 자신이 촬영하여 폰에 저장한 사진을 볼 수 있게끔 해준다.

어플리케이션을 제작할 때 소스코드에는 분명 스마트폰에 저장된 사진을 읽어오는 기능이 있을 것이다. 이 기능은 `library`함수를 사용하여 구현했을 것이고, 실제 동작할 때 `library`는 I/O를 하기위해 System Call을 호출할 것이다. 커널에게 부탁한다는 매커니즘이 시스템 콜이라는 점을 다시한 번 떠올리자.

```bash
# Note
별도의 함수를 만들어서 스마트폰에 저장된 파일을 읽어오는 것보다는 라이브러리로 구현된 소스코드를 사용하는 것이 훨씬 효율적이다. 만약 코드를 직접 만든다고 해도 시스템 콜을 적절히 배합해서 원하는 동작을 하게끔 구현해야하는데 굳이 이렇게 할 필요가…
```

커널에서는 유저가 원하는 사진 파일을 시스템 콜을 호출한 유저 영역으로 넘겨줘야 할 것이다. 때로는 커널이 유저 영역으로부터 데이터를 가져와야 하는 경우도 있을 것이다. 즉 어플리케이션이 제대로 동작하기 위해서는 유저 프로그램과 커널 프로그램이라는 서로 독립된 프로그램 사이에 데이터를 주고 받을 수 있는 수단이 반드시 필요하다.

![image-20200806163607107](https://user-images.githubusercontent.com/58545240/90212198-b896b300-de2d-11ea-86f0-3a2be15d99b5.png)

**그러한 기능들은 오직 커널만이 가지고 있다.** 리눅스는 멀티 유저 시스템이고 시스템의 보안을 위해서 오직 커널만이 모든 메모리에 접근이 가능하다. 좀 더 자세히 살펴보면, 커널이 유저에게 데이터를 보내줄 수는 있어도 **유저가 커널로부터 데이터를 읽어 올 수는 없고** 커널이 유저한테서 데이터를 읽어올 수는 있어도 **유저가 커널한테 데이터를 보낼 수는 없다.** 모든 I/O는 커널을 통해서만이 이루어 진다.

```bash
# Note
이쯤 되면 유저는 거의 커널의 노예라고 할 수 있다. 모든 중요한 행위는 커널에게 부탁해야한다. 감히 컴퓨터에 직접적으로 데이터를 쓴다거나 읽어온다든가 하는 행위는 절대 할 수 없다.
유저가 요청하는 데이터의 바이트의 수는 커널이 디스크에서 받아오는 것처럼 일정한 바이트의 단위가 아닌 4바이트, 7바이트 등 여러가지가 될 수 있기 때문에 커널에는 유저가 원하는 바이트 만큼 넘겨주는 기능 등이 존재한다.
```

---

## 6.3 System Call Number

그럼 커널에 대해 더 자세히 알아보기에 앞서 트랩전에 정해지는 시스템 콜 번호에 대해 구체적으로 알아보고 가자.

![image-20200806163714470](https://user-images.githubusercontent.com/58545240/90212209-bcc2d080-de2d-11ea-94ca-031ca7d1f0c7.png)

`System call number`는 커널의 `system call table`의 인덱스 번호로 사용되어 `system call function`의 주소의 시작값을 불러오는 용도로 사용된다. `System call number`는 컴파일러와 OS를 제작한 회사에서 정하며 이렇게 정해진 번호는 변경 할 수 없다.

그렇다면 리눅스에 자신만의 `시스템 콜(System Call)`을 만들 수는 없을까? `sys_write()`나`sys_read()`처럼 내가 특정 기능 수행하는 시스템 콜을 정의하고 사용할 순 없을까? 물론 직접 만들 수 있다!

![image-20200806163738941](https://user-images.githubusercontent.com/58545240/90212214-c3514800-de2d-11ea-8859-42924d150bae.png)

시스템 콜을 만들기 전에 먼저 새로운 시스템 콜을 만드는 것의 장점을 살펴보자. 우리는 새로운 시스템 콜을 만들 때 우리가 원하는 특정 기능만을 위한 코드를 작성할 수 있다. 즉 기존에 존재하는 시스템 콜 보다 간단하고 성능 또한 좋게 만들 수 있다.

```bash
# Note
예를 들어, 여러분은 컴퓨터 화면에 특정 알파벳만을 출력하는 기능을 새로 정의할 수 있을 것이고 이는 알파벳 뿐만 아니라 숫자, 기호 등을 출력해줄 수 있는 기존의 printf() 함수보다 훨씬 코드도 간결하고 효율적일 것이다.
```

분명 시스템 콜을 직접 만들어 사용하면 성능도 좋고 기존 시스템 콜보다 간결할 수 있다는 장점이 존재하지만 이보다 훨씬 큰 단점이 존재한다. 새로운 시스템 콜을 만들게 되면, 그 시스템 콜만의 새로운 `system call number`가 필요하게 된다.

이렇게 새로 제작할 때마다 `system call number`를 정의하게 되면 새로만든 시스템 콜은 그것을 제작한 플랫폼에서만 사용할 수 있다. 즉 다른 플랫폼에서 본인이 만든 시스템 콜(예를 들어 99번)을 호출하는 것은 불가능하다. 다른 플랫폼에는 99번에 해당하는 시스템 콜이 존재하지 않거나 다른 시스템 콜일 수 있기 때문이다. **플랫폼 의존적**이라는 치명적인 단점 때문에 보통 시스템 콜을 직접 만들어서 사용하는 일은 거의 없다.

또한 한번 만든 시스템 콜은 **추가만 가능하고 변경은 불가능**하기 때문에 나중에 수정을 하는 것도 불가능하다. 그렇다면 새로운 시스템 콜은 만드는 건 아예 하지 말아야 할까? 다행히도 방법은 있다.

![image-20200806163811969](https://user-images.githubusercontent.com/58545240/90212220-c815fc00-de2d-11ea-9e65-fd202bb923bd.png)

그 방법은 바로 기존에 있던 시스템 콜인 `read`나 `write`에 있는 **파일 디스크립터(File Descriptor)**을 활용하는 것이다. 파일 디스크립터는 뒤에 다루겠지만 먼저 간단히 설명을 하자면 **운영체제가 만든 파일이나 소켓을 편하게 부르기 위해서 부여한 숫자**이다.

파일 디스크립터는 보통 적은 숫자만이 활용이 되고 있어 보통은 잘 쓰지 않는 999번 등에 본인의 파일 디스크립터를 지정하고 사용하면 커널안에 내장된 시스템 콜에 영향을 주지 않고도 사용할 수 있다. 훨씬 안전한 방법이다.

`Robert M. Love`의 책에서도 권장하는 방식이고 전 세계 모든 유닉스 사용자들이 이러한 방식을 사용하고 있다고 한다.

# **7. Process Management**

---

시스템 콜에 대한 내용은 이 정도로 정리하고, **Process Management**에 대한 내용으로 넘어가자. Process Management는 커널이 하는 아주 중요한 임무 중 하나로서 반드시 짚고 넘어가야 할 부분 중 하나다.

---

## 7.1 OS Kernel

1강에서 우리는 운영체제가 어떤 역할을 하는지를 배웠다. 운영체제는 **하드웨어 자원을 관리**하고 **프로그램들을 지원**해주는 역할을 한다.

![image-20200806163848933](https://user-images.githubusercontent.com/58545240/90212233-ccdab000-de2d-11ea-923c-bd53d75748dd.png)

이와 마찬가지로 **운영체제의 핵심**인 커널 또한 같은 역할을 한다. 위 그림을 살펴보자. 커널은 위로는 프로그램들을 지원하고 밑으로는 하드웨어(CPU, Memory, Disk, TTY)를 관리하는 데이터와 기능들을 가지고 있는 프로그램이다.

```bash
# Note
실제로 위 아래 개념이 존재하는 것은 아니고 유저 프로그램과 하드웨어의 중간다리 역할을 한다는 점을 보여주기 위해 그림과 설명이 저렇게 제공된 것이다.
```

효율적인 하드웨어 관리와 유저 프로그램을 지원하기 위해 커널은 자체적인 **Internal Data Structure**을 가지고 있다.

![image-20200806163915750](https://user-images.githubusercontent.com/58545240/90212272-e3810700-de2d-11ea-90ae-6d7701e64552.png)

먼저 하드웨어 관리를 위한 **Data Structure**안에는 **각 하드웨어에 대한 정보**가 담겨있다. 예를 들어 Memory 하드웨어에 관한 Data Structure `mem`에는 이 **Memory의 크기가 어느정도이며 어디서부터 어디까지 메모리가 사용되고 있는지 등** 관리를 위해 필요한 내용들이 담겨있다.

하드웨어 뿐만 아니라 프로세스들을 관리하기 위한 Data Structure또한 존재한다. 우리는 이러한 Data Structure를 **PCB(Process Control Block)**이라 부른다. 즉 **프로세스를 지원하고 관리하기 위한 정보들이 담겨있는 데이터 구조체**이다.

위에서 설명한 프로세스와 하드웨어를 관리하기 위한 데이터가 담겨있는 데이터 구조체를 통틀어 **메타데이터(metadata)**라고 부른다.

```bash
# Note
컴퓨터공학을 공부하다보면 정말 많이 만나는 용어 중 하나가 메타데이터다. 데이터를 관리하기 위한 데이터라고 생각하면 이해가 편하다. 도서관에 수많은 책들이 존재하는데, 책들을 관리하기 위해서는 효율적인 전산 시스템이 필요하듯 메타데이터 또한 시스템에 있어 필수적인 요소다.
```

**그렇다면 프로세스를 관리하기 위한 metadata에는 어떤 정보들이 있을까?**



![image-20200806163949014](https://user-images.githubusercontent.com/58545240/90212279-e845bb00-de2d-11ea-8dff-3fb1bad23ef9.png)

`metadata`에는 다음과 같은 내용등이 담겨있다.

- PID(프로세스 식별자)
- 프로세스의 우선순위
- 대기 현상 (디스크를 읽고 쓰는 등 입출력 작업에는 waiting이 일어난다.)
- 프로세스의 상태 (동작 중인지, 수면 중인지)
- 디스크 내 이미지의 위치
- 메모리 내 이미지의 위치(메모리 안에 코드가 저장되어 있는 위치)
- 열린 파일들(유닉스에서 **파일은 바이트의 연속이고 각종 디바이스 또한 전부 파일로 취급**한다. 참고로 **제일 먼저 오픈하는 파일은 키보드와 스크린 파일**이다.)
- 현재 프로세스가 실행되고 있는 환경에 대한 정보
- 터미널
- 상태 백터 저장 공간 (`state vector save area`라는 용어 자체에 친숙해지는 것이 좋다.)

```bash
# Note
- 만약 프로세스 A가 CPU를 점유하고 있다가 디스크에 용무가 생겨 디스크에게 갔는데 디스크가 먼저 들어 온 일을 처리하고 있었다면 기다림(waiting)을 신청하고 디스크가 작업을 끝내기를 기다린다. 인간 세계에서는 대기 시간이 고작 몇 초도 안걸리는 작업이라고 생각할 수 있지만 이 정도의 시간은 CPU 입장에서는 몇억, 몇 천억년의 시간이기에 A가 기다리는 동안 A가 점유하던 CPU를 다른 프로세스에게 주게 되는데 이때 A가 하고 있던 작업 내용을 A의 PCB(Process Control Block)에 저장을 한다.

- 이때 이 저장 공간을 state vector save area라고 한다. state vector save area는 Register들을 저장하고 있는 공간이다. Register라는 건 State of Flipflop(0과 1)이 32개가 모여있는 집합이다. 프로세스의 상태들을 저장한다고 이해하면 된다.
```

- 부모, 자식 프로세스
- 실행 시간

**이처럼 metadata에는 프로세스와 하드웨어를 관리하는데 있어 필요한 모든 정보를 담고 있다.**

다음으로 `state vector save area`를 자세히 살펴보자. 먼저 앞서 예시로 들었던 **프로세스 A의 기다림(waiting) 신청**이라는 개념에 대해 자세히 설명하겠다. 도대체 기다린다는 건 뭐고 커널 내에서 어떻게 동작하는 걸까?

![image-20200806164033632](https://user-images.githubusercontent.com/58545240/90212287-ed0a6f00-de2d-11ea-84a9-a8beebd34070.png)

우리는 은행에 가서 일을 처리하려 할 때 이미 창구에 다른 사람이 먼저 일을 보고 있으면 **번호표**를 뽑고 기다린다. 이처럼 프로세스 또한 본인이 사용하고 싶은 하드웨어가 이미 다른 프로세스에 의해 사용되고 있으면 대기표를 뽑고 기다려야 한다.

프로그램적으로 위 과정을 이해해보면 다음과 같다. **프로세스가 자신의 PCB에 사용하고 싶은 하드웨어에 대한 링크를 걸어놓고 Waiting Queue(대기열)에 들어가게 된다.** 만약 본인 앞에 다른 프로세스가 똑같은 하드웨어를 사용하려고 이미 `Waiting Queue`에 있는 상황이라면 먼저 기다리고 있던 프로세스의 뒷 순서로 `Waiting Queue`에 들어간다.

이런 `Waiting Queue`중 **CPU에 링크를 걸어놓고 기다리는 것을 ready queue**라고 **하고 디스크에 링크를 걸어놓고 기다리는 것을Disk I/O queue(또는 Disk wait queue)라고 한다.**

# **8. Child Process 생성하기**

---

컴퓨터를 부팅하면 제일 먼저 **커널 프로세스**가 로드된다. 그리고 이 커널은 터미널이 켜질때 마다 그에 해당하는 **Shell**, 즉 **Child Process**를 만든다. Shell은 사용자의 입력을 기다리고 입력이 들어오면 그에 따른 작업을 수행해주는 프로그램이다. 사용자가 `Mail`이라고 입력하면 `Mail`이라는 `Child Process`가 생성 된다. 이처럼 프로세스들이 진행될 때는 자식 프로세스(Child Process)가 생성되면서 진행된다. 따라서 커널을 공부할 때 Child Process는 반드시 알아야하는 개념이다. 지금부터의 설명은 아래 그림과 함께 살펴보도록 한다.

![image-20200806164057432](https://user-images.githubusercontent.com/58545240/90212296-f4317d00-de2d-11ea-8789-5b6639473b3a.png)

```bash
# Note
잠시 여기서 프로그램과 프로세스의 차이에 대해 알아보자. 프로그램과 프로세스의 차이는 명확하다. 프로그램은 보조 기억 장치에서 실행이 되기만을 기다리는 정적인 데이터의 집합이고, 프로그램이 명령어와 데이터와 함께 메모리에 적재되면 프로세스가 되는 것 이다. 즉, 프로세스란 실행 중인 프로그램을 뜻한다.
```

1강에서 배웠듯 프로그램에는 `User Stack`과 `Kernel Stack`이 존재한다. `User Stack`은 프로그램에서 `function`을 사용할 때 사용된다. `Kernel Stack`은 유저 모드에서 시스템 콜을 통해 `커널의 function`들을 사용할 때 필요한 자료구조로 프로그램 실행에 필요한 Local Variable들을 저장하기 위한 공간이다. 만약 자료구조를 가변적인 Stack구조로 사용하지 않고 늘 공간을 확보해둔다면, 프로그램 크기가 엄청 커지고 운영 비용만 비싸질 것이다.

지금부터는 `Child Process`를 생성하기 위한 과정들을 살펴볼 것이다. `Child Process` 생성을 위해서는 먼저 Process의 정보가 들어있는 `PCB(Process Control Block)`를 만들고 그 PCB에 해당하는 Process를 만들어 줘야한다.

진행 순서는 아래와 같다.

1. **PCB 공간을 만들어 준다.** 초기값으로 **Parent Process의 PCB를 복사**해온다. Parent가 사용하던 Resource(터미널, 키보드, 스크립트)를 자식 프로세스도 사용하게 되는 것이다. `Parent Process`의 실행 환경이 `Child Process`의 실행 환경이 된다.
2. `Child Process`가 들어갈 수 있는 메모리 공간을 확보하여 초기값을 지정한다. 이를 위해 커널은 Memory의 Data Structure에 가서 빈 메모리 공간을 찾아 공간을 지정해준다. 지정된 공간에 Child Process의 값들을 넣기 전에 먼저 `Parent Process`의 **image를 똑같이 복사**를 해준다. 이 이유는 후에 등장한다. 프로세스 처리과정을 간편화하기 위해 복사한다고 일단 기억해두자.
3. 디스크로부터 `Child Process`에 **새로운 image를 로드한다.**
4. 새로 생긴 `Child Process`의 `PCB`를 **CPU의 ready queue에 등록**하여 CPU를 사용 할 수 있게끔 준비해준다. (아직까지 CPU는 `Parent Process`가 사용 중 이기 때문이다.)

이러한 4가지 과정을 시스템 콜의 용어로 정리하면 **두가지**로 정리할 수 있는데,

1. 1번과 2번의 과정을 **Fork**라고 부른다. (Parent와 동일한 것을 만든다.)
2. 3번과 4번의 과정을 **Exec**이라고 부른다. (디스크로 부터 새 이미지를 읽어온다.)

---

## 8.1 Fork

일단 Fork(포크)에 대해 알아보기 전에 Fork는 **한번 호출하면 두번 리턴한다**라는 개념으로 기억하자. 지금은 이해가지 않더라도 일단 이 사실을 받아들이고 설명을 읽어보자.

두번의 리턴 중 첫번째 리턴은 `Parent Process`가 본인이 가지고 있는 Process 상태를 그대로 `Child Process`에 복사하고 CPU의 `ready queue`에 `Child Process`를 등록 시켜놓고 다시 `Parent Process`로 리턴하는 과정이다.

```bash
# Note
단순히 함수를 호출한 후 리턴해서 그 다음 실행흐름으로 위치했다는 의미다. 프로그램적으로 너무나도 당연한 과정이다.
```

그 후 `ready queue`에 등록되어 대기중이었던 `Child Process`가 CPU를 점유하게 된다. `Child Process`가 실행되는데, `Child Process`는 만들어질 당시 `Parent Process`와 동일한 `PCB(Process Control Block)` 즉, 같은 `State Vector`를 가지고 생성되었기 때문에 **Fork를 호출하고 난 바로 그 다음 진행 시점**에서 실행된다.

즉 `Child Process`는 `Parent Process`가 가지고 있는 정보들 뿐만 아니라 **프로그램 진행 상황까지 완전히 똑같은 상태**를 가지게 되고 이런 현상 때문에 **Child Process 또한 Fork에서 리턴**하게 된다.

그렇기 때문에 한번 Fork를 해서 두번 돌아온다는 표현이 생긴 것 이다. 단, 운영체제가 이런 두 가지의 return으로 일어나는 혼동을 막기위해 리턴하는 값은 다르게 해준다. 지금까지 설명한 과정을 아래 그림과 함께 살펴보자.

![image-20200806164202055](https://user-images.githubusercontent.com/58545240/90212306-fa275e00-de2d-11ea-9895-c1910f4457ff.png)

Fork가 두번 리턴되는데 한번은 `Parent Process`로, 한번은 `Child Process`으로 리턴한다. 그리고 리턴할 때의 값은 **pid 값**이다. `pid`는 Process Id라는 의미로 이는 유닉스 시스템에서 각 프로세스에게 할당하는 고유 식별값이다. `pid`값이 0이라면 가면 `Child Process`를 의미하고 그게 아니라면 현재 실행 중인 프로세스는 `Parent Process`다.

```bash
# Note
fork()는 두 번 리턴하는데, 각 리턴값은 다음과 같다. Child Process에게는 0값을 리턴하고 Parent Process에게는 Child Process의 pid(process id)를 리턴한다.
```

아래의 프로그램을 리뷰하면서 내용을 정리해보자.

![image-20200806164234130](https://user-images.githubusercontent.com/58545240/90212312-fdbae500-de2d-11ea-99c9-1a38ffb4bf53.png)

`fork.c`라는 이름을 가진 소스파일이고, fork를 호출하는 프로그램이다. 프로그램의 출력 결과를 예상 해보고 확인하면서 지금까지 배운 `fork`를 리뷰해본다.

`fork()`를 호출하면 위 그림에 나와 있는 코드가 그대로 복사되어 `Child Process`에게 할당된다. **하나 더 생성**이 되는 것이다. 그럼 `Parent Process`와 `Child Process`는 서로 같은 코드와 상태를 가지고 있게 되는 것이다.

`fork()`호출의 리턴값 `pid`의 값에 따라 `Child Process`가 실행되거나 `Parent Process`가 실행된다. 결국엔 둘 다 실행되겠지만 둘 중 누가먼저 실행되는지 위 코드에서는 정확히 파악하기 어렵다. 보통은 부모 프로세스가 먼저 실행된다.

---

> 이번 강의에서는 System Call이 일어나는 절차에 대해 System Call Wrapper Routine, System Call Number 등을 배웠고 커널이 프로세스들과 하드웨어들을 관리하기 위해 정보를 모아둔 Data Structure인 metadata에 대해서 배웠다. 또 마지막으로 Child Process의 생성과정 중 Fork에 대해 간략히 알아보았다. 다음 3강에는 Fork를 좀 더 자세히 살펴보고 Exec에 대해 설명한다. 

---

# **9. 주요 시스템 콜 동작 원리**

---

> 2강에서 설명했던 `fork()`의 작동 원리에 대해서 이어서 설명한다. `fork()`뿐만아니라 이번 3강에서는 **다양한 시스템 콜**에 대해 학습한다. 또한 **데몬(Daemon)과 서버(Server)**에 대해서도 간단히 학습할 것이다.
>
> 시작하기 앞서 이번 강의에서 등장할 그림들에 오류가 있다는 점을 언급하고 싶다. 오류가 있는 부분은 별도로 빨간색으로 마크해서 원래 있어야할 곳으로 표식을 해놓거나 중간 중간 어떤 부분에 오류가 있는지를 언급을 했으니 부디 설명을 읽으면서 헷갈리지 않길 바란다.

---

## 9.1 Fork(2)의 동작 원리

![image-20200811161310555](https://user-images.githubusercontent.com/58545240/90212319-027f9900-de2e-11ea-9714-4e1599ec9c50.png)

그림에서 수정된 사안이 하나 있는데, **printf(“I am parent!\n”)**부분이 else 구문에 속해야하는 것이 맞다. 이점을 주의해서 아래 설명을 보자.

위의 소스코드로 동작하고 있는 프로그램이 **쉘(Shell) 프로그램**이라고 해보자. 쉘 프로그램은 사용자로부터 입력을 기다리고 입력된 명령을 토대로 프로그램을 실행하는 교통 정리 프로그램이라고 우리는 배웠다. 쉘이 시작되면 명령어를 입력할 수 있는 **터미널 혹은 프롬프트 창**이 등장할 것이고 쉘은 터미널 혹은 프롬프트 창에 사용자로의 명령이 입력되기를 기다리고 있다. 아래와 같은 화면을 생각하면 된다.

![image-20200811161331469](https://user-images.githubusercontent.com/58545240/90212329-06abb680-de2e-11ea-878a-dd197246b191.png)

우리가 쉘에 Microsoft의 Word 프로그램을 실행시키는 **word**라는 명령을 터미널에 입력했다고 해보자. 입력된 명령어를 받은 쉘은 가장먼저 **fork( )**를 진행한다. **fork( )**를 호출하면 자식 프로세스가 생성되면서 부모 프로세스와 완전히 동일한 **소스코드(image)** 갖게된다. 코드 뿐만 아니라 부모 프로세스의 **PCB(Process Control Block)**도 그대로 물려 받는다. (PCB에 대해서는 [2강 강의노트](https://medium.com/pocs/리눅스-커널-운영체제-강의노트-2-78406a13c5c9)에서 자세히 다뤘으니, 기억이 나지 않는다면 2강 강의노트에서 PCB 키워드로 검색을 해서 확인하길 바란다.)

```bash
# Note
fork()는 두번 리턴된다. 한 번의 리턴은 자식 프로세스에게 0값을 리턴하고 나머지 한 번은 부모 프로세스에게 자식 프로세스의 프로세스 아이디값을 리턴한다.
```

아직은 부모 프로세스가 **CPU를 점유**하고 있기에 **fork( )**로부터 리턴된 **pid값**은 자식 프로세스의 **pid**값이고 부모 프로세스는작업`printf("I am parent!n")`을 마저 진행한다. 자식 프로세스의 pid값을 리턴 받음으로써 부모 프로세스는 자식 프로세스를 알고 통제할 수 있는 것이다. 부모 프로세스로부터 복제되어 생성된 자식 프로세스는 현재 **ready queue**에서 **CPU**가 자신에게 할당되기를 기다리는 중이다.

앞서 2강에서 **fork( )**는 **두 번 리턴된다**고 설명한 바 있다. 첫 번째 리턴에서는 **자식의 pid(Process Id)**를 리턴하므로 if 조건문을 건너 띄고 else 구문으로 넘어간다. else구문으로 넘어가면 **printf(“I am parent!n”);**가 실행되고 모니터 화면에는 **I am parent**가 나타나게 될 것이다. 작업을 다 마친 부모 프로세스는 종료가 된다.

이후 **CPU**의 점유권은 자식 프로세스에게 넘어가게 된다. 이론상 **ready queue**에 대기하고 있던 다른 프로그램들이 없었다고 가정한다면, 부모 프로세스가 끝남과 동시에 자식 프로세스는 **CPU**를 쥐게 된다.

자식 프로세스는 어떻게 동작할까? 위에서 `fork()`가 실행되면서 부모의 코드(이미지) 뿐만 아니라 **PCB를 통째로 복사**했기 때문에 다음에 어디서부터 실행해야할지 알려주는 **PC(Program Counter)와 SP(Stack Pointer) 등 또한 복사**되었다. 즉 PCB에 존재하는 **State Vector Save Area영역 (이하 state vector로 서술함)**에 있는 `PC`와 `SP`등을 복사했기 때문에 자식 프로세스의 코드가 실행될 때는 맨 처음부터 실행되는 것이 아니라 `fork()` 중간에서부터 다시 진행하게 되어 있다.

대부분의 프로그램은 초기 실행될 때 `main()`부터 시작한다. **PCB에 그렇게 명시되어 초기화가 되기 때문**이다. 하지만 지금 다루고 있는 자식 프로세스의 경우는 PCB에서 가리키고 있는 다음 실행주소(Program Counter)가 `fork()`에 있었기 때문에, **자식프로세스는** `**fork()**`**중간 영역부터 진행**한다. (중간 영역이라는 건 `fork()` 함수가 한창 진행중일 때 복사가 일어났으므로, 그 진행중이었던 파트부터 다시 진행된다는 의미로 해석하면 된다.)

자식 프로세스가 `fork()`에서 리턴되면, **자식 프로세스 코드 안의 pid 변수는 0의 값(자식 프로세스 pid는 보통 0)**을 가지기 때문에 **I am Child \n**이 화면에 출력되게 된다. 지금까지 다룬 내용을 다시한 번 정리하면서 아래 그림을 살펴보자.

![image-20200811161413429](https://user-images.githubusercontent.com/58545240/90212367-1aefb380-de2e-11ea-87a5-5f3f39525cd0.png)

위에서 수정했던 것과 마찬가지로 일단, `printf("I am Parentn")`는 else문에 속해 있어야 하는 것을 염두하고 살펴보면, 첫 번째 출력값인 **I am Parent는 일단 부모 프로세스가 시행한 작업**이다. 그리고 부모프로세스가 끝나면서 자식 프로세스가 CPU를 점유하게 되면서 `fork()`로부터 리턴 값을 받아 if문 조건을 만족하게 되고, `printf("I am Child n")`를 실행하게 된다. 따라서 **I am Child는 자식 프로세스가 시행한 작업**이라고 할 수 있다. if문 끝단에 있는 `execlp` 구문 같은 경우는 바로 아래에서 이어서 설명한다.

## 9.2 Exec(2) 동작 원리

![image-20200811161524397](https://user-images.githubusercontent.com/58545240/90212413-365abe80-de2e-11ea-888c-b02f81c1466e.png)

**exec(2) 시스템 콜**에 대해 알아보기 전 몇 가지 배경지식에 대해 먼저 짚어보고자 한다. 위 그림을 보면서 함께 설명을 따라가보자. 먼저 `exec()`에 매개변수를 살펴보면, `/bin`이 보인다. `/bin`은 **바이너리(binary) 파일만 모아둔 폴더(directory)**를 의미한다. 그 폴더 안에는 바이너리 프로그램들이 수 십개가 존재하고 있는데, 그 바이너리 프로그램 마다 원래는 `a.out`의 형식으로 되어 있지만 **그 이름을 각자의 프로그램 제작사의 입맛에 맞게끔 설정해 놓았다(ls, cat, hwp, ppt 등).**

코드를 살펴 보면 자식 프로세스 차례가 왔을 때 `I am child!` 부분의 출력문을 출력하고, **execlp(exec 계열 함수)를 실행**하게 되어 있다. `exec` 시스템 콜은 현재 돌아가고 있는 프로세스 위에 **자신의 프로세스로 완전히 덮어씌어(over write) 버린다.** 덮어쓴 후 **exec 매개변수로 왔던 그 프로그램의 main( )으로 가는 것**이 `exec`의 작동 원리다.

새로운 프로세스가 생기는 것이 아니기 때문에, **pid(Process Id)는 변하지 않는다.** 다만 **프로세스를 구성하는 코드(기계어 코드)와 데이터, 힙, 그리고 스택 영역의 값들이 exec으로 발생하는 새로운 프로그램의 것으로 바뀌게 된다.**

![image-20200811161548608](https://user-images.githubusercontent.com/58545240/90212423-3a86dc00-de2e-11ea-97fc-905ef470ebbd.png)

설명은 위와 동일하다. **exec은 자신의 프로세스를 현재 진행 중인 프로세스 위에 덮어 써버린다.** 덮어 씀과 동시에 **date의 main( )으로 넘어가는 것**이고, 그 쪽에서 날짜를 출력해주는 작업을 진행한다. 그래서 유닉스나 리눅스에서는 **프로세스의 생성이 fork( )하고 exec( )을 하는 두 스텝으로 존재한다.**

**fork( )는 image(= 소스코드)와 PCB를 전부 복사**하는데, `exec()`의 경우에는 현재 image에 새로운**실행(execute)코드를 디스크로부터 바이너리 파일 형태로 가져온 후**에**현재 image에 덮어 씌우기(over write)를 진행**하고 **자신 프로세스의 main( )으로 진행하는 것**이다. 한마디로 기존의 작업하던 것을 자신의 프로그램으로 갈아 치우고 자신의 프로그램을 가동시키는 행위라고 할 수 있다.

## 9.3 Wait(2) 동작 원리

![image-20200811161612801](https://user-images.githubusercontent.com/58545240/90212429-3f4b9000-de2e-11ea-914d-0c98398c1270.png)

시스템 콜은 결국 **커널모드로 진입하는 것**을 뜻한다. 위 그림을 보면서 **wait( )**에 대해 알아보자. 어떤 프로그램이 `wait()`를 호출하면 **해당 프로그램의 CPU 사용권한을 박탈**한다. 위 그림의 본문 첫 줄에 등장하는 것처럼 **프로세스 P_A로부터 CPU 사용 권한을 박탈**한다(**preempt**).

![image-20200811161628504](https://user-images.githubusercontent.com/58545240/90212434-4377ad80-de2e-11ea-892f-27e1319290f4.png)

임의의 프로세스 A(위 그림에서 P_A 라고 표현되어 있음)가 `wait(2)` 시스템 콜을 호출하면 **커널모드(K)의 트랩 핸들러(Trap Handler)에 진입**하여 **wait( ) 시스템 콜 실행**을 하게 되는데, 이때 **시스템콜을 호출한 프로세스로부터 CPU를 뺐는다(preempt).**

풀어쓰자면, 커널은 보통 자신의 작업을 다 하고 나면 호출한 프로세스의 유저 모드로 돌아가야 하는데, 유저모드로 돌아가지 않는다.

커널이 아닌 프로그램은 자신의 주소(address)에 한정되서 read, jump 등을 수 할 수 있지만 **커널은 어디로든 가고 jmp(점프)할 수 있기 때문에** `ready queue`에 가서 준비된 프로세스 중 **우선순위가 가장 높은 프로그램의 PCB를 찾아서 PC(Program Counter)를 알아낸 후에 PC(프로그램 카운터)가 가리키 쪽으로 가는 것(jmp)이다.** 이 과정이 **preempt**라 부른다.

![image-20200811161645908](https://user-images.githubusercontent.com/58545240/90212437-470b3480-de2e-11ea-9b0e-4298f2acfe68.png)

그 아래의 그림을 살펴보자. 이번에는 **부모 프로세스에 초점**을 맞춰서 살펴보자. `fork( )`후에 if문을 통과한 후에 else문에서 부모 프로세스는 자신의 일을 수행한다. 모든 일을 마친 후 소스코드의 마지막으로 가보니 **wait( ) 시스템 콜을 호출**하고 있다.

![image-20200811161700182](https://user-images.githubusercontent.com/58545240/90212450-4c687f00-de2e-11ea-9eab-3092917c442c.png)

**wait( ) 시스템 콜을 호출하면, 부모 프로세스는 잠들게 된다.자식 프로세스가 끝날 때까지** 잠을 잔다(sleep). CPU는 자식 프로세스에게 넘어가고 자식프로세스는 자신이 할 일을 수행한다. 자식이 하는 일 중에 `execlp("/bin/date"...)`라는 명령어가 마지막으로 있으니 해당 명령어를 마지막으로 수행하고 자식 프로세스는 중료한다.

자식 프로세스가 종료했을 때 **CPU는**자식 프로세스로부터 **부모 프로세스를 찾는다.** 그 후 CPU는 **부모 프로세스를 대기명단(ready queue)에 등록**시킨다. 이후 **부모가 CPU 점유권을 받았을 때! 그 때가 바로 wait( ) 시스템 콜이 끝나는 지점**이다. 부모는 이후 자신의 남은 일이 있었다면 해당 작업을 진행하게 된다.

```bash
# Note
비유를 들자면, 메일 프로그램을 들 수 있다. 메일 프로그램을 이용하는 목적은 상대에게 메일을 보내는 것이므로 우리는 ‘메일 쓰기’를 클릭할 것이고, 곧 텍스트를 입력할 수 있는 에디터가 나타난다. 여기서 메일은 부모프로세스고 텍스트 에디터는 자식 프로세스라고 할 수 있는데, 우리가 메일 쓰기를 마치면 자식 프로세스(텍스트 에디터)가 종료하면서 부모 프로세스(메일 프로그램)가 다시 등장하게 된다.
```

## 9.4 Exit(2) 동작 원리

메인함수 `main()`가 끝날 때는 **반드시 exit(2) 시스템 콜이 존재**한다. 설령 우리가 소스 프로그램을 작성할 때, `exit()`을 직접 기입하지 않았더라도 **컴파일러가 알아서 main( ) { }의 마지막에 exit(2) 시스템 콜을 삽입**하게 되어 있다. 아래 그림을 살펴보자.

![image-20200811161746093](https://user-images.githubusercontent.com/58545240/90212455-512d3300-de2e-11ea-8838-db8002b33a4b.png)

자식 프로세스(`pid: 0`)의 작업 중 `execlp("/bin/date", ...)`가 있고 위에서 배웠듯이 `exec(2)`계열의 시스템 콜(`exec, execv, execlp ...`)이 실행되면서 현재 있는 프로세스 위에 **인자로 주어진 프로세스(date)를 덮어 씌어버린다.** 그리고 곧장 해당 프로세스의 `main()`을 실행시키게 된다. 원래 저 노란 박스(main 함수가 들어 있는)에는 `exit()`이라는 소스코드가 존재하지 않았다. 하지만 컴파일러가 컴파일을 할 때 삽입을 해줬고, 실제 만들어진 이진파일(binary file)을 열어 보면, `exit(2)`에 해당하는 코드가 들어있게 된다.

![image-20200811161805188](https://user-images.githubusercontent.com/58545240/90212483-66a25d00-de2e-11ea-8104-4b0817d44a11.png)

위 그림에는 `exit(2)`의 작동 원리가 좀 더 상세하게 적혀 있다. 이후 들어오는 신호들을 전부 무시해버리고, 파일들이 열려 있다면 파일들을 닫는다. 또한 메모리 영역에서 해당 프로세스가 차지하고 있는 부분(image)을 해제(deallocate) 해버리고, 부모 프로세스에게 통보한다. 그리고 `exit(2)`을 호출한 프로세스의 상태를 좀비(ZOMBIE)상태로 설정한다. (좀비 상태라는 건 다음 강의에서 다루게 된다.)

커널에서 일어나는 동작으로는 **먼저 exit(2)을 호출한 프로세스의 CPU를 빼았고, ready queue에 있던 다른 프로세스에게 CPU를 넘겨**준다. 이 과정을 **스케쥴링(scheduling)**한다고 표현하는데, 실제로 **exit(2)을 호출하게 되면 커널 안의 schedule( ) 함수가 호출**된다. 스케쥴 함수 관련 설명은 글의 마지막 3번 부분에서 다룬다.

# **10. 시스템 콜 요약 정리 (Summary)**

---

![image-20200811161833210](https://user-images.githubusercontent.com/58545240/90212494-6ace7a80-de2e-11ea-8346-e799499a1e72.png)

지금까지 우리는 프로세스를 위한 4가지 시스템 콜에 대해 살펴 보았다. `fork()`는 부모 프로세스와 아주 유사한 자식 프로세스를 만들어 내고, `exec()`은 진행 중인 프로세스 위에 새로운 프로세스 이미지를 덮어 씌운 후 `main()`으로 가게 된다. `wait()`은 이 시스템 콜을 호출한 프로세스를 잠들게 하는 것이고, `exit()`은 가지고 있던 모든 자원(resource)을 반환하고 부모 프로세스에게 알려주는 역할을 한다.

# **11. Context Switch (유저 모드와 커널 모드 사이의 전환)**

---

지금부터 설명하는 내용은, 설명과 함께 그림을 봐야 이해가 잘되니 이 점 꼭 유의해서 설명을 차근차근 살펴보자.

![image-20200811161900993](https://user-images.githubusercontent.com/58545240/90212496-6efa9800-de2e-11ea-918c-aac3bc4ddf95.png)

1. 유저가 쉘이 띄어 준 프롬프트에 **명령어(“ls”)를 입력**한다. 쉘 입장에서 이 프로세스를 실행시키기 위해서 **fork( )를 실행**한다. 여기서 **쉘은 부모 프로세스**가 되고 **새롭게 생기는 프로세스는 자식 프로세스**가 된다. `fork()`가 동작하면서 쉘의 **PCB와 쉘의 a.out(코드)을 그대로 복사**한다. 그러나 **CPU는 아직 쉘에게 할당**되어 있기 때문에 `ls`가 실행되거나 하진 않는다.
2. **부모 프로세스 쉘이 wait( )을 호출**하게 되고 **쉘은 잠들게 된다.** 잠들면서 **부모 프로세스는 CPU의 대기 리스트(queue)에 들어가게 된다.**
3. 자식 프로세스는 부모 프로세스와 똑같은 코드 및 상태를 가지고 있으므로 **fork( ) 중간에서 동작**하게 된다. `fork()`로부터 리턴된 값은 자식 프로세스를 뜻하는 `pid`값 0으로 **자식프로세스는 execlp("/bin/ls" ...)를 실행**하게 된다.
4. **디스크로부터** `**ls**`**를 로드**한다. 자식 프로세스가 **기존의 부모 프로세스(쉘)로부터 그대로 복사해왔던 이미지 위에 그대로 덮어**씌운다(overwrite). 덮어 씌운 후 `**ls**`**의 메인 코드로 가서 코드를 실행**한다(`ls`가 실행된다).
5. `ls`가 끝나면**exit(2) 시스템 콜**을 하게 되어 있고, `exit(2)`을 호출함으로써 **다시 커널모드로 들어와서 커널은 CPU를 다른 프로세스에게 할당**하게 된다. 이 때 **wait(2) 시스템 콜이 끝난 것**으로 인지를 하게 된다.
6. (그림에는 7번으로 되어 있음) 높은 우선순위를 가지고 기다리고 있던 프로세스가 없다면, **기존의 부모 프로세스(쉘)는 다시 동작**하게 된다.

![image-20200811161919872](https://user-images.githubusercontent.com/58545240/90212500-7326b580-de2e-11ea-8046-0d7987978fb9.png)

위 그림은 **쉘의 유저모드와 커널모드를 왔다 갔다 하는 것을 시간 순서로 도식화**해놓은 것이다. 위에 해당하는 부분은 가볍게 훑어보는 것으로 아래의 그림으로 넘어가 보자.

![image-20200811161937216](https://user-images.githubusercontent.com/58545240/90212508-77eb6980-de2e-11ea-8638-acc206229787.png)

Kernel의 경우 **하드웨어를 위한 자료구조**, 즉 테이블이 하나 존재한다. 그 자료구조를 위 그림에서는 **struct CPU**라고 표현하고 있다. 위 그림의 상황을 보자면, 먼저 CPU가 P1을 실행시키고 있다(**파란 글씨로 P1 was running on CPU**). 그리고 **P1이 wait(2) 시스템 콜을 호출**한다. 시스템 콜을 호출하면서 **커널은 CPU state vector(PC, SP 등)를 P1의 PCB에 저장**한다.

이렇게 상태 값을 기억하는 이유는 **wait(2) 시스템 콜이 끝났을 때 wait(2)을 호출한 프로세스가 다시 정상적으로 작업을 원활하게 진행하기 위해서**다. 보다시피 **P1과 P2의 PCB는 커널 코드 안에**있다. 위 그림의 Kernel 파트를 보면, 커널에는 2종류의 자료구조가 존재하고 있다. P1과 P2에 해당하는 PCB들을 각각 하나씩 가지고 있는데, **커널 안에는 기본적으로 각 하드웨어 자원들 마다(for each hardware resource) 자료구조**가 존재하고 또한 **각 유저 프로세스 마다(for each user processs) 자료구조가 존재**한다.

P1은 자신의 **state vector에 해당하는 값들을 P1에 대응되는 PCB에 써주고(저장하고)**, CPU는 이제 그 다음 실행해야 할 프로세스에게 자신을 넘겨줘야 한다. CPU는 ready queue를 따라가서 CPU를 쓰겠다고 줄을 서 있는 프로세스들의 PCB를 살펴보고 우선순위가 제일 높은 프로세스를 선택한다. 그 프로세스가 동작하기 위해서는 **그 프로세스에 해당하는 PCB로부터 레지스터 값들을 가져와서 자신이 가지고 있는 PC, SP 등에 저장**해야 한다. **CPU안에 있는 PC(Program Counter)가 P2의 PC로 바뀌었기 때문에 P2의 PC가 가리키고 있는 곳부터 실행(run) 된다.**

![image-20200811162008321](https://user-images.githubusercontent.com/58545240/90212520-7d48b400-de2e-11ea-82bf-665ed9caac34.png)

다음으로 **Context Switch**에서 중요한 역할을 맡고 있는 **schedule 함수**에 대해 살펴보자. `schedule()`은 internal 함수이고, `Kernel a.out`에 알려지지 않는 함수다. **internal 함수랑 정 반대되는 성격을 가진 것이 바로 시스템 콜이며, 시스템 콜은 커널 a.out에 알려지고 커널 밖에서 부를 수 있다.** (커널이 금단의 영역이라면, **시스템 콜은 그 영역에 접근할 수 있는 유일한 방법**이다.) **반면에 schedule( )은 커널 안에서만 부를 수 있는 함수**이다. 즉 유저모드(커널 밖)에서는 요청조차 할 수 없다.

우선 이 `schedule()`은 **다음에 실행될 프로세스를 찾아 선택**한다. 그리고선 **context_switch( )라는 함수를 호출**한다. `schedule()`은 `read(), wait(), exit()`과 같은 함수가 호출한다. `read()`의 경우를 생각해보면, 사실 디스크로부터 데이터를 읽어와 달라는 요청은 CPU 입장에서는 몇 억년 걸리는 일이다. 디스크에 간다고 해서 바로 정보를 읽어올 수 있는 확률은 매우 적기 때문에(다른 프로세스에서도 디스크를 사용중일 수 있기 때문에) 필연적으로 대기하는 시간이 생기게 되는데 이 시간 동안 CPU가 가만히 있을리 없다. 모든 자원은 제때 제때 효율적으로 사용이 되어야 하기 때문에 CPU를 다른 프로세스에 할당 해주어야만 한다. 그래서 `read()`에서도 `switch()` 호출이 일어나는 것이다.

**context_switch( )**를 부르면, **현재 CPU state vector를 은퇴하는 프로세스의 PCB에 쓰고,새로 등장(arising)하는 프로세스로부터 PCB를 로드**하고, **해당 PCB의 PC로부터 다시 프로그램을 진행**하는 작업을 해준다.

즉 `schedule( )`은 **CPU의 임자가 바뀌어야 할 때(read( ), wait( ), exit( ))마다 불리고, 새로운 임자에게 할당해주기 위한 내부 작업을 진행**한다.

# **12. 총정리**

---

지금까지 다뤘던 내용들을 총 엮어서 설명을 진행한다. 꽤나 복잡한 그림이 엮여 나오니 설명과 함께 따라오도록 노력해보자. 일단 아래 그림에 분홍색 구간은 커널이다. 커널 안에는 여러가지 시스템 콜이 존재하고 있다. 그리고 이 시스템 콜들은 `context_switch()`와 같은 내부함수와 연관이 있으며 각 하드웨어 자원마다 자료구조가 존재(struct CPU)한다. 아래 그림 또한 그림에 오류가 있는 부분이 있는데, 오류가 난 부분은 설명하면서 함께 나오니 너무 걱정할 필요는 없다.

![image-20200811162036018](https://user-images.githubusercontent.com/58545240/90212525-8174d180-de2e-11ea-92ae-0daaa0ae15d6.png)

1. `fork()`를 진행하면 커널로 진입한다. **커널에서 fork( )는 부모 프로세스와 똑같은 image를 생성**한다.
2. 점선으로 표시된 이유는 **아직 CPU 제어가 부모 프로세스에 있기 때문에** 자식 프로세스로 향하는 선은 점선으로 표시가 되어 있다.
3. 그 다음 `fork()` 작업이 끝나고 리턴한다. 앞서 언급했듯 부모 프로세스에서 `fork()`를 실행했을 때의 결과값과 자식 프로세스가 실행했을 때의 결과값은 다르다고 했다. 일단 **첫번째로 리턴되는 건 부모 프로세스의 PID**가 리턴되므 else문으로 가서 **wait( ) 시스템 콜을 호출**한다.
4. `wait()` 시스템 콜의 요청을 처리하기 위해 **또 다시 커널모드로 진입**한다. **wait( )은 CPU를 잠시 포기하겠다는 의미이기 때문에 context_switch( ) 함수를 실행**한다.
5. 그림을 정정해야 한다. `wait()`에서 `context_switch()`로 가는 것이기 때문에 **5번 화살표의 방향은 반대가 되어야**한다. `context_switch()` 함수가 실행되면서, 먼저 **CPU에 있던 state vector 영역에 해당하는 정보를 부모 프로세스의 PCB에 덮어 쓴다(저장한다).** 이렇게 저장을 해야 후에 자식 프로세스의 작업이 끝나고 돌아왔을 때, 부모 프로세스의 PCB에 저장되어 있는 상태값들을 보고 후에 다시 부모 프로세스로 돌아가서 남은 작업들을 원활하게 처리할 수 있다.
6. 그런데 자식 프로세스가 생겨날 때 애초에 부모프로세스에서 `fork()`가 일어나던 시점에 형성된 것이므로, 자식 프로세스의 `PC(Program Counter)`는 `fork()` 중간을 가리키고 있었을 것이다. 따라서 제어흐름은 6번 화살표를 따라 `fork()`로 가게 되고,**자식 프로세스의 시작은 fork( )에서 시작되는 것**이다.
7. 자식 프로세스에서 실행되고 있는 `fork()`의 리턴 값은 당연히 자식 프로세스의 PID일 것이다. 따라서 자식 프로세스가 실행하기로 되어 있는 `exec()`이 호출된다.
8. **exec( )이 해주는 작업은 하드 디스크에 저장되어 있는 프로그램 코드(유저가 exec 시스템 콜의 매개변수로 준 프로그램)를 불러들여 현재 진행되고 있었던 프로세스 이미지 위에 덮어 씌우는 작업**이다.
9. 따라서 디스크에 유저가 `exec()`시스템 콜에 매개변수로 넘긴 `ls`에 해당하는 프로그램이 **현재 진행중이었던 쉘(자식 프로세스) 위에 덮어 씌어지게 된다.**

![image-20200811162058192](https://user-images.githubusercontent.com/58545240/90212534-876ab280-de2e-11ea-8405-4a890757f4e1.png)

10. 덮어씌어진 후에 **ls 프로그램의 main( )으로 흐름이 넘어간다.**

11. `ls`의 코드가 전부 실행된 후 **exit( )이 호출되면서 흐름은 12번으로 넘어간다.**

12. 소스코드 상에 `exit( )`이 존재하지 않아도 컴파일러가 알아서 삽입을 해주기에, `exit()`을 무사히 실행할 수 있다. `exit()`은 지금까지 **진행중었던 프로세스로부터 CPU를 뺐고 다른 프로세스에게 재할당해 주는 과정**이 있기에 마찬가지로 **context_switch( )를 호출하게 된다.**

13. 자신을 호출한 프로세스로부터 **CPU를 뺐고, ready queue에 가서 CPU를 기다리고 있던 프로세스 중 우선순위가 높은 프로세스를 골라서 해당 프로세스의 PCB 안의 상태값들을 현재 CPU의 레지스터에 복사 붙여넣기(복붙) 한다.**
14. ` ready queue`에 부모 프로세스만 남아있다고 가정한다면, 부모 프로세스가 선택되어 실행될 것이고 부모 프로세스는 `wait()`을 진행하고 있었기 때문에 `wait()` 중간부터 다시 실행된다.
15. 14번까지의 작업이 끝났다면 쉘은 다시 사용자로부터 또다른 명령을 기다리고 있게된다.

## 12.1 용어 정리

총정리인 만큼, 기존의 프로그램과 프로세스 차이에 대해서도 한 번 짚어보고 가도록 한다.

![image-20200811162228948](https://user-images.githubusercontent.com/58545240/90212564-9d787300-de2e-11ea-8ae6-fde355072c02.png)

**프로그램이 실행중일 때** 우리는 프로그램을 **프로세스**라 부른다. `a.out` 형식을 가지고 `main()`함수부터 시작하게 되어 있다. 스케쥴링과 보호의 단위이고, 자원을 할당받는 과정을 수반하고 유저모드와 커널모드를 왔다갔다 하면서 진행된다.

![image-20200811162244670](https://user-images.githubusercontent.com/58545240/90212573-a1a49080-de2e-11ea-8f57-bf6bedc0fec3.png)

유저 영역(user space)의 **text는 instruction(명령문)을 의미**한다. `data`와 `bss`에 대한 설명은 위 그림의 Note파트에 서술되어 있다. 먼저, 두 개의 배열(array)이 존재한다. A라는 배열은 초기값을 할당해줬고 B라는 배열에는 초기값을 주지 않았다. 만약 배열의 크기가 100만 정도에 전역변수로 선언되어 있다면? A 배열처럼 초기값을 할당 해줬다면 디스크에서 백만 개의 셀을 갖고 있어야 한다(사전에 자원이 지급됨). 만약 초기값을 주지 않았다면 디스크에 실제로 존재하진 않고 해당 배열이 실행 중 로드 될 때만 할당되게 된다.

**초기에 값이 할당된 부분을 data**라고 하며 **초기에 할당되지 않은 데이터 부분을 bss**라고 한다. **heap은 동적 메모리 할당**에 쓰여지는 데이터 영역이며 **stack은 함수 호출 등에 사용되는 자료구조**다.

커널 영역(kernel space)에는 **PCB와 stack**이 존재한다. HW(CPU) 쪽에서는 **state vector**가 존재한다. 이런 것들을 합쳐서 우리는 **context라고 부른다.**

## Daemon (데몬) 또는 Server

서버(혹은 데몬)는 무엇일까? 근본적으로 서버는 **a.out(실행 파일)이다**. 다만 조금 특이한 알고리즘을 가지고 있을 뿐이다. 아래 그림을 살펴보자.

![image-20200811162320894](https://user-images.githubusercontent.com/58545240/90212581-a5d0ae00-de2e-11ea-9296-cb7e5a5a6e83.png)

맨 처음 서버 혹은 **데몬이 시작되는 건 부팅 될 때(boot time)**다. **부팅하고 나서 대부분의 시간은 잠들어 있다. 요청이 올 때만 해당 요청을 서비스 해주고 서비스가 끝나면 또 잠들게 된다. 이런 프로그램을 우리는 데몬 또는 서버라고 부른다.** 만약 프린트 서버가 존재한다고 하면, 프린트 서버는 말 그대로 프린트 요청이 올 때만 프린트를 해주고 그 이외에는 잠든다. 네트워크 서버 또한 네트워크 요청(연결, 해제 등)이 올 때만 처리하고 그 이외에는 잠든다.

서버라는 것은 하드웨어의 개념이 아니라 **소프트웨어의 개념**인 것이다. **항상 incoming request가 오는지 안 오는지 지켜보고 있으며 서비스가 올 때만 서비스를 해주게 되어 있다.**

![image-20200811162335435](https://user-images.githubusercontent.com/58545240/90212585-a9fccb80-de2e-11ea-9782-7c8925d23216.png)

리눅스 시스템에서 사용되는 **명령어 ps(Process State)를 살펴보자**. 현재 기기에서 **어떤 프로세스가 작동**하고 있는지를 나타낸다. `-e` 옵션의 경우 시스템 프로세스까지 전부 보여주는 명령어다. 웹서버나 네트워크서버 등의 모든 시스템 프로세스의 상태를 보여주는 명령어다.

![image-20200811162351823](https://user-images.githubusercontent.com/58545240/90212589-ad905280-de2e-11ea-8187-5055f00c9169.png)

보통 데몬이나 서버 프로그램의 경우 이름 뒤에 `d`자가 붙는다. `httpd`는 웹에서의 통신에 사용되는 데몬이고, `ftpd`는 파일전송 서버를 나타내는 등 다양한 서버와 데몬이 존재하고 있다.

---

> 생각만큼 크게 어렵지 않았던 3강이다. 결국 모든 프로그램은 알고리즘을 이해하는 것이 전부가 아닐까 하는 생각이 든다. 애초에 프로그램이란 건 논리의 집합이고, 해당 논리대로 작업을 하는 것이니 그 논리만 파악하고 있으면 그 프로그램을 아는 것이니까. 그나저나 강의 노트를 작성하는 건 생각만큼 쉬운 일이 아니라는 걸 다시한 번 체감한다. 내가 듣고 이해하는 것과 다시 누군가에게 풀어서 설명하는 건 천지차이니까.

---

# **13. 복습**

---

> 이번 4번째 강의에서는 `fork()`를 통해 프로세스를 생성해 내는 과정에 대해 더 자세히 알아보는 시간을 갖는다. 또 PCB 내용을 분류해 볼 것이며 `fork()`와는 조금 다른 `clone()`에 대해서도 다룰 예정이다. 이번 강의는 부모 프로세스가 어떻게 자식 프로세스를 어떤 과정을 통해서 만들어 내는지를 확실히 알아야 이해할 수 있기에 먼저 지금까지 배운 내용 중 일부분을 복습을 하고 4강을 진행 할 것이다.

---

지금까지 한 내용들은 아래 등장하는 두개의 그림에 잘 정리 되어있다. 그림에 나와있는 순서들을 머리속에 담아만 둘 수 있다면 앞으로 좀 더 심화적인 내용을 이해할 때 큰 도움이 될 것이다. 그럼 지금부터 그림과 함께 설명을 보도록 하자.

![image-20200811162640825](https://user-images.githubusercontent.com/58545240/90212592-b2550680-de2e-11ea-829e-e785933c30c8.png)

가운데에 보라색 박스로 그려져 있는 커널이 있다. 그리고 좌측에 유저가 작성한 프로그램인 쉘이 있다. 또 여기서 살펴볼 프로세스는 쉘 프로세스로 부모(Parent)와 자식(Child) 두개가 존재한다. 실제로 동작할 때는 훨씬 더 많은 프로세스들이 동작하고 있기 때문에 CPU 자원을 바로바로 받지는 못한다는 점을 알아두고 아래 흐름을 살펴보자.

1. 먼저 우리 프로그램에다 `ls` 명령어를 쳤다고 가정하자. 그러면 프로그램은 `ls`라는 자식 프로세스를 만들려고 할 것이다.
2. 그럼 자식을 만들기 위해 먼저 `fork()`를 실행한다. 이때 이 `fork()`는 쉘에 있는게 아니라 커널안에 있는 것이다. 시스템을 직접적으로 다루는 중요한 동작은 모두 커널이 관리한다. `fork()`는 작성된 프로그램과 똑같은 데이터를 복사해 만들어 줄 것이다. 그림에 표시된 점선은 제어흐름이 넘어간다는 뜻이 아니라 단지 데이터만 복사 된다는 뜻이다.
3. 그렇게 `fork()`를 하고나서 다시 돌아와서 **PID 값**을 비교해 보니 자식 프로세스의 pid값이 리턴되었으므로 현재 부모 프로세스 제어흐름에 있다는 뜻이므로 `else`로 간다. `fork()`는 두번 리턴되는데 한번은 부모 프로세스에게 `fork()`로 만들어진 자식 프로세스의 pid값을 넘겨주고 한번은 자식 프로세스에게 0값을 넘겨준다. 자식 프로세스의 제어 흐름에는 0값이 전달된다.
4. 이렇게 `else`로 들어온 부모 프로세스는 시스템 콜인 `wait()`을 호출한다. 이때 `wait()`를 한 이유는 부모 프로세스가 **CPU**를 포기하고 자식 프로세스에게 CPU를 넘겨주기 위한 것이다. 즉 실행흐름을 자식 프로세스에게 넘겨주기 위함이다.
5. 그러면 `wait()`에서 CPU를 넘겨주기 위해 `context_switch()`를 실행하면서 지금까지 동작했던 부모 프로세스의 `state vector`들을 부모 프로세스의 `PCB(Process Control Block)`에 저장한다. 그 후 **CPU**를 기다리고 있는 프로세스들의 정보가 있는 `ready queue`에 가서 우선순위가 제일 높은 프로세스의 `PCB`를 **CPU**에 연결 시켜 준다. 이때 알아야 할 내용은 커널은 유저마다 **커널 스택**을 하나씩 가지고 있다는 점이다. 현재 커널 스택에는 `wait()`와 관련된 지역 변수들이 먼저 들어가 있다. 그리고 그 위에 `context_switch()`에 관련된 지역 변수들이 저장되어 있다. 부모 프로세스의 `PCB`에는 이러한 정보들이 저장되어 있다.
6. **CPU**를 처음으로 넘겨받은 자식 프로세스는 `return`부터 해야하는 상황에 처해있다. 자식프로세스는 만들어 질때 부모 프로세스의 상태정보를 똑같이 복사해 만들어지기 때문에 `fork()`작업을 마무리 하고 있던 부모프로세스의 상황 또한 그대로 복사 되기 때문이다. 그래서 자식 프로세스는 `fork()`로 `return`을 하게 되면서 `fork()`는 두번 리턴한다는 개념이 생겨난 것이다. 단지 이번에는 자식 프로세스의 실행흐름이라는 점이 다르고 리턴된 `pid`값이 0이고 0값을 토대로 `if`와 `else`중 프로그램 내에서 어떤 제어흐름으로 갈지를 결정하게 된다.
7. 리턴된 `pid`값이 0인것을 보면 자식 프로세스라는 뜻이므로 `if`문 안으로 들어가게 된다. 거기서 `exec()`을 하게 된다.
8. 위 그림에서는 `exec()`에 매개변수가 `ls`인 상황이다. 이 명령어는 매개변수로 넘어온 프로그램을 찾고 해당 프로그램 이미지를 로드한다. 따라서 `exec()`이 실행되면서 디스크에 가서 `ls`를 찾는다.
9. 그 후 자식 프로세스쪽에 디스크에서 찾은 `ls`내용을 덮어씌운다. 이로서 자식 프로세스는 더 이상 부모 프로세스의 복제품이 아닌 자신만의 역할을 하는 프로세스로 된다.

![image-20200811162702431](https://user-images.githubusercontent.com/58545240/90212594-b7b25100-de2e-11ea-8c31-ad8b4cd6518b.png)

10. `exec()`을 통해 디스크에서 `ls`를 가져와 현재 이미지(코드)에 덮어씌우고

11. 자식 프로세스는 자신이 할 일을 진행한다. 할 일이란 `ls`가 하는 작업과 동일하다.

12. 일을 다 하고나면 이제 CPU가 필요 없으니 프로세스를 종료하기 위해 시스템 콜 `exit()`을 호출한다.

13. 그럼 이제 또 CPU를 다른 프로세스를 주기 위해 `context_switch()`를 하게 되고 이때 부모 프로세스의 PCB를 불러온다. 그럼 이때 커널의 스택에는 `wait()`와 그 위에 `context_switch()`가 쌓여있는 상태로 있다. 보라색 커널 영역의 그림에는 스택이 반대로 표현되어 있다. 또한 `wait()`과 `context_switch()`사이에 있는 `exec()`과 `exit()`은 중간에 분명 스택에 쌓이긴 했으나 13번 실행흐름 전에 각각 실행이 끝나면서 스택에서 빠져나가 있는 상태다.

14. 마지막으로 스택의 가장 상위에 위치하고 있는 `context_switch()`에서 `wait()`으로, 그리고 `wait()`에서 다시 부모쪽에서 시스템 콜 `wait()`을 호출한 곳으로 돌아가게 된다.

*이렇게 하면 `fork()`의 과정이 끝이난다. 복습을 통하여 부모의 프로세스가 어떻게 자식 프로세스를 생성하고 자식 프로세스는 어떻게 종료되는지에 대해 알아보았다. 그렇다면 이제 본격적으로 `fork()`를 통해 프로세스를 생성하는 과정을 자세히 알아보자.*

# **14. Process Create**

---

부모 프로세스가 자식 프로세스를 만들어 내는 작업을 할 때는 두번의 오버헤드(overhead)가 발생한다. 이 오버헤드들은 `fork()`를 하는 도중 발생하며, 첫번째 오버헤드는 부모 프로세스의 이미지를 자식에게 복사할 때 생기고 두번째는 부모 프로세스의 PCB를 자식 프로세스에 복사하며 생긴다.

작업 과정을 자세히 알아보기 전에 먼저 PCB의 구성에 대해 알아보자.

```bash
# Note
오버헤드: 어떤 처리를 하기 위해 들어가는 간접적인 처리 시간, 메모리 등을 말한다.
A라는 처리를 10초만에 했지만 안전성 고려 때문에 처리가 15초가 걸리는 B의 방식은 오버헤드가 5초가 발생한 것이다. 또한 이러한 B의 방식을 개선해 12초가 걸리면 오버헤드가 3초 단축되었다고 말한다.
```

## 14.1 리눅스의 PCB와 Thread

![image-20200811162926413](https://user-images.githubusercontent.com/58545240/90212619-cdc01180-de2e-11ea-955b-468da4afae14.png)

PCB에는 다양한 정보들이 수 킬로바이트라는 꽤 큰 용량으로 구성되어 있다. 이러한 PCB의 내용을 분류를 하자면 **task basic info**와 프로세스가 오픈한 파일들에 대한 정보가 들어있는 **file**, 프로세스가 접근 중인 file system에 대한 정보인 **fs**가 있으며 프로세스가 사용 중인 터미널 정보 **tty,** 사용 중인 메인 메모리에 대한 정보 **mm**과 여러 신호 정보인 **signals**들로 나눌 수 있다.

리눅스는 이렇게 분류된 요소들을 하나의 구조(Struct)로 묶지 않고 그림 오른쪽에 나와 있는 것 처럼 **6개의 구조**로 나눠 관리한다.

![image-20200811162940267](https://user-images.githubusercontent.com/58545240/90212629-d284c580-de2e-11ea-84a0-3674088a269e.png)

먼저 그림의 왼쪽 상자에 나와있는 것처럼 크게 `task_struct`가 있다. 이건 리눅스가 가지고 있는 `PCB`인데 그 안에는 여러개의 `struct`들에 대한 내용이 있고 그 옆에 보라색으로 `*mm, *tty등`이 있는 것을 알 수 있다. 보라색으로 표시되어 있는 **포인터(\*)**를 따라가면 각각이 가르키는 파일, 메모리를 등을 만나볼 수 있다. 이는 오른쪽 그림에도 나와있는데, 그림을 보면 왼쪽 노란 상자에 `task basic info`가 있고 그곳에서 화살표로 가르키는 곳을 따라가면 각각의 구조(struct)들이 나온다. 이처럼 **리눅스의 PCB**는**1개의 구조가 아닌 6개**의 구조로 나눠져 있다.

그렇다면 리눅스는 어째서 1개가 아닌 6개로 나눠서 관리하는 것일까.

![image-20200811162955337](https://user-images.githubusercontent.com/58545240/90212637-d6184c80-de2e-11ea-83ca-44eb4c0f10cd.png)

위 그림의 왼쪽에 있는 노란 상자들을 보자. **저 6개의 상자들이 있어야 하나의 PCB**이다. 전에 `fork()`를 통해 자식 프로세스를 생성하고 이때 부모 프로세스의 정보를 그대로 복사한다고 했는데 그 정보가 바로 위 그림에 나와 있는 정보다.

그렇다면 부모 프로세스의 노란색 상자 6개 구성요소가 전부 자식 프로세스에게 복사되는 것일까? 만약 그렇게 `fork()`가 동작한다면 `files, fs, tty, mm, signals` 등을 각각 읽고 쓰는데 많은 자원이 사용된다. **모든 걸 복사**해서 자식 프로세스를 만든다고 했을 때 부모 프로세스의 PCB 정보를 `read()`할 때 사용되는 바이트, 자식 프로세스의 PCB에 `write()`할 때 필요한 바이트가 각각 필요하므로 상당한 부하가 걸린다. 따라서 이러한 제작 방식을 **heavy-weight creation**이라 칭한다. 초기 리눅스가 구현될 때는 이런 방식으로 구현되었다고 한다.

그러나 막상 시스템을 만들다보니 부모 프로세스가 가지고 있는 `tty(터미널)`나 `fs(파일 시스템)`는 자식이 가지고 있는 것과 동일한 경우가 많다. 즉 복사하는 게 아니라 공유를 할 수 있다. 자식 프로세스에게는 부모 프로세스가 가지고 있는 `tty`나 `fs`등의 주소만 알려줘서 같은 자원을 공유하는 방식으로 생성되는 것을 **light-weight creation** 칭한다.

자식 프로세스가 부모 프로세스와 다르게 사용할 것들만 **선택적으로 복사**하자라는 아이디어로 구현한 이 방식은 전부 복사할 때 들어가는 하드웨어 자원과 오버헤드를 최소화 시키는 장점이 있다.

자, 그럼 위에서 배운 개념을 바탕으로 좀 더 구체적인 예시를 들고 이해해보자. 게임을 만든다는 상황을 가정해보자.

![image-20200811163014029](https://user-images.githubusercontent.com/58545240/90212643-d9abd380-de2e-11ea-97b9-b47d69b560b4.png)

위 그림을 보면 정 가운데에 검은 네모 상자는 메인 메모리를 뜻한다. 맨 위의 박스들은 여러개의 CPU를 표현하고 있고 CPU마다 각각 프로그램 카운터를 내장하고 있다. 이런 상황에서 `Game XYZ`가 실행되고 있다고 가정하자. 이 게임은 지금 `CPU #0`위에서 실행되고 있고 `Game XYZ` 프로세스를 `CPU #0`의 `PC(프로그램 카운터)`가 가르키고 있다. 그리고 각 `CPU`를 위해서 `PCB`가 좌측에 노란 상자로 존재하고 있다. 각 `PCB`는 6개의 구성요소로 되어 있다.

이런 상황에서 자식 프로세스를 전통적인 방법으로 만들었다고 생각해 보자. 그럼 `a.out`도 복사하고 `PCB`도 똑같이 복사해서 자식 프로세스를 만들 것이다. 이렇게 되면 위에서 말했던 것처럼 오버헤드가 발생한다. 어떻게 하면 오버헤드를 줄여줄 수 있을까?

```bash
# Note
`프로그램 카운터(Program counter,PC)`: 마이크로프로세(중앙 처리 장) 내부에 있는 레지스터 중의 하나로서, 다음에 실행될 명령어의 주소를 가지고 있어 실행할 기계어 코드의 위치를 지정한다. 때문에 명령어 포인터 라고도 한다.

`프로세서 레지스터(Processor Register)`: 컴퓨터의 프로세서 안에서 자료를 보관하는 아주 빠른 기억 장소이다. 일반적으로 현재 계산을 수행중인 값을 저장하는 데 사용된다.

`a.out`: 과거 유닉스 계통 운영 체제에서 사용하던실행 파일과목적 파일형식. assembler out의 약자이다.
```

![image-20200811163107160](https://user-images.githubusercontent.com/58545240/90212649-dd3f5a80-de2e-11ea-8518-cbe39fe60775.png)

오버헤드를 줄이기 위해 고안된 방법이 바로 자식 프로세스를 생성할 때 프로세스로 만들지 않고 **Thread**로 만드는 것이다. **Thread**는 모든 구조를 복사해 오는게 아니라 **CPU관련 정보들을 가지고 있는 Task basic info만 복사해 오는것을 칭한다.** 아래 그림을 보면 자식들은 부모의 구조들을 전부 복사해오지 않고 **Task basic info**만 복사해 와서 나머지는 부모와 공유해 사용한다. 이러한 방식을 light-weight creation이라고 한다.

```bash
# Note
`프로세스`와 `스레드`의 차이: 프로세스는 운영 체제로부터자원을 할당받는 작업의 단위이고 스레드는 프로세스가 할당받은자원을 이용하는 실행의 단위이다.
```

![image-20200811163140061](https://user-images.githubusercontent.com/58545240/90212654-e0d2e180-de2e-11ea-9db9-920535f39bf7.png)

위의 내용들을 정리해보면, 리눅스에서 **Thread**는 `PCB`에서 **Task basic info만 복사**해오고 다른 PCB 데이터는 **공유**를 한다. 덕분에 데이터의 복사는 줄고 자식 프로세스를 만들때의 오버 헤드가 최소화 된다. 그래서 리눅스에서의 **Thread**라는 것은 프로세스를 만들때 **light-weight 방식**으로 만든다고 한다. 그리고 이런 방식은 단순히 복사를 하는 `fork()`가 아닌 `**clone()**`이라는 시스템 콜을 사용한다.

![image-20200811163202711](https://user-images.githubusercontent.com/58545240/90212665-e4666880-de2e-11ea-9da6-cba562ccfa6e.png)

`clone()` 시스템 콜을 살펴보기 위해 위의 그림을 살펴보자. 가운데 `clone()` 시스템 콜이 있다. `clone()`을 호출할 때 부모 프로세스는 바이너리 비트 5개를 매개변수로 넘긴다. 만약 **이 5개의 비트가 전부 11111이면 모든걸 복사하고 00000이면 Task basic info만 복사해오는 제일 light-weight 복사 방식을 하라는 것을 뜻한다.** 이러한 방식으로 생성된 자식 프로세스는 프로세스라 하지 않고 **Thread(스레드)** 칭한다.

여기서 만약 `clone()`의 **바이너리 비트 5개가 clone(11111)이면 모든 걸 복사하는 전통적인 heavy-weight 방식인 fork()를 해달라는 의미와 같은 뜻이 된다.** 이렇게 생성된 자식 프로세스는 프로세스가 된다.

```bash
# Note
Unlike fork(2), these calls allow the child process to share parts of its execution context with the calling process.
```

# **15. Process Copy**

---

지금까지는 프로세스가 생성 되는 과정에 대해서 알아보았다. 부모 프로세스가 자식 프로세스를 생성 할 때 두가지 오버 헤드가 생긴다는 것을 배웠다. **첫번째는 PCB를 복사할 때 생기고 두번째는 image를 복사 할 때 생긴다.** **PCB는 하얀색 도화지의 속성(크기, 질감, 모양)이라면 image는 그 도화지 위에 색칠된 그림이라고 볼 수 있다.** 그렇기에 PCB보다는 **image를 복사해 오는 오버 헤드가 더 크게 발생한다.**

부모 프로세스한테 `ls`를 명령하면 바로 자식 프로세스가 자신만의 속성을 갖고 생성되는 것이 아니라, 먼저 부모 프로세스의 상태 정보를 복사하고 그 위에 자식 프로세스가 갖는 속성을 덮어 씌운다. 근데 생각해보면 이런 과정 자체가 너무 비효율적인 과정이라고 생각 할 수도 있다. **어차피 덮어씌울 걸 왜 굳이 부모의 image까지 복사하는 과정이 필요한 것일까.**

물론 위와 같은 완전한 복사가 항상 비효율적이라는 것은 아니다. 어떤 유저는 부모 프로세스가 가지고 있던걸 정확히 똑같이 복사하고 싶어할 수도 있다. 예를 들어 hwp 문서를 키고 또 똑같은 hwp 문서를 새로 키고 싶어하는 경우도 있을 수 있기 때문이다. 그러나 대부분은 이메일을 키고 거기서 이메일을 쓰는 일을 하듯이 부모 프로세스와는 다른 일 처리를 하는 경우가 대부분이다.

그래서 고안된 아이디어는 다음과 같다. 모든 코드를 복사해 오는 것이 아니라 **페이지 매핑 테이블만 복사**해 오는 방법이다. 이 방법을 사용하면 **자식 프로세스는 image를 부모 프로세스로부터 가져오는 것이 아니라 부모 프로세스의 image를 가르키는 페이지 매핑 테이블만 복사해서 가져오게 된다.** 자식 프로세스는 페이지 매핑 테이블을 가지고 execute를 하게 되고 Instruction(실행 명령)을 가져오는 동안에는 부모와 **같은 페이지**를 쓸 수 있게 된다.

```bash
# Note
`페이지 테이블`: 페이징 기법에 사용되는 자료구조로서, 프로세스의 페이지 정보를 저장하고 있는 테이블이다. 테이블 내용은 해당 페이지에 할당된 물리 메모리의 시작 주소를 담고있다.

`페이징 기법`: 컴퓨터가 메인 메모리에서 사용하기 위해 2차 기억 장로부터 데이터를 저장하고 검색하는 메모리 관리 기법
```

그런데 이렇게 같은 페이지를 사용하다 보면 문제가 생기는 경우가 발생한다. 자식과 부모 프로세스 둘 다 페이지에서 `read()`해서 데이터를 읽는건 상관이 없지만, 만약 페이지에 **write()**를 해서 무언가를 페이지에 쓰게 된다면 어떻게 될까.

`write()`를 하는 경우에만 한정해서 그 **페이지만 부모와 자식에게 하나씩 복사**본을 따로 만들어 주면 된다. 이런 과정을 **Copy on Write(COW)**라고 부른다. 아래 그림을 살펴보자.

![image-20200811163304453](https://user-images.githubusercontent.com/58545240/90212683-e9c3b300-de2e-11ea-96e8-4d9391a7534b.png)

**Copy On Write**방식을 이용하면 처음 시작할 때 자식 프로세스는 페이지 테이블만 가지고 있을뿐 독자적인 image라는 것은 없다. **그러나 부모나 자식 프로세스중 하나라도 페이지에 변화를 주면 그 부분만 각각 복사를 하게 된다.** 이런 방식은 image를 약간은 게으르게(lazy) 만들어 주는 방식으로 볼 수도 있을 것이다.

위 그림을 보면서 다음 설명을 같이 따라가 보자. 먼저 부모 프로세스쪽에서 `fork()`를 하게 된다. 부모 프로세스 정보를 전부 복사 하는 것이 아니라 **COW 방식으로 페이지 테이블만 복사해 가져온다.** 그리고 나서 `fork()`를 했던 곳으로 돌아온다. 그리고 나서 `wait()`시스템 콜을 호출해서 CPU를 자식 프로세스에게 넘겨주려 할 것이다. 그럼 CPU가 자식 프로세스에게 넘어가서 자식도 `fork()`로부터 리턴해서 자식 프로세스만의 작업들을 수행할 것이다. 이때 자식이 페이지를 읽어 오는건 상관없지만 **write()를 하게 되면 그 페이지에 대해서만 복사를 한다.**

그런데 보통 자식 프로세스는 `fork()`에서 돌아오면 거의 바로 `exec()`을 하게 된다. 즉 전에 부모 프로세스의 이미지를 복사 했든 안했든 자신만의 이미지(코드)로 싹 갈아엎는다. 여기서 문제가 발생하는데, 부모 프로세스가 `fork()`를 하고나서 돌아오고 나서 문제가 생긴다. 부모 프로세스가 `fork()`에서 돌아와서**바로 wait()를 안하고 다른 일을 처리할 경우 3분의 1 정도는 보통 write()의 기능을 한다.** 그 말인 즉슨 자식 프로세스가 CPU를 점유하기 전에 페이지 테이블에 계속 변화가 발생하게 되는 것이다.

이런 행위는 계속해서 **Copy On Write**를 하게 될 것이고 **이렇게 복사 된 값들은 사실 자식 프로세스가 exec()을 하게 되면 어차피 덮어 씌워지기 때문에 결국에는 의미없는 복사를 하고 있는게 된다.** 그렇다면 어떻게 하는 것이 효율적인 방법일까. 아래 그림을 살펴보자.



![image-20200811163323148](https://user-images.githubusercontent.com/58545240/90212737-0b249f00-de2f-11ea-92a5-d94c8e1ef857.png)

**그림의 위쪽 부분은 위에서 설명한 불필요한 COW가 발생하는 경우를 나타낸 것이다.** `fork()`로 돌아온 부모 프로세스가 자식 프로세스에게 `wait()`으로 CPU를 넘겨줄 때 까지 계속해서 의미없는 COW를 만들어 내고 있는 그림이다.

이를 해결하기 위해서 다음과 같은 새로운 방법을 사용한다.

1. 부모 프로세스가 `fork()`를 호출해서 자식 생성을 끝내고 `fork()`를 했던 곳으로 돌아가려 한다. 즉 커널에서 유저모드로 돌아가려 하고 있다.
2. 이때 `fork()`안에서 **자식 프로세스의 CPU 우선 순위를 확 높여버린다.** 이렇게 우선순위를 높이는 이유는 커널에서 유저모드로 돌아갈때에는 우선순위가 제일 높은 프로세스한테 CPU를 넘겨주기 때문이다.
3. **이렇게 되면 CPU가 부모한테 돌아가는 것이 아니라 자식 프로세스한테 가게 된다.** 이렇게 CPU를 받은 자식 프로세스는 바로 `exec()`을 하게 되고 CPU를 다 쓰게 되면 `exit()`으로 CPU를 다음 순서로 넘겨주게 된다.
4. 그럼 이제 부모 프로세스가 CPU를 받게되고 `fork()`에서 돌아오고 본인이 할 일을 하게된다.

**이러한 방식을 통해 쓸데없이 일어나는 COW를 방지하고 자식 프로세스는 성공적으로 복사된다.**

---

> 4강에서 핵심을 뽑자면 아래의 2가지를 꼽을 수 있겠다.
>
> 1. 리눅스가 PCB를 6개의 구조로 나누어서 관리한다는 것과 PCB를 전부 복사하지 않고 필요한 것들만 복사하는 것을 **Thread(Light Weight Creation)**라고 한다는 점.
> 2. 복사한 페이징 매핑 테이블의 불필요한 Copy On Write를 방지하기 위해 `fork()`에서 돌아올때 부모가 아닌 자식에게 먼저 CPU를 줘 오버 헤드를 막는점.
>
> 고건 교수님께서 친절하게 `fork()`의 과정까지 복습을 해주시고 나서 심화 내용을 들어가 차근차근 이해하기 쉽게 강의를 따라갈 수 있었던 것 같다.

---

# **16. Kernel Thread(커널 스레드)**

---

> 이번 5번째 강의에서는 프로세스간 CPU 점유권의 이동이 어떤 매커니즘으로 이루어지는지를 다루게 된다. CPU를 할당해준다는 것은 단순히 프로세스의 우선순위 말고도 고려해야할 것들이 많다. 리눅스 운영체제는 과연 이러한 숙제를 어떻게 풀고 있는지 지금부터 살펴보려 한다.

---

먼저 강의노트 4에서 다뤘던 내용들을 잠시 떠올려보자. **스레드(Thread)가 있고 프로세스(Process)**가 있었다. **프로세스**는 **부모의 것(Task basic info + files, fs, tty, mm, signals)을 전부 그대로 복사**한 것(heavy-weight creation)이고,**반대로 최소한으로 복사(light-weight creation)한 것이 스레드**이다.

또한 커널은 **메모리 상주 프로그램(memory resident program)**이다. `main()`함수가 있는 **평범한 프로그램의 특성 +부팅할 때 부터 메모리에 올라와서 컴퓨터의 전원을 완전히 차단할 때까지 메모리에 상주**한다는 특성을 함께 가지고 있다. 여기까지 떠올렸다면 이제 아래 그림을 보면서 오늘 다룰 주제 중 하나인 **커널 스레드**를 살펴보자.

![image-20200811163518896](https://user-images.githubusercontent.com/58545240/90212742-0f50bc80-de2f-11ea-99cd-54da4747f672.png)

위 그림의 우측 보라색으로 칠해진 부분은 커널영역을 의미한다. 컴퓨터가 **맨 처음 부팅(booting)하면 분명 커널의** `**main()**`**부터** 실행할 것이다. 그 후 커널 프로세스가 동작하던 중 커널 프로세스 내에서 **시스템 콜** `**clone()**`**을 호출**하게 되면 자식 프로세스가 생기는데, 이때 **부모 프로세스가 가르키는 PC(Program Counter) 와 자식 프로세스가 가리키는 PC는 각각 다른 곳**을 가리키고 있을 수 있다.

그래서 만약 CPU 코어가 3개가 있다고 한다면, **각 CPU코어의 PC는** `**main()**`**과** `**f1()**`**과** `**f2()**`**를 가리키고 있을 수 있다**(CPU dispatched for child & starts here). 그렇게 만들어진 **자식 프로세스를 커널 스레드**라 한다. 이는**자식 프로세스들이 커널 코드를 실행(execute)하고 있기 때문**이다. 대부분의 경우 이런 함수들은 **서버(server) 혹은 데몬(daemon)**이다.

서버와 데몬의 알고리즘은 **기본적으로 무한 루프(endless loop)**라고 우린 배웠다. 또한 서버와 데몬은 **대부분의 시간을 자면서(sleep)** 보낸다. 그러다 **요청(request)이 오면 깨어나 그 작업을 처리해주고 또 잔다.** 네트워크와 관련된 작업을 처리하는 데몬이라면 네트워크 서버라고 부를 수 있고, 만약 프린트 요청을 기다리고 있다면 프린트 서버라고 할 수 있다.

결국 커널 스레드는 **커널 프로세스가** **`clone()`**을 호출해서 light weight overhead로 자식을 만들어준 것**이다. 또한 커널 메모리 영역과 코드를 똑같이 접근하고, 커널 코드를 실행한다. 당연하게도 커널 스레드는 커널 영역에만 존재한다. 많은 데몬(웹서버, 프린트 등)들이 커널 스레드이다.

![image-20200811163537777](https://user-images.githubusercontent.com/58545240/90212750-12e44380-de2f-11ea-8ee3-574a1ced591b.png)

위 그림을 보면 CPU가 여러개 있고 **CPU 마다 PC(Program Counter)를 가지고 있다.** 지금은 PC가 커널을 가리키고 있다. 여기서 커널이 `clone()`을 통하여 스레드 2개를 만들어줬다고 가정해보자. **CPU #2에 할당된 것은 프린트 데몬(서버)**이고, **#3에 할당된 것은 PageFault 데몬**이다. 앞 장에서 다뤘듯 **스레드는 위 그림의 우측 노란박스에 해당하는 Task basic info 파트만 복사를 하고 나머진 부모 프로세스와 공유**를 한다.

**Task basic info 안에는 state vector save area가 존재**하기 때문에**각 스레드마다 별도의 Program Counter와 Stack Pointer를 갖고 있을 수 있는 것**이다. 각 스레드가 **각자의 Stack**을 갖고 있기 때문에 **개별적으로 커널 내의 다른 함수들을 호출하면서 실행**될 수가 있다.

# **17. Process State**

---

프로세스의 상태에는 **ready, running, waiting이 존재**한다. **running은 프로세스 입장에서는 최상의 상태**이며, running 중 Disk I/O를 요구하는 사건이 발생하면 CPU가 해당 프로세스의 상태를 **waiting**으로 바꾼다. waiting의 경우 시그널의 상황에 따라 2가지의 반응이 있을 수 있는데 이 부분은 중요한 내용은 아니므로 넘어가도록 한다.

![image-20200811163620597](https://user-images.githubusercontent.com/58545240/90212756-17a8f780-de2f-11ea-85f7-bd7bca6b5c98.png)

**I/O가 끝나고 상태는 wait에서 ready**로 넘어가게 된다(CPU는 항상 바쁘다). **I/O가 끝나고 ready list에 참여해서 기다리다 보면 자신의 차례가 올 것**이다. 차례가 오는 것을 **Scheduler dispatches**라고 표현한다. dispatch과정을 상세하게 풀어보자면, `**context_switch()**`**의 동작으로 설명**할 수 있다.

첫번째로 현재 사용중이던 프로세스의 `state vector`를 저장한다. 그리고나서 **run하고 싶은 프로세스의 state vector를 CPU에 로드한 후, Program Counter가 가리키는 곳으로 가는 것이 dispatch의 과정**이다.

CPU가 주어지면 **어느 정도의 시간(time slice)만큼만 동작하고, 시간이 끝나면 다시 ready list로 가는 구조**인데, 일단 프로세스가 성공적으로 `exit()`했다고 가정해보자. 프로세스가 `exit()`을 하게 되면 **zombie 상태**가 된다.

좀비 상태라는 것은 `a.out`도 날라가고 `file`도 전부 `closed`되고 메인 메모리도 다 뺐기고, `PCB`만 남은 상태를 의미한다. 왜 PCB가 남아있는 것일까?

만약 부모 프로세스(parent)가 지금까지 기다리고 있었으면, parent가 깨어나 CPU를 쥐고 실행될 것이다. **이 때 parent는 자신이 잘 동안child가 뭘 했는지, 디스크와 CPU를 얼마나 썼는지, 제대로 끝났는지 등에 대한 내용을 child의 PCB에서 확인한다.**

위와 같은 중요한 정보들이 `PCB`에 있기 때문에 `PCB`는 남겨둬야 한다. **따라서 자식 프로세스의 PCB는 부모 프로세스가 말소시키는 것이 맞다. parent가 말소시킬 때까지는 자식은 zombie상태인 것이다. 따라서 최상위 부모 프로세스는 자식 프로세스들이 사용한 모든 자원을 전부 파악할 수 있어야 한다.**

# **18. Kernel Scheduling (커널 스케쥴링)**

---

![image-20200811163649168](https://user-images.githubusercontent.com/58545240/90212766-1bd51500-de2f-11ea-9b70-5cbc30a56da4.png)

리눅스에서는 어떤 프로세스가 다음에 실행될 프로세스일까? 물론 **priority(우선순위)**가 가장 큰 프로세스가 실행될 것이다. 하지만 **time slice를 가지고 있는지**도 반드시 확인해야 한다.

타임 슬라이스(time slice)에 대한 설명은 아래 그림을 보면서 살펴보도록 한다.

![image-20200811163702784](https://user-images.githubusercontent.com/58545240/90212771-20013280-de2f-11ea-9930-d071769b1e64.png)

먼저 위쪽 네모박스 안의 내용을 살펴보자. **CPU가 어떤 프로세스에게 할당될 때는 일종의 시간제한**이 있게 된다. 위 예시에서는 100ms라고 되어 있다. 하지만 안타깝게도 이마저도 못쓰게 되는 경우가 발생할 수 있다. 현재 프로세스보다도 더 급한 작업이 요구되었을 때, CPU를 또다시 빼앗길 수 있다.

위 네모박스 안에서는 **20ms를 사용하다가 CPU를 뺏겨버렸고 남은 80ms는 추후에 다시 CPU를 받았을 때 사용**해야 한다. 여기서 **남은 80ms를 remaining timeslice**라고 한다.

즉, 리눅스에서 프로세스가 CPU를 차지하기 위해서는 **우선순위가 높아야 하고 남은 타임슬라이스가 0보다 커야 한다.** 아래 그림을 보면서 스케쥴링이 정확하게 어떻게 이루어지는지 알아보자.

![image-20200811163720250](https://user-images.githubusercontent.com/58545240/90212774-242d5000-de2f-11ea-84ff-e971a5e9ca36.png)

**먼저 맨 위의 레디큐(Ready Queue)**를 살펴보자. 레디큐에는 **PCB가 여러개 들어 있다. 바로 실행(run)할 수 있는 작업들이 쭈욱 연결**되어 있는 것이다.

이런 구조에서의 **문제점은 멀티 프로세서 시스템이 되서 CPU의 갯수가 증가하게 되면 연결된 머신의 수에 따라 레디큐에 연결되는 PCB도 기하급수적으로 많아질 것**이라는 데 있다. 500개의 프로세스가 돌고 있다고 가정한다면, `context_switch()`를 실행할 때마다 이 500개에 해당하는 레디큐의 내용을 전부 뒤져서 우선순위가 높은 프로세스를 골라내야 하는데, 탐색 작업이 상당히 비효율적(시간이 오래걸림)이라는 문제가 생긴다.

그래서 이번에는 **위 그림의 중간에 있는 구조(큐를 두 개로 나눈 구조)로 설계**를 해 보았다. 하나의 레디큐로 두는 것은 탐색하는데 비효율적이라고 생각으로부터 나온 설계다. **높은 우선순위와 낮은 우선순위를 가지는 두 개의 큐를 제작**했다. 하나의 큐 보다는 확실히 효율적이겠지만, 이마저도 만족스럽지 않기에 **위 그림의 맨 아래와 같은 설계(여러 개의 큐를 두는 구조)**를 해봤다. **좀 더 분류를 세분화해서 여러 개의 큐**를 만들었다. 각 큐는 `PCB`를 가리키는 포인터로 이루어져 있다.

하지만 주황색으로 구현된 큐를 보니 중간 중간 비어 있는 큐(2번과 4번 큐)도 보인다. 이러한 큐까지 탐색할 필요는 없으니 좀 더 효율적으로 탐색을 해볼 수 있지 않을까? 라는 생각을 할 수 있다.

따라서 아래 그림과 같이 **해당 우선순위에 해당하는 큐가 비었는지 안 비었는지를 체크할 수 있는 비트 배열을**하나 더 두게 된다.

![image-20200811163734972](https://user-images.githubusercontent.com/58545240/90212784-27c0d700-de2f-11ea-82e6-67bdfef4d452.png)

위 그림의 좌측에는 바이너리 배열이 하나 있는데, **0은 해당 인덱스의 포인터를 따라가면 해당 인덱스의 큐가 비어 있다**는 뜻이고 **1이라면 해당 인덱스의 큐에 내용물이 있다**는 뜻이다. **유닉스에서는 이 바이너리 배열을 비트맵**이라 부른다.

이렇게 비트로된 배열을 사용함으로써 **탐색 속도가 향상**될 수 있다. 이 비트맵의 인덱스가 얼마나 존재하는지가 시스템에서 다루는 난이도가 얼마나 세분화되어 있는지를 나타낸다. 132개의 인덱스가 존재한다고 가정한다면, 우선순위가 132개로 나뉘어진다는 뜻이다. **우측에는 큐로 이루어진 배열**이 그 구현체다. **해당 큐의 각 내용물은 PCB**로 이루어져 있다.

위 그림의 맨 밑을 보면 **구조체가 하나 존재**한다. 그 구조체 **안에는 비트맵과 큐 배열**이 존재한다. 멀티 프로세서 시스템에서 CPU가 10개 있다고 한다면, 이 구조체가 **각 CPU마다 존재**한다고 생각하면 된다. 비트맵과 큐 배열을 함께 포함하고 있는 구조체가 바로 **priority array (우선순위 배열)**이다. 아래 그림을 살펴보자.

![image-20200811163749527](https://user-images.githubusercontent.com/58545240/90212828-4030f180-de2f-11ea-8523-694eaaec6e95.png)

CPU 스케쥴러가 `context_switch()`가 일어날 때마다, 레디큐에서 우선순위가 가장 높은 프로세스를 뽑아야 하는데 이때 해야 할 작업은 **비트맵을 스캔하고 0이 아닌 항목이 있다면, 해당 난이도의 큐를 찾아내서 큐의 작업내역을 순회하며 실행**하는 것이다. 0이 아닌 비트맵이 있다면 포인터를 따라가서 해당 큐의 작업 내역을 실행하면 된다. 아래 그림을 살펴보자.

![image-20200811163802908](https://user-images.githubusercontent.com/58545240/90212830-43c47880-de2f-11ea-900c-069185b1e45e.png)

**레디 큐라는 것은 ready to run a cpu**를 의미한다. 즉 **바로 실행될 수 있는 프로세스들을 모아둔 큐**이다. 하지만 만약 레디 큐에 있던 프로세스가 자신에게 **할당된 타임 슬라이스를 다 썼다면어떻게 해야할까? 타임 슬라이스를 다 사용한 프로세스는 위의 그림에서 나타난노란색 영역(Expired array)으로 빠지게 된다.** 다음 타임 슬라이스를 배정받기 전까지는 레디 큐에 있을 자격이 없기 때문에 **Expired array**에서 대기하게 된다. **이 두개의 array는 각 CPU마다 존재한다.**

![image-20200811163820776](https://user-images.githubusercontent.com/58545240/90212835-46bf6900-de2f-11ea-9447-2917cf6e3eb0.png)

좀 더 정확하게 설명을 하자면, **Active 영역에 있는 작업들이 모두 처리된 후 Expired 영역으로 가게 된 프로세스들은 일률적으로 (한 번에) 타임 슬라이스를 배정**받게 된다. 그 뒤 **Expired 영역이 Active 영역으로, 기존의 Active 영역은 Expired 영역으로 변환(interchange)된다.** 이 변환 과정은 단순히 **서로가 가리키는 포인터가 바뀌는 작업을 의미**하는데 구현 코드는 아래 그림의 맨 아래 네모박스에 나와있다.

![image-20200811163837483](https://user-images.githubusercontent.com/58545240/90212842-4a52f000-de2f-11ea-95db-39672a7d9505.png)

# **19. Kernel Preemption**

---

![image-20200811163854332](https://user-images.githubusercontent.com/58545240/90212850-4de67700-de2f-11ea-8586-a0b861cbd1c5.png)

## 19.1 Mutual Exclusion — 상호 배제

컴퓨터 시스템을 얘기할 때 가장 중요한 파트 중 하나가 **상호 배제 문제**이다. 시스템이 정상적으로 작동하기 위해서는 이 상호 배제 개념은 반드시 필요하다.

설명을 진행하기 전, 먼저 `X++`이라는 연산이 정확하게 어떻게 이루어져 있는지부터 이해하고 가자. 우리가 보통 프로그래밍 언어를 사용할 때 `X++`과 같은 단항연산자를 사용하면, 하나의 명령만으로 덧셈이 정상적으로 이루어지는 것 같지만 실제로 동작하는 기계 입장에서 이 단항연산 과정은 3단계로 나누어진다. 그 과정은 아래와 같다.

1. `X`를 저장소로부터 읽어서 `CPU 레지스터`로 읽어들인다.
2. `CPU` 안에서 `ALU` 연산을 진행한다.
3. `CPU` 로부터 나온 결과를 다시 저장소에 쓴다.

먼저 우리가 흔히 사용하는 **X와 같은 변수 또는 데이터는 항상 저장소(storage)에 존재**한다. `CPU`안에는 저장시킬 수 있는 용량이 얼마 없고 또 비싸기 때문에 `CPU`안에 많은 변수와 데이터를 저장할 수는 없다. 때문에 우리가 흔히 알고 있는 컴퓨터 시스템에서 **데이터는 저장소에 저장되고 연산은 CPU에서 이루어지게 되는 것**이다. 이 개념을 먼저 숙지한 후 아래의 그림과 함께 설명을 보자.

![image-20200811163924928](https://user-images.githubusercontent.com/58545240/90212857-5179fe00-de2f-11ea-868a-2e0dc93f91b9.png)

위 그림을 보면 두 개의 프로세스가 존재하고, 이 두 프로세스는 **한 개의 변수 X를 공유**하고 있다. 프로세스 A가 먼저 `X`에 대한 `X++`를 연산한 후 메모리에 저장하고, 그 다음 우측의 프로세스 B가 변수 `X`를 레지스터로 읽어들여 연산처리를 한 후 디스크에 저장한다. 이렇게만 한다면, `X`라는 변수는 정상적으로 `11 -> 12 -> 13`순으로 디스크에 저장될 것이다.

그런데 만약 이 두 프로세스가 `X`를 동시에 읽어들여 연산할 경우에는 문제가 발생하게 된다. A가 `X`를 읽은 후에 `++`연산을 진행하는 도중 B가 `X`를 읽어들였다고 가정해보자. A는 아직 더한 값을 디스크에 쓰지 않았기 때문에 아직 `X`는 초기값 `11` 그대로다. 따라서 B가 읽어들인 `X`값은 `11`이다.

여기서 A가 값을 `12`로 증가시킨 후 디스크에 `12`를 기록했다고 해도, 결국 B또한 `12`로 증가시킨 후에 기록하기 때문에 덧셈은 한 번밖에 일어나지 않는다. **두 번의 덧셈연산이 제대로 동작하지 않은 것이다.** 이처럼 프로세스 간 **공유된 변수를 접근하는 부분을 Critical Section**이라 부른다.

크리티컬 섹션에는 하나의 프로세스만 접근해야만 한다. 그래야 위와 같은 오류가 발생하지 않는다. **이러한 원칙이 바로 상호 배제(Mutual Exclusion)의 원칙**이다. 크리티컬 세션은 하나의 프로세스만 접근이 가능하다. 또 그래야만 한다.

유닉스는 지난 40년간 이러한 문제를 어떻게 해결했을까? 그 방법은 매우 단순하게도, **커널모드인 경우에는 CPU를 뺐지 않고 유저모드일 경우에만 CPU를 뺐는다. 커널에 있을 때는 CPU preemption을 고려하지 않아도 된다.**

하지만 이러한 설계에는 문제가 있다. **중요한 작업이 도중에 발생했다고 해도, Kernel 모드이기 때문에 CPU를 다른 곳에 할당하지 못한다면 리얼타임시스템(real-time system)과 같이 빠른 처리와 빠른 전환이 어려워진다.** 커널이 작업중임에도 CPU를 가져올 수 있어야 진정한 리얼타임시스템이 가능하다. 이 부분을 어떻게 해결할지가 아래 그림에 나와있다.

![image-20200811163941105](https://user-images.githubusercontent.com/58545240/90212865-550d8500-de2f-11ea-962c-b63c39455b6a.png)

위 그림의 좌측 상단을 보면, `쉘(sh)`이 `read()` 시스템 콜을 호출하고 있다. 함수 안에 변수들이 쓰이면서 스택에 이러한 변수들이 담기게 된다. 우측의 `mail` 프로그램에서도 `send()` 시스템 콜을 호출하고, 이 호출 또한 지역변수를 자신만의 스택에 담는다. **그러나 둘은 같은 공용 변수에 접근하고 있다. 어떻게 하면 두 프로세스가 원활하게 동작이 가능할까?**

리눅스에서는 **공용 변수에 접근할 때만 따로 lock을 건다**. 접근이 끝났다면 unlock을 한다. **즉, lock이 되어 있다면 커널모드이건 아니건 CPU를 뺐는 일은 발생하지 않는다.** 그러나 **unlock이라면 크리티컬 세션이 아닌 것이므로 커널 모드임에도 CPU를 다른 프로세스에게 할당하는 것이 가능**하다. 리얼타임시스템을 고려한 리눅스에서의 설계다.

![image-20200811163959603](https://user-images.githubusercontent.com/58545240/90212875-5939a280-de2f-11ea-80a2-f87c6a494dc5.png)

위의 `preempt_count`가 바로 `lock의 갯수`이다. CPU를 뺏으러 왔을 때, **preempt_count가 0이면 공용 변수에 접근하는 프로세스가 하나도 없다는 것이기 때문에 CPU를 뺏을 수 있다.** `need_resched`의 경우는 리얼타임 시스템이 CPU를 다른 프로세스에 할당해주기 위해 왔는데, `preempt_count`가 `0`이 아니여서 뺐을 수는 없으니 "지금 다른 우선순위 높은 프로세스가 CPU를 기다리고 있어!"라는 표시를 해주는 용도로 사용한다. 따라서 **preempt_count 가 0으로 되고 need_resched 플래그가 세트되어 있다면, 현재 공용변수에 접근하고 있는 것이 없으니 CPU를 다른 프로세스에게 할당해도 좋다는 의미가 된다.**

---

> **프로세스 스케쥴링을 할 때 우선순위와 타임슬라이스, 그리고 크리티컬 영역까지 고려한다는 점을 알게 되었다.** 커널을 설계하는 개발자들이 얼마나 치밀하고 효율적으로 설계하기 위해 노력했는지를 간접적으로나마 알 수 있었다. 커널과 같은 하나의 훌륭하고 완벽한 프로그램을 만드는 그 날까지 노력해야겠다.

---

# **20. 타이머와 시간 관리**

---

> 지난 5강에선 **Timeslice 라는 CPU에게 주어지는 사용시간과 CPU의 사용 순서를 관리하는 커널 스케쥴링에 대하여 공부를 했다.** 이번 시간에는 그 시간의 단위에 대한 공부와 **여러 개의 인터럽트가 일어났을 때의 관리방법**에 대하여 공부를 할 것이다.

---

우리는 시계가 돌아갈때 나는 소리를 째깍째깍 거린다고 표현을 하며 이는 영어로 Tick Tack이라고 표현이 된다. 이때 일초에 1000번 **째깍거리면** 1000 헤르츠(Hertz, HZ)라고 하고 이는 1 밀리세컨드(1 Millisecond)가 된다. 이러한 표현들은 물리학에서 사용하는 표현들이고 `#define HZ 1000`이라는 표현을 사용하면 **1초에 1000번 인터럽트**가 걸리는 설정으로 된다. 대부분의 경우에는 `100`을 걸어 놓는다.

![image-20200811164119341](https://user-images.githubusercontent.com/58545240/90212901-6e163600-de2f-11ea-9fb3-a21e714b9356.png)

**시스템이 켜진(부팅 된) 이후**에 몇 번 tick을 했는지 기록한 것을 우리는 **Jiffies**라고 표현을 한다. 이는 전역 변수이며 카운터의 역할을 한다. 이러한 `Jiffies`를 설정된 `HZ`로 나누면 몇 초가 흘렀는지 알 수 있다.

```bash
# Note
예시: Jiffies가 24000이고 HZ가 100으로 설정되어 있었다면 24000/100 즉 시스템이 부팅된 이후 240초가 흘렀다는 것을 알 수 있다.
```

위에서 우리는 `HZ`의 단위로 인터럽트를 걸기위해 이러한 시간 개념을 도입했다는 것을 살펴봤. 그렇다면 **왜 1초에 100번씩이나 인터럽트가 걸려야 하는 것 일까. 왜 I/O 인터럽트처럼 어떤 입력이나 할 일이 생겼을 때만 인터럽트를 하면 되는 것 아닐까**라는 의문을 품을 수 있을 것이다.

시스템에 시간 단위를 도입한 이유는 **먼저 특정 시간마다 반복이 필요한 일들을 처리하려면 시스템이시간의 개념을 알아야 하기 때문이다.** 사실 가장 중요한 이유는 **스케쥴링**에 필요하다는 점이다. 프로세스들은 CPU를 사용할 수 있는 시간인 **Timeslice**를 배정받게 된다.

이때 얼마만큼의 시간이 지났는지 파악해 다음 작업에 `CPU`를 넘겨주는 등의 역할을 할때 시간의 단위를 사용하게 된다. 즉, 하나의 작업이 다 끝날때까지 다음 작업이 기다리는 것이 아닌 `**HZ**`**의 단위로 계속해서 프로세스들이 돌아가며 작업을 할 수 있게 해주기 위해 계속해서 특정 시간마다 인터럽트를 걸어주는 것 이다.**

```bash
# Note
첫번째 이유에 대한 예시: 만약 안구건조증이 있는 사람이라면 2시간 마다 하던 일을 멈추고 반복적으로 약을 넣어줘야 한다.
두번째 이유에 대한 예시: 학교에서 하루종일 한 과목만 할 수는 없기에 수업 시간 종소리를 통해 다음 수업을 진행한다.
```

그렇다면 이런 인터럽트의 횟수를 `100`에서 `1000`번을 하게 `HZ`를 설정하면 좋은 것 일까? 꼭 그렇지만은 않다. 물론 인터럽트 하는 주기가 늘어났기에 반응을 해주는 횟수는 증가할 수 있다. **그러나 많은 인터럽트를 하면서 오버헤드가 증가하기 때문에 현재 쓰는 장치들은 대부분 100 HZ라는 적절한 숫자로 설정해 준 것이다.**

![image-20200811164153528](https://user-images.githubusercontent.com/58545240/90212915-77070780-de2f-11ea-9a78-76b3106cd8ea.png)

이러한 시스템의 시간은 크게 **Timer 와 Real-Time Clock(RTC)** 2가지로 나눌 수 있다. 먼저 **Timer**는 **주기적으로 CPU에게 인터럽트**를 거는 역할을 한다. 이러한 **Timer**는 프로그램적으로 인터럽트를 걸 수 있게 설정할 수도 있다.

**Real-Time Clock**은 **현실 세계의 시간**을 표현하며 PC의 전원을 꺼놔도 보조 베터리를 통해 계속해서 현재 시간을 측정 한다. 후에 다시 PC를 켰을때 시스템은 **Real-Time Clock**의 시간을 보고 현재 시간을 표시한다. 그렇다면 이제 이런 타이머 인터럽트를 관리하는 핸들러의 실제 구현을 아래 그림과 함께 알아보자.

![image-20200811164205969](https://user-images.githubusercontent.com/58545240/90212920-7bcbbb80-de2f-11ea-9347-6320b3659bb4.png)

먼저 인터럽트가 걸리면 `dotimer(struct ptregs *regs)`로 들어와 `Jiffies`를 하나 증가시키게 된다. 이때 `Jiffies`는 시스템이 켜진 이후 매번 카운트를 하는 역할을 함으로 `64비트`라는 매우 큰 크기로 정해준다. 그 후에 `update_process_times(user_mode(regs))`를 통해 몇번이나 인터럽트가 걸렸는지 업데이트를 하는데 이때 인터럽트가 걸린 순간에 User 모드였는지 Kernel 모드였는지 같이 기록을 한다.

이러한 타이머는 프로그램으로 특정 시간으로 설정 할 수도 있으며 지연 시킬 수도 있다는 점 정도만 알아두고 이런 타이머에 의해 걸리는 인터럽트에 대해 자세히 알아보자

# **21. 인터럽트**

---

**인터럽트란 CPU가 프로그램을 실행하고 있을 때, 입출력 하드웨어나 예외상황 등이 발생해 작업을 처리가 필요할 경우에 커널에게 처리해 달라고 요청하는 것**이다. 그럼 먼저 하드웨어적 문제중에서 CPU에 대해 알아보자

## 21.1 CPU

CPU는 먼저 인스트럭션을 가져온다. 그리고 그 인스트럭션을 분석하고 실행을 한다. 이렇게 실행을 하기 위해선 데이터를 읽어와야 하는 경우도 종종 있게 된다. 또한 연산을 처리 했다면 처리된 결과를 반환해준다. 인스트럭션이 다 끝났으면 그 다음 인스트럭션을 가져오기 위해 프로그램 카운터를 증가시킨다(보통은 `4 byte` 정도 증가시킨다).

![image-20200811164234316](https://user-images.githubusercontent.com/58545240/90212928-7f5f4280-de2f-11ea-842e-2ea9b3af277b.png)

이런 일련의 작업을 하던 중 **Disk가 인터럽트를 걸었다**고 생각해보자. 그러면 **Interrupt Request Bit** 한 비트를 설정을 한다. 이 비트가 걸려있으면 작업을 계속 돌지 않고 **프로그램 카운터에 인터럽트 핸들러의 새로운 주소를 저장**한다. 그리고 다시 진행을 해서 인스트럭션을 가져오면 이제 아까 저장했던 인스트럭션을 가져와 점프를 한 효과를 보게된다. 물론 이런 인터럽트가 걸리지 않게끔 설정할 수도 있다. 예를 들어 컴퓨터를 부팅할 때의 경우엔 `Interrupt Request Bit` 설정을 `disable` 시켜서 인터럽트 당하지 않게 만들 수 있다.

위의 경우는 디스크 하나만 인터럽트를 걸었을 경우에 대한 설명이었고, 아래 그림은 인터럽트를 거는 장치들이 많을 때의 상황을 설명한다.

![image-20200811164249731](https://user-images.githubusercontent.com/58545240/90212934-82f2c980-de2f-11ea-9ab9-dc12775e6de3.png)

여러개의 장치가 인터럽트를 거는 것을 통제하기 위해 **Interrupt Controller**라는 개념을 도입했다. 우리는 이런 **Interrupt Controller**를 **PIC(Programmable Interrupt Controller)**라고 부른다. 여기서 `Programmable`이 붙는 이유는 소프트웨어적으로 관리가 가능하기 때문에 프로그램이 가능하다는 뜻이 붙은 것이다.

## 21.2 PIC

![image-20200811164310341](https://user-images.githubusercontent.com/58545240/90212940-86865080-de2f-11ea-9b4d-a0761e124f45.png)

PIC에서는 여러개의 장치들이 인터럽트 요청을 한다. 이때 이런 요청들을 한 장치들은 **Interrupt Request Line(IRQ Line)**에 연결이 된다. 이렇게 요청이 들어온 요청 라인들은 **Mask Register**을 통해서 `0`일 경우 차단이 되고 `0`이 아닐 경우엔 그 다음 단계로 통과가 된다. 이러한 `Masking`은 소프트웨어적으로 차단할 장치등을 설정할 수 있다. 그 후에**Interrupt Request Register**에서 `Masking`이 되지 않은 장치들만 요청을 할 수 있게 설정해 주는 단계를 처진다.

이렇게 설정이 된 요청들 중 우선순위가 제일 높은 요청을 **Priority Register**에서 받고 지금 인터럽트 요청이 진행 중 이라고**In Service Register**에 등록하고 **INTR**, 즉 `CPU`에게 인터럽트 요청을 한다. 이때 어떤 `IRQ Line`에서 요청이 들어온건지를 **Vector**에 넣어 보낸다. 그렇게 요청을 보내고 나서 `CPU`가 요청을 처리 했다는 **ACK**신호를 보낼 때 까지 다른 `PIC`와 장치들은 차단되어 있다. 이러한 요청 처리 단계를 위해 CPU는 빠르게 일들을 처리해 줘야 한다.

![image-20200811164325640](https://user-images.githubusercontent.com/58545240/90212949-8b4b0480-de2f-11ea-8972-797efa46b0fb.png)

지금까지 설명한 요청 단계를 정리해 보면 위의 그림과 같이 나온다. `PIC`가 `INTR`과 `Vector`을 `CPU`에게 보내고 그럼 `Interrupt Request Bit`을 설정하고 그럼 그걸 본 `CPU`는 그 요청을 처리할 새로운 공간을 만들어 일들 처리후 `ACK`를 다시 `PIC`에게 보낸다. 이러는 동안 요청을 보낸 `PIC`은 `ACK`가 올때까지 다른 요청들을 차단하고 기다리고 있는다.

## 21.3 SMP & AMP

이번에는 여러개의 요청들이 멀티프로세싱 환경에서는 어떤 방법으로 처리되는지 알아보자.

![image-20200811164348476](https://user-images.githubusercontent.com/58545240/90212953-90a84f00-de2f-11ea-97dc-c7adfc158b21.png)

먼저 `CPU` 2개가 있고 이 `CPU`가 `bus`에 달려있다고 하자. 그리고 모든 I/O 장치들은 **multi APIC(Advanced Pic)**에 달려있다. `APIC`이란 멀티 프로세서를 위한 `PIC`이다. 또 다른 `APIC`은 각각의 `CPU`에 달려있는데 이때의 조그만한 `PIC`들은 **Local APIC**이라 하며 여기에는 정기적으로 인터럽트를 걸어주는 **Timer**만 달려있다.

![image-20200811164401220](https://user-images.githubusercontent.com/58545240/90212974-9f8f0180-de2f-11ea-9456-d41dcb60e905.png)

여기서 잠시 컴퓨터 구조에 대한 얘기를 해보자. `CPU`가 `0000000~7777XXX`번 메모리를 메모리 관리 유닛에게 보내면 위 그림의 좌측 **Memory** 쪽으로 달려간다. 그러나 `7777XXX ~ 7777777`번 메모리를 유닛에게 보내면 **I/O Interface**쪽으로 버스를 타고 달려가게 되어있다.

`I/O 버스`들에는 각각 I/O 장치들이 연결이 되어있는데 이런 장치들은 컴퓨터 뒤에 보면 있는 각종 연결장치, 즉 **I/O Interface Card**들로 연결이 되어있다. 이런 `I/O Interface Card`의 구조는 `Address, Data`와 보조 레지스터인 `Control, Status` 등으로 구성되어 있다. 그 외에 장치를 만든 회사 이름, 모델 아이디 등등이 들어있으며 어느 카드에 꽂혀있는 장치인지 등에 대한 정보도 들어있다. 이러한 인터페이스를 바탕으로 위 그림의 오른쪽 처럼 각종 장치들이 인터페이스 카드들에 연결이 되어 있는 것이다.

![image-20200811164413347](https://user-images.githubusercontent.com/58545240/90212980-a3228880-de2f-11ea-8cdb-eb2b51275b9d.png)

위 그림은 방금 설명한 과정을 **SMP(Symmetric Multiprocessing), 즉 대칭형 멀티 프로세싱 방식**에서 처리하는 과정을 나타내는 그림이다. `SMP`란 **두 개 이상의 동일한 프로세서가 하나의 메모리, I/O 디바이스, 인터럽트 등의 자원을 공유하여 단일 시스템 버스를 통해 각각의 프로세서는 다른 프로그램을 실행하고 다른 데이터를 처리하는 시스템**을 말한다.

즉, **두 개 이상의 프로세서가 하나의 컴퓨터 시스템을 공유**하도록 연결되어 있으며, **각각의 프로세서가 독립적으로 자신의 작업을 처리하는 방식**이다. 이러한 방식에서 디바이스가 I/O 인터페이스 카드에 연결이 되어 요청을 보내면 `APIC`이 받아 처리를 하는데 `CPU`간의 차이가 없는 대칭형이기 때문에 어느 `CPU`에 요청을 전달하는지는 다음과 같은 두 가지 방식을 사용한다.

첫번째 방식은 **Static Distribution방식으로** 정적으로 정해진 곳에 보낸다. 이 경우에는 이미 만들어진 `Static Table`을 통해 결정을 하게 된다.

두번째 방식은 **Dynamic Distribution 방식**으로 동적으로 결정을 하는데 이때 **동적 IRQ 분배 알고리즘**을 통해 보낼 곳을 정한다. 이 알고리즘의 목표는 **우선순위가 제일 낮은 프로세스를 돌리고 있는 CPU에게 IRQ를 주는것**이다.

그렇다면 프로세스를 실행하고 있는 경우에는 어떻게 처리를 해야 할까. 이를 해결 하기 위해 모든 프로세스에 카운터를 둔다. 이 **카운터의 값이 가장 큰 CPU**가 `IRQ`를 받게 되는데 이때 카운터를 `0`으로 낮춰주고 `IRQ`를 받지 않은 다른 모든 `CPU`의 카운터는 증가시킨다. 이렇게 카운터를 증가시킴으로서 **나중에 어떤 CPU가 IRQ를 제일 적게 받아 제일 처리를 많이 안했는지 분별할 수 있는 척도로 사용을 하게된다.**

![image-20200811164429791](https://user-images.githubusercontent.com/58545240/90212987-a6b60f80-de2f-11ea-818f-f342324da2c7.png)

그럼 이제 **Asymmetric Multiprocessing(AMP), 비대칭형 멀티 프로세싱에선 어떤 방식을 사용하는지 알아보자.** `AMP`란 **두개 이상의 각각의 프로세서가 자신만의 다른 특정 기능을 수행하는 방식**을 말한다.

예를 들어 하나의 프로세서가 메인 운영체제를 실행하도록 하고 다른 프로세서는 I/O 기능을 전용으로 수행하는 형식 등을 말한다. 이런 구조에서 `IRQ`를 처리하는 방식이 위 그림에 나와있다. 각각의 `CPU`가 다른 작업들을 하고있다. 이때 **Master CPU**는 **본인만의 메모리**를 가지고 있는데 여기 **OS 커널**이 들어있다.

즉, **Master CPU만 I/O 인스트럭션을 가지고 있는 구조**이기에 다른 `CPU`가 I/O를 하려면 `Master CPU`에게 신호를 보내야 한다. 이러한 **주종 관계라는 간단한 구조 덕분에 디자인이 쉽다.**

그러나 `Master CPU`에게 시스템 콜 요청이 많아지면 과부화가 쉽게 걸리며 `Master CPU`가 망가지면 아무것도 할 수 없기 때문에 문제가 생긴다. 과거에는 `AMP` 방식만 사용하다 처리 방식이 발전이 되어 `SMP`로 바뀌었다.

---

>Timer와 장치들의 인터럽트 및 처리방법에 대해 공부하였다.

---



# **22. 커널 모듈**

---

리눅스 커널에서 **`module`**은 광범위하게 쓰인다.

아마 대다수가 device driver로 동작하는 형태일 것이고 드물게는 network protocol이나 filesystem을 지원하는 데도 쓰일 것이다.

module 형태를 취함으로써 가질 수 있는 이점은 **flexibility**가 좋기도 하고, 뭔가 시스템 내에서 변화를 추구할 떄 쉽게 반영하고 그 결과를 확인할 수 있는 점이다.

module과 관련하여 Linux에서 제공하는 utility는 다음과 같다.

- **lsmod** : load되어 있는 module들을 보여준다.

- **insmod** : Insert module, 말그대로 module을 load시켜준다.

- **rmmod** : remove module, 말그대로 module을 제거해준다.

- **modprobe** : module을 load시키거나 upload시켜준다.

- **depmd** : module과 연관된, 혹은 module과 상관성이 있는 database를 재생성해준다.

  => 여기서 상관이 있다는 것은 module 중에서도 독립적으로 동작하는 module이 있는가 하면 다른 module을 참조하면서 동작하는 것도 있는데 이를 지칭하는 것이다.

- **modinfo** : module에 대한 정보를 출력해준다.

*참고로 이런 utility는 system management와 관련이 있는 실행 binary이므로 보통 /sbin 안에 들어 있을 것임을 유추할 수 있다.*

보통 module의 확장자는 파일이름 뒤에 `.ko`라고 붙는데 이는 **kernel object**를 나타내는 것이고, 현재 시스템에 어떤 module들이 붙어있는 지를 확인하고 싶으면 위의 utility 중 **lsmod**를 해주면 된다.

현재 test 중인 ubuntu에는 다음과 같은 module이 인식되어 있다.

![image-20201012174355800](https://user-images.githubusercontent.com/58545240/97410267-32750d80-1942-11eb-90aa-a749e92af6e5.png)

만약 이렇게 인식된 파일들의 `ko`파일을 찾고 싶다면 `/lib/module`로 가면된다. 아마 여러 폴더가 있을텐데, 현재 kernel version에 해당하는 폴더로 접근하면 된다!

![image-20201012174430902](https://user-images.githubusercontent.com/58545240/97410242-2be69600-1942-11eb-8abc-59318f47c0f3.png)

**modprobe**는 앞에서 소개했던 insmod와 rmmod를 합친 것이다. 다만 합친다고 완전히 똑같은 것이 아니라 약간의 차이가 있다.

- lsmod에 출력되어 있다고 해서 무조건 rmmod를 통해서 module을 unload 시킬 수 없다. 또한, module이 어떤 process에 의해서 점유되어 있어도 rmmod를 통해서 unload 시킬 수 없다. 이유는 insmod나 rmmod는 해당 module에 대한 dependency를 고려하지 않고 load/unload를 수행하기 때문이다.
- deependency를 고려해야 할 경우는 insmod나 rmmod보다는 modprobe를 사용하는 것이 효과적이다. modprobe는 dependency를 고려해 먼저 load되어야 할 module이 있는 경우 해당 module에 대한 동작을 처리한 후에 정상적으로 명령을 수행한다.

# **23. LINUX에서 WI-FI 연결하기**

> LINUX 터미널이나 서버 모드에서 Wi-Fi 연결하는 방법이다.
>
> Ubuntu18.04에서 iptimeA3000UA를 설치할 것이다.

### 1. USB 연결

제품의 USB단자를 PC에 연결한다.

### 2. 드라이버 설치

Windows환경에서는 iptime 공식홈페이지에 이 제품에 제공되는 드라이버가 있어서 바로 설치하기만 하면 된다.

하지만 여기서는 리눅스 전용 드라이버를 제공하지 않는다.

대신 별도로 `Realtek 88 12BU chipset`을 설치하면 된다.

터미널로 들어가서 다음 명령을 실행시켜준다.

- **드라이버 설치명령**

```bash
sudo apt update 
sudo apt install dkms bc git 
git clone https://github.com/cilynx/rtl88x2BU_WiFi_linux_v5.3.1_27678.20180430_COEX20180427-5959 
sudo dkms add ./rtl88x2BU_WiFi_linux_v5.3.1_27678.20180430_COEX20180427-5959 
sudo dkms install -m rtl88x2bu -v 5.3.1 
sudo modprobe 88x2bu
```

### 3. WIFI 연결

설치가 끝나면 오른쪽 상단에 무선LAN 아이콘(와이파이 모양)이 뜬다.

패스워드가 있다면 패스워드 입력 후 연결하면 잘 연결된다.

아직까지는 사용하면서 인터넷이 끊겨지는 둥 하는 불편함은 없었다.

### 4. 무선랜 인터페이스 확인

```bash
$ iw dev
```

![image-20201016134935514](https://user-images.githubusercontent.com/58545240/97410229-2426f180-1942-11eb-855a-01726842be50.png)

명령어 실행 결과, 무선랜 카드 인터페이스 이름은  wlx88366cf619d8 으로 확인되고 있습니다. 앞으로 이 인터페이스 이름을 이용하여 설정에 사용되게 됩니다.

### 5. 무선랜 인터페이스 활성화

아래의 명령어로 인터페이스를 확인 한다.

```bash
sudo ip link show wlx88366cf619d8
```

![image-20201016135046682](https://user-images.githubusercontent.com/58545240/97410161-0e193100-1942-11eb-9d72-4ebda3ddb66d.png)

현재 무선랜 카드가 활성화 되어있지 않으므로, 다음 명령어를 사용하여 무선랜 카드를 활성화한다.

```bash
sudo ip link set wlx88366cf619d8 up
```

그리고 다시 무선랜 카드 정보를 확인하면, 활성화된 무선랜 카드 정보를 확인할 수 있다.

### 6. 연결상태 확인

다음 명령어를 사용하여, 현재 무선랜 카드의 연결상태를 확인한다.

```bash
iw wlx88366cf619d8 link
```

![image-20201016135322928](https://user-images.githubusercontent.com/58545240/97410199-196c5c80-1942-11eb-80d3-d44ff5345e8f.png)

현재 WIFI에 연결되어 있지 않다.

### 7. WIFI 스캔

다음 명령어를 사용하여 WiFi 정보를 스캔합니다. 스캔 후 나타나는 WiFi 중에서 비밀번호가 없는 WiFi 와 WPA/WPA2 암호화 방식을 사용하는 WiFi에 대해서 나눠서 설명드리도록 하겠습니다.

#### 공개된 WiFi 일 경우

공개된 WiFi 일 경우 아래와 같은 명령어를 사용하여, SSID를 확인 한 다음 바로 WiFi에 연결 할 수 있습니다.

아래의 명령어는 iptime 이라는 WiFi에 접속하는 명령어 입니다. 그 다음 연결접속 정보를 확인 후 IP를 할당 받으면 됩니다.

```bash
sudo iw wlx88366cf619d8 scan
sudo iw dev wlx88366cf619d8 connect iptime
```

#### 비공개된 WiFi 일 경우 (WPA/WPA2)

WPA/WPA2 암호화 방식을 사용하는 WiFi 정보입니다. 네트워크를 스캔하면 다음과 비슷한 결과가 나타납니다.

```bash
$ sudo iw wlx88366cf619d8 scan
// 생략
BSS 34:cc:28:05:f0:58(on wlx88366cf619d8)
	TSF: 175628935496 usec (2d, 00:47:08)
	freq: 2432
	beacon interval: 100 TUs
	capability: ESS Privacy ShortSlotTime (0x0411)
	signal: -69.00 dBm
	last seen: 800 ms ago
	Information elements from Probe Response frame:
	SSID: sw4t
	WPA:	 * Version: 1
		 * Group cipher: TKIP
		 * Pairwise ciphers: TKIP CCMP
		 * Authentication suites: PSK
	RSN:	 * Version: 1
		 * Group cipher: TKIP
		 * Pairwise ciphers: TKIP CCMP
		 * Authentication suites: PSK
		 * Capabilities: 1-PTKSA-RC 1-GTKSA-RC (0x0000)
// 생략
```

위의 스캔된 정보에서 가장 중요한 내용이 SSID 와 암호화 프로토콜입니다. RSN 방식이 WPA2 를 나타냅니다. WiFi의 SSID 이름이 sw4t일 경우 설정 후 접속하는 방법입니다.

아래의 명령어를 실행 후, WiFi 패스워드를 입력하면 설정 파일이 생성되게 됩니다.

```bash
sudo wpa_passphrase sw4t > wpa_supplicant.conf
```

이 설정 파일을 이용하여 다음 명령어를 이용하여 Wi-Fi에 접속하면 된다.

```bash
sudo wpa_supplicant -B -i wlx88366cf619d8 -c wpa_supplicant.conf
```

![image-20201016153618739](https://user-images.githubusercontent.com/58545240/97410425-618b7f00-1942-11eb-971d-dd7d7f4aaa7d.png)

위의 명령어에서 사용된 옵션의 의미는 다음과 같습니다.

- **-B** : 백그라운드 실행
- **-i wlx88366cf619d8**: 무선랜 인터페이스 이름
- **-c wpa_supplicant.conf** : WiFi 설정 파일 경로

### 8. WIFI 연결 정보 확인

Wifi에 접속 후 연결 정보를 확인한다.

```bash
sudo iw wlx88366cf619d8 link
```

명령어 실행 결과 SSID가 출력되며, 연결정보가 나타난다.

### 9. DHCP 주소 할당

성공적으로 WiFi에 접속되면, 아래의 명령어로 IP 주소를 할당 받는다.

```bash
sudo dhclient wlx88366cf619d8
```

위의 명령어 실행결과, 에러 없이 IP 주소가 할당되었을 경우 WIFI 연결이 성공적으로 이뤄진 것이다.

# **24. Device Driver**

---

> **Device** 
>
> - 네트워크 어댑터, LCD 디스플레이, 오디오, 터미널, 티보드, 하드디스트, 플로피디스크, 프린터 등과 같은 주변 장치들을 말함
> - 디바이스 구동에 필요한 프로그램, 즉 디바이스 드라이버가 필수적으료 요구됨
>
> **Device Driver**
>
> - 실제 장치 부분을 추상화시켜 사용자 프로그램이 정형화된 인터페이스를 통해 디바이스를 접근할 수 있도록 해주는 프로그램
> - 디바이스 관리에 필요한 정형화된 인터페이스 구현에 요구되는 함수와 자료구조의 집합체
> - 표준적으로 동일 서비스 제공을 목적으로 커널의 일부분으로 내장
> - 응용 프로그램이 하드웨어를 제어할 수 있도록 인터페이스 제공
> - 하드웨어 독립적인 프로그램을 작성할 수 있도록 함.

## 24.1 리눅스 디바이스 드라이버

### - 사용자 관점에서의 디바이스 드라이버

- 사용자는 디바이스 자체에 대한 정보를 알 필요 없음.
- device는 하나의 파일로 인식됨
- 파일에 대한 접근을 통하여 real device에 접근 가능함

![image-20201028135315115](https://user-images.githubusercontent.com/58545240/97408497-816d7380-193f-11eb-9281-20dec1ba14a4.png)

### - 리눅스에서의 디바이스

- 리눅스에서 디바이스는 특별한 파일로 취급되고, 액세스가 가능함. 사용자(응용 프로그램)은 file operation을 적용할 수 있다.
- 각 디바이스는 **Major Number**와 **Minor number**를 가진다.
  - `Major Number` : 디바이스 장치 구분
  - `Minor Number` : 같은 종류의 디바이스들을 구분

## 24.2 디바이스 드라이버의 종류

![image-20201028135434082](https://user-images.githubusercontent.com/58545240/97408513-88948180-193f-11eb-8286-2b5e50f335c6.png)

![image-20201028135447261](https://user-images.githubusercontent.com/58545240/97408545-93e7ad00-193f-11eb-82e7-125311441ef5.png)

### - 문자 디바이스

> Char Device

- 자료의 순차성을 지닌 장치
- 버퍼 캐쉬를 사용하지 않음
- 장치의 `raw data`를 사용자에게 제공
- Terminal, Serial/Parallel, Keyboard, Sound Card, Scanner, Printer 등

리눅스에서의 문자 디바이스

![image-20201028135633869](https://user-images.githubusercontent.com/58545240/97408555-9a762480-193f-11eb-92ef-0e3d64558e40.png)

맨앞 `c` => 파일 관련 정보 중 첫 문자인 c는 **`char device`**를 의미한다.

### - 블록 디바이스

> Block Device

- **random access** 가능
- 블록 단위의 입출력이 가능한 장치
- 버퍼 캐쉬에 의한 내부 장치 표현
- 파일 시스템에 의해 mount 되어 관리되는 장치
- 디스크, RAM Dis, CD-ROM 등

![image-20201028135759397](https://user-images.githubusercontent.com/58545240/97408572-9fd36f00-193f-11eb-9e90-3533af1a2d87.png)

마찬가지로 파일 관련 정보 중 첫 문자이는 b는 **`block device`**를 의미한다.

### - 네트워크 디바이스

> Network Device

- 대응하는 장치파일이 없음
- 네트워크 통신을 통해 패킷을 송수신 할 수 있는 장치
- 응용프로그램과의 통신은 표준 파일 시스템 관련 호출 대신에 `socket()`이나 `bind()`등의 시스템 호출
- Ethernet, PPP, ATM, ISDN 등이 있다.

## 24.3 디바이스 드라이버의 Major & Minor Number

**Major Number(주 번호)**

- 커널에서 디바이스 드라이버를 구분/연겨랗는데 사용
- 같은 디바이스 종류를 지칭. 1Byte (0~255 사이의 값)

**Minor Number(부 번호)**

- 디바이스 드라이버 내에 장치를 구분하기 위해 사용
- 각 디바이스의 부가적인 정보를 나타냄. 2Byte(부번호)
- 하나의 디바이스 드라이버가 여러 개의 디바이스 제어 가능



*ex) `ls -al /dev/sdb`*

![image-20201028140243569](https://user-images.githubusercontent.com/58545240/97408585-a4982300-193f-11eb-9b7b-4d150ff1aafd.png)

## 24.4 디바이스 드라이버 구조

리눅스 시스템 구조 상의 디바이스 드라이버는 아래와 같다.

![image-20201028140326578](https://user-images.githubusercontent.com/58545240/97408634-b5e12f80-193f-11eb-931a-f43273aaf7d9.png)

위의 `Kernel area` 쪽을 보면 디바이스 인터페이스 위에 디바이스 드라이버들이 있다. 문자 디바이스 드라이버는 버퍼캐시를 사용하지 않기 때문에 (그래서 나중에 ioremap_nocache함수를 사용함) 실습에서 많이 사용한다.

# **25. 모듈 프로그래밍**

---

**커널 모듈(Kernel Module)**

- 시스템 부팅후에 동적으로 `loading`할 수 있는 커널 구성요소를 말한다.
- 커널을 다시 컴파일하거나 시스템 재부팅 할 필요없이 커널의 일부분을 교체하는 것이 가능하다
- 디바이스 드라이버, 파일 시스템, 네트워크 프로토콜 등이 모듈로 제공된다.

> 일반 응용프로그램과 뭐가 다른거죠??

커널 모듈은 일반 응용 프로그램과 달리 main함수가 없다.

대신에 커널에 로딩 및 제거될 때 불러지는 함수가 존재하는데, 이는 아래와 같다.

- Loading 시 : `module_init()`로 지정된 함수 호출
- Unloading 시 : `module_exit()`로 지정된 함수 호출

## 25.1 리눅스 디바이스 드라이버의 특성

1. 커널 코드
   - 디바이스 드라이버는 커널의 한 부분이므로, 커널의 다른 코드와 마찬가지로 잘못되면 시스템에 치명적인 피해를 줄 수 있다.
2. 커널 인터페이스
   - 디바이스 드라이버는 리눅스 커널이나 자신이 속한 서브시스템에 표준 인터페이스를 제공해야 한다.
3. 커널 매커니즘과 서비스
   - 디바이스 드라이버는 **메모리 할당**, **인터럽트 전달**, **wait queue**와 같은 표준 커널 서비스를 사용할 수 있다.
4. Loadable
   - 대부분의 리눅스 드라이버는 커널 모듈로서, 필요할 때 `Load`하고 더이상 필요하지 않을 때 `Unload`할 수 있다.
5. 설정 가능(Configurable)
   - 리눅스 디바이스 드라이버를 커널에 포함하여 컴파일할 수 있다. 어떤 장치를 넣을 것인지는 커널을 컴파일 할 때 설정할 수 있다.

## 25.2 간단한 커널모듈 작성해보기

### 1. hello.c

```c
//hello.c
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/init.h>

MODULE_LICENSE("GPL");
MODULE_AUTHOR("BUTTER SHOWER");
MODULE_DESCRIPTION("module programming - hello module");

static int __init module_begin(void) {
    printk("Hello, linux kernel module. \n");
    return 0;
}

static void __exit module_end(void) {
    printk("Good Bye!\n");
}
module_init(module_begin);
module_exit(module_end);
```

1. 필요한 include 파일들 include 시켜주기
2. module license 지정
   - 종류 : `GPL, GPL v2, Dual BSD/GPL, Proprietary` 등
   - 커널 2.6부터 반드시 지정해야 함
3. `module_init` 함수 작성 및 등록 : `module_init(init_func);`
4. `module_exit` 함수 작성 및 등록 : `module_exit(exit_func);`

### 2. 커널 모듈 프로그램을 위한 Makefile

```bash
obj-m += hello.o
all :
	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules
clean :
	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean
```

`Makefile`이라는 이름으로 위의 코드를 붙여 만들어 준후, `make` 명령어를 하면 아래와 같이 실행된다.

![image-20201028141518368](https://user-images.githubusercontent.com/58545240/97408651-bbd71080-193f-11eb-8c1c-1a7cb287aee4.png)

**컴파일 과정**

1. `hello.o` 파일 먼저 생성
2. `modpost`를 C 소스파일에 적용해 `.ko`에서 요구되는 추가 정보를 부착하여 `hello.mod.c`를 생성한 후 컴파일 -> `hello.mod.o` 생성
3. 두개의 `.o`파일을 링크하여 `.ko(kernel object)`파일을 생성

### 3. 모듈 적재(loading) 그리고 제거(unloading)

1. 생성된 모듈(hello.ko)을 로딩

   ```bash
   insmod hello.ko
   ```

2. 커널에 적재된 모듈 목록보기

   ```bash
   lsmod
   ```

3. 모듈 제거

   ```bash
   rmmod hello # (.ko가 붙지 않는다)
   ```

4. hello 모듈 동작 확인

   ```bash
   # 모듈 적재와 제거 시에 메시지들이 출력되는지 확인
   #dmesg 또는
   #tail -f /var/log/kern.log (-f 옵션 : 계속적인 변화 출력)
   ```

## 25.3 디바이스 드라이버 작성 방법

### 1. 디바이스 드라이버 함수 작성

- struct file_operations 정의
- open, release, read, write, ioctl 함수 구현
- init, exit 함수 구현

### 2. 커널에 디바이스 드라이버 등록

> Init 함수에서 수행

```c
int res;

res = register_chardev();	// char driver
res = register_blkdev();	// block driver
res = register_netdev();	// network driver
```

(사실 register_xxxdev() 함수 파라미터에 필요한 값들이 있지만 일단 생략.. 추후 포스팅 예시를 보면 알 것이다)

### 3. 컴파일/로딩

```bash
# make		... Makefile 작성 후 실행
# insmod	... 생성된 .ko 파일 load
```

### 4. 디바이스 파일 생성

```bash
# mknode [디바이스 파일 이름] [드라이버타입] [주번호] [부번호]

(예시)
#mknod /dev/LED c 239 0
```

필요하면 속성을 변경해주면 된다.

```bash
# chmod ug+w /dev/LED
```

### 5. 디바이스 파일에 입출력하는 응용프로그램 작성 및 테스트

디바이스 파일에 입출력하는 응용프로그램을 작성하고, 테스트한다.

커널 영역을 침범하는 파일이기 때문에 작성에 유의해야 한다.

## 25.4 디바이스 드라이버 골격

디바이스 드라이버의 골격은 아래와 같다.

![image-20201028142247092](https://user-images.githubusercontent.com/58545240/97408684-ca252c80-193f-11eb-8047-c30f48b61892.png)

# **26. 커널에 모듈 추가하기**

> **Kconfig**
>
> => `Kernel`에 새로운 장치를 추가하고, 해당하는 모듈을 추가하기 위해서는 먼저 **Kconfig**라는 파일을 알아야한다~

**Kconfig**를 정리하자면

1. Tree구주로 된 Configuration option들의 집합으로 자신만의 종속성을 가지고 있음
2. Child Entry는 Parent Entry가 선택되어 볼 수 있게 되었을 때만 보임
3. Menu entry들은 각각의 config option을 정의하고 있음.
4. 각각의 config option들은 자신만의 type을 가지며, **tristate, bool** type이 있음
   - `tristate` : **`<*> <M> <>`**
   - `bool` : **<*> <>**
5. 이 type에 따라 어떤 선택을 하느냐에 따라 Kernel에 해당 모듈을 포함하는 여부가 결정된다.( 보통 `make menuconfig`에서 선택한다.)
6. 사용자에 의해 값이 설정되지 않으면 `default`값이 config option에 설정된다.

*아래는 한 폴더의 sound/drivers/Kconfig 파일의 일부이다.*

![image-20201030162612401](https://user-images.githubusercontent.com/58545240/97673544-4bf79000-1acf-11eb-80c7-a950d0b875a1.png)

- 모듈을 선택할 때 사용하지 않는 기능을 `<*>`로 선택하면 빌드되어 나오는 이미지(Kernel을 full build할 때 나오는 zImage)가 너무 커지며, 필요한 기능을 `< >`으로 선택하면 kernel이 정상동작 하지 않을 수 있으니 주의해서 선택해야 한다.
- `<M>`으로 선택할 경우 Kernel 빌드시 `***.ko` 파일을 생성하며, Kernel이 부팅하며 초기화 작업을 할 때 **insmod**로 등록하여 사용한다. 해당 모듈을 내릴 때는 **rmmod**를 사용한다.

## Module 추가하기

> **Module**을 추가하기 위해서는 **Makefile**과 **Kconfig**를 수정해야 한다.
>
> 여기서는 그중 그나마 만만한 Char Device Driver 쪽에 추가하기로 한다.

1. **Makefile**

   - 먼저 `[Kernel] / drivers / char / Makefile`을 연다.
   - 모듈이 빌드되도록 내용을 추가한다.

   ![image-20201030163247119](https://user-images.githubusercontent.com/58545240/97673552-5154da80-1acf-11eb-9e2d-fefcb1006371.png)

   - CONFIG_MY_MODULE이라는 이름이 중요한데, 이 이름을 Kconfig 파일과 맞춰야 하기 때문이다.

2. **Kconfig**

   - `[Kernel] / drivers / char / Kconfig` 파일을 연다
   - Config에 추가되도록 아래 내용을 추가한다.

   ![image-20201030163355759](https://user-images.githubusercontent.com/58545240/97673576-59147f00-1acf-11eb-8db2-bd1777d644b6.png)

   - **DEPEND_ON_ME**를 선택해야 **MY_MODULE**이 보인다.
   - **MY_MODULE**은 Makefile의 **CONFIG_MY_MODULE**과 CONFIG_ 뒷 부분이 일치해야 한다.

3. **`$ make menuconfig`**

   - **`Device Drivers > Character devices`**로 들어간다.
   - 아래 그림에서처럼 **Depend on test **항목이 추가됨을 볼 수 있다.

   ![image-20201030163536757](https://user-images.githubusercontent.com/58545240/97673592-603b8d00-1acf-11eb-9aed-d30e37455f12.png)

   - **Depend on test**를 선택한다.

   ![image-20201030163557669](https://user-images.githubusercontent.com/58545240/97673600-6598d780-1acf-11eb-92d4-6a9c8633e333.png)

   - **Add My Module**이 추가 된 것을 볼 수 있다.

   *참고로 menuconfig에서 `[]`형태는 `bool` type이고 `<>`형태는 `tristate` type이다.

   - Kernel이 빌드된 후에 depends on이 사용된 항목들은 **`[kernel] / include / generated / autoconfig.h`**에 define값이 만들어 진다.

   ![image-20201030163720768](https://user-images.githubusercontent.com/58545240/97673617-6d587c00-1acf-11eb-9166-0d25f87c44b4.png)

   - 위의 그림처럼 **CONFIG_MY_MODULE**이 포함됨을 확인할 수 있다.

# **27. Platform Device & Driver**

---

> 플랫폼디바이스는 하드웨어에 내장되어 있기 때문에 `hot-pluggable`이 아니다.
>
> 이 장치를 위한 드라이버는 장치의 존재를 확인할 필요가 없고 그냥 작동시키고(전원을 켜고) 장치가 작동하게 할 수 있도록 필요한 것을 작업한다.
>
> 만약 장치가 발견되지 않으면 드라이버는 가볍게 무시된다.

플랫폼 디바이스는 리눅스 커널이 `USB`나 `PCI`와 같은 버스를 통해 동적으로 감지할 수 없는 `SoC(System-on-Chip)`에 내장된 시스템 장치이다.

커널은 플롯폼 디바이스 메커니즘을 제공함으로써 실제로 존재하는 하드웨어에 대해 알 수 있다. 이 글에서는 플랫폼 디바이스의 커널 인터페이스에 대해 설명하며 디바이스 트리와 통합을 하기 위한 내용을 설명하겠다.

## Platform Drivers

플랫폼 디바이스는 `struct platform_device`로 정의되며 `<linux/platform_device.h>`에서 찾을 수 있다. 

이 장치들은 가상 "플랫폼 버스"에 연결된 것으로 간주된다. 

따라서 플랫폼 디바이스의 드라이버는 플랫폼 버스에 등록해야 한다. 이 등록은 `platform_driver` 구조체를 통해 수행된다.

![image-20201105104020008](https://user-images.githubusercontent.com/58545240/98198238-dd518100-1f6b-11eb-87e1-52f0a82ad430.png)

최소한 `probe()` 및 `remove()` **콜백**을 제공해야 하며, 다른 콜백은 전원 관리와 관련이 있으며 필요시 제공하면 된다.

- `(*probe)`
  - HW 디바이스 존재 유무를 판단하기 위해 호출되고 해당 디바이스를 사용하기 위해 디바이스 드라이버와 바인드한다.
  - 디바이스 리소스를 사용하여 `irq`, 레지스터 맵핑 등을 할 수 있다.
- `(*remove)`
  - 디바이스의 사용을 완료시킨 경우 호출된다.
- `(*shutdown)`
  - 디바이스의 전원을 끄려고할 때 호출된다.
- `(*suspend)`
  - 디바이스가 절전 모드로 진입할 때 호출된다.
- `(*resume)`
  - 디바이스가 절전모드로부터 정상모드로 돌아올 때 호출된다.
- `driver`
  - 디바이스 드라이버 구조체가 임베드된다.
- `*id_table` : *아래 설명 참조*
- `prevent_deffered_probe`
  - probe 유예금지

드라이버가 제공해야하는 다른 것은 버스 코드가 실제 장치를 드라이버에 **바인딩(binding)**하는 방법이다.

사용할 수 있는 두 가지 메커니즘이 있는데 그 중 하나는 **id_table**이다. `id_table`의 구조체는 다음과 같다.

![image-20201105104336761](https://user-images.githubusercontent.com/58545240/98198298-06721180-1f6c-11eb-9b9a-dd4fffa58406.png)

`id_table`이 있으면 플랫폼 버스는 새 플랫폼 디바이스의 드라이버를 찾을 때마다 이를 스캔한다. 장치 이름이 `id_table`항목의 이름와 일치하면 장치는 관리를 위해 드라이버에게 제공되며 일치하는 `id_table` 항목에 대한 포인터도 드라이버에서 사용할 수 있다.

`id_table`을 제공하지 않을 경우 driver 필드에 드라이버 이름을 제공해야 한다. 예를 들어 `soundgen`드라이버는 다음과 같은 `platform_device`로 설정된다.

![image-20201105104649332](https://user-images.githubusercontent.com/58545240/98198347-1be73b80-1f6c-11eb-8ca5-24554c266cc2.png)

이 설정을 통해 **"snd_soundgen"**으로 식별되는 모든 장치가 이 드라이버에 바인딩된다.

플랫폼 드라이버는 다음을 통해 반드시 자신을 커널에 알려야 한다.

![image-20201105104846594](https://user-images.githubusercontent.com/58545240/98198312-0c67f280-1f6c-11eb-87c4-375f5d00a3db.png)

이 호출이 성공하면 드라이버의 `probe()` 함수가 호출 될 수 있다. 이 함수는 인스턴스화 할 장치를 설명하는 `platform_device` 포인터를 가져온다.

![image-20201105105136020](https://user-images.githubusercontent.com/58545240/98198377-29042a80-1f6c-11eb-9ad0-ec9c8f2cb517.png)

`dev` 필드는 필요한 상황(ex. DMA 매핑 API)에서 사용할 수 있다.

장치가 `id_table`항목을 사용하여 일치하면 `id_entry`는 일치하는 항목을 가리킨다.

`resource`배열은 메모리 매핑된 I/O 레지스터 및 인터럽트를 포함한 다양한 리소스를 찾는 데 사용할 수 있다. 리소스 배열에서 데이터를 가져오기 위한 여러가지 도우미 함수가 있는데 몇가지 함수의 예를 보자.

![image-20201105105517088](https://user-images.githubusercontent.com/58545240/98198390-2f92a200-1f6c-11eb-8a96-e384652fc369.png)

마지막 매개변수는 `index`를 나타내며 0은 첫 번째 리소스이다. 예를 들어 드라이버는 다음을 통해 두번째 `MMIO` 영역을 찾을 수 있다.

```c
r = platfrom_get_resource(pdev, IORESOUCRE_MEM, 1);
```



**이 드라이버를 OS에 등록해야 한다. 디바이스가 당연히 시스템에 존재한다는 것을 알기 때문에 `platform_driver_register()` 대신 `platform_driver_probe()`를 사용해야 한다.**

이 두 기능의 차이점은 `register()`는 OS가 시스템에 들어오고 나올 때 드라이버를 일치시키기 위해 유지하는 드라이버 목록에 이 드라이버를 넣으라는 것이다.

플랫폼장치는 항상 시스템에 존재하므로(핫 플러그 불가능) 플랫폼 드라이버를 OS 드라이버 목록에 넣을 필요가 없다.

`probe()`는 우리가 OS가 플랫폼 장치가 일치하는 이름으로 존재하는지 확인하도록 요청하는 것이다. 장치가 있으면 해당 `probe()`기능이 호출되고 존재하지 않으면 드라이버는 무시된다.



## Platform Device

처음에 언급했듯이 플랫폼 디바이스는 본질적으로 검색 할 수 없으므로 커널에 장치의 존재를 알리는 다른 방법이 있어야 한다.

이는 일반적으로 관련 드라이버를 찾는 데 사용되는 정적 `struct platform_device` 구조체를 작성하여 수행된다. 예를 들어 간단한 장치는 다음과 같이 설정될 수 있다.

![image-20201105110521877](https://user-images.githubusercontent.com/58545240/98198414-420cdb80-1f6c-11eb-92af-359b6beb856a.png)

이 선언은 1페이지 `MMIO` 영역이 `0x10000000`에서 시작하고 `IRQ 20`을 사용하는 "foomatic"장치를 설명한다. 장치는 다음을 통해 시스템에 알린다.

![image-20201105110842707](https://user-images.githubusercontent.com/58545240/98198419-46d18f80-1f6c-11eb-97cc-2d3d7f308a58.png)

- `device_initailize(&pdev->dev)` : 플랫폼 디바이스 내부의 디바이스 구조체의 멤버들을 초기화
- `arch_setup_pdev_archdata(pdev)` : 플랫폼 디바이스의 `archdata` 조작이 필요한 경우 호출
- `return platform_device_add(pdev)` : 플랫폼 디바이스 추가

```c
int platform_device_register(struct platform_device *pdev);
```

이를 `platform_device_simple_register`로 사용할 수도 있다.

![image-20201105111020446](https://user-images.githubusercontent.com/58545240/98198427-4cc77080-1f6c-11eb-85ab-7c124620d3d3.png)

```c
struct platform_device *device;
device = platform_device_register_simple(SND_SOUNDGEN_DRIVER, 0, NULL, 0);
```



이렇게 플랫폼 디바이스와 관련 드라이버가 모두 등록되면 드라이버의 `probe()`함수가 호출되고 장치가 인스턴스화된다.

플랫폼 디바이스를 제거하려면 `platform_device_unregister()`함수를 사용한다.

## - 드라이버 모듈 진입부

**플랫폼 드라이버 모듈의 마지막에 다음과 같은 매크로함수 중 하나를 사용하여 플랫폼 드라이버의 등록부 코드를 준비한다.**

- 모든 디바이스 및 드라이버 공통 진입부
  - device_initcall(foo_init)
    - 커널에 임베드하여 빌드한 경우 부트업 시 동작하며 모든 디바이스 및 드라이버의 진입부에 사용된다.
    - 커널에 임베드하지 않고 모듈로 빌드한 경우 module_init()과 동일하게 동작한다.
  - module_init(foo_init) & module_exit(foo_exit)
    - insmod를 사용한 모듈 방식으로 모든 디바이스 및 드라이버의 진입부에 사용된다.
- 플랫폼 드라이버용 진입부
  - builtin_platform_driver(foo_driver)
    - 커널에 임베드하여 빌드한 경우 부트업 시 동작하며 플랫폼 드라이버 진입부에 사용된다.
    - 커널에 임베드하지 않고 모듈로 빌드한 경우 module_platform_driver()와 동일하게 동작한다.
  - module_platform_driver(foo_driver)
    - insmod를 사용한 모듈 방식으로 플랫폼 드라이버용 진입부에 사용된다.
- 플랫폼 디바이스 생성 및 플랫폼 드라이버용 진입부
  - builtin_platform_driver_probe(foo_driver, foo_probe)
    - 커널에 임베드하여 빌드한 경우 부트업 시 동작하며 플랫폼 디바이스를 생성하면서 플랫폼 드라이버 진입에 사용된다.
    - 커널에 임베드하지 않고 모듈로 빌드한 경우 module_platform_driver_probe()와 동일하게 동작한다.
  - module_platform_driver_probe(foo_driver, foo_probe)
    - insmod를 사용한 모듈 방식으로 플랫폼 디바이스를 생성하면서 플랫폼 드라이버 진입에 사용된다.

**플랫폼 드라이버를 등록하기 위해 다음을 준비한다.**

- probe() 함수
- platform_device_id 구조체
- platform_driver 구조체

1. **`device_initcall()`** 매크로 함수 사용 시

   *device_initcall() 함수의 동작은 다음과 같이 내부적으로 두 가지 상황으로 처리된다.*

   - 이 드라이버가 커널에 임베드되어(커널의 menuconfig에서 ‘*’ 선택) 로드하는 경우
     - 커널의 부트업 시 initcall 호출 과정에서 foo_driver를 등록한다.
     - 그리고 이에 대한 디바이스도 이미 로드되어 매치 가능한 경우 bind하여 곧바로 foo_probe() 함수를 probe한다.
   - 커널에 임베드되지 않고 모듈 형태로 빌드한(커널의 menuconfig에서 ‘m’ 선택) 후 사용자 영역에서 insmod 명령에 의해 로드하는 경우
     - insmod 명령에 의해 드라이버 모듈을 로드하고 foo_init() 함수를 호출하여 foo_driver를 등록한다.
     - 역시 이에 대한 디바이스도 이미 로드되어 매치 가능한 경우 bind하여 곧바로 foo_probe() 함수를 probe한다.

   ```c
   static int __init foo_init(void) {
       return platform_driver_register(&foo_driver);
   }
   device_initcall(foo_init);
   ```

2. **`module_init() & module_exit()`** 매크로 함수 사용 시

   *드라이버를 항상 모듈로 만들어 insmod에 의해서만 드라이버를 로드하려면 device_initcall() 대신 module_init()을 사용한다.*

   ```c
   static int __init foo_init(void) {
       return platform_drvier_register(&foo_driver);
   }
   static void __exit foo_exit(void) {
       platform_driver_unregister(&foo_driver);
   }
   module_init(foo_init);
   module_exit(foo_exit);
   ```

3. **`builtin_platform_driver()`** 매크로 함수 사용시

   *커널 부트업 시 사용할 드라이버를 등록한다. device_initcall()보다 더 심플하게 코딩할 수 있다.*

   ```C
   builtin_platform_driver(&foo_driver);
   ```

   ![image-20201105113153816](https://user-images.githubusercontent.com/58545240/98198490-69fc3f00-1f6c-11eb-8ae4-38da229ee155.png)

   ![image-20201105131126045](https://user-images.githubusercontent.com/58545240/98198536-826c5980-1f6c-11eb-9704-281ad8ce18c3.png)

   

4. **`moudle_platform_driver()`** 매크로 함수 사용 시

   위의 `module_init()` 및 `module_exit()` 함수를 아래 매크로로 유사하게 만들어 사용하는 방법으로 매우 심플하게 코딩할 수 있따.

   ```c
   module_platform_driver(&foo_driver);
   ```

   

   ![image-20201105131020416](https://user-images.githubusercontent.com/58545240/98198499-6ec0f300-1f6c-11eb-90a6-9c2f38e4dcaa.png)

   ![image-20201105131052830](https://user-images.githubusercontent.com/58545240/98198520-7b454b80-1f6c-11eb-8c11-6b2f65ed189e.png)

   

5. **`builtin_platform_driver_probe()`** 매크로 함수 사용 시

   builtin_platform_driver() 매크로 함수와 유사한 방식이다. device_driver 구조체에서 probe 함수 및 id_table을 지정하지 않은 것이 다른 점이다. 역시 아래와 같이 매우 심플하게 코딩할 수 있다.

   ```c
   static struct platform_driver foo_driver = {
           .remove = __exit_p(foo_remove),
           .driver = {
               .name = "foo",
           },
   };
   builtin_platform_driver_probe(foo_driver, foo_probe);
   ```

   ![image-20201105131425113](https://user-images.githubusercontent.com/58545240/98198623-b6477f00-1f6c-11eb-917b-6983f26d7755.png)

6. **`module_platform_driver_probe()`** 매크로 함수 사용 시

   module_platform_driver() 매크로 함수와 유사한 방식이다. device_driver 구조체에서 probe 함수 및 id_table을 지정하지 않은 것이 다른 점이다. 역시 아래와 같이 매우 심플하게 코딩할 수 있다.

   ```c
   static struct platform_driver foo_driver = {
           .remove = __exit_p(foo_remove),
           .driver = {
               .name = "foo",
           },
   };
   module_platform_driver_probe(foo_driver, foo_probe);
   ```

   ![image-20201105131543392](https://user-images.githubusercontent.com/58545240/98198630-bb0c3300-1f6c-11eb-8a14-eb9c0493a639.png)

## 전체소스

*해당 소스는 ALSA Sound Card - Dummy의 일부와 동일하다*

```c
#include <linux/init.h>
#include <linux/err.h>
#include <linux/platform_device.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/jiffies.h>
#include <linux/slab.h>
#include <linux/time.h>
#include <linux/wait.h>
#include <sound/asound.h>
#include <sound/memalloc.h>
#include <sound/pcm.h>
#include <sound/core.h>
#include <sound/control.h>
#include <sound/info.h>
#include <sound/initval.h>

MODULE_AUTHOR("KIM SUNG HYUN <dhkdghehfdl@gmail.com>");
MODULE_LICENSE("GPL");
MODULE_DESCRIPTION("THIS IS VOIP soundcard");
MODULE_VERSION("1.0");

/* defaults */
#define USE_FORMATS     (SNDRV_PCM_FMTBIT_U8 | SNDRV_PCMFMTBIT_S16_LE)

struct soundgen_timer_ops {
    int (*create)(struct snd_pcm_substream *);
    void (*free)(struct snd_pcm_substream *);
	int (*prepare)(struct snd_pcm_substream *);
	int (*start)(struct snd_pcm_substream *);
	int (*stop)(struct snd_pcm_substream *);
	snd_pcm_uframes_t (*pointer)(struct snd_pcm_substream *);
};

#define get_soundgen_ops(substream) \
    (*(const struct soundgen_timer_ops **)(substream)->runtime->private_data)

/*
플랫폼 장치를 만든 후 ALSA 하위 시스템에 장치를 등록해야한다.
이것은 사운드 카드를 생성하여 수행된다.
*/
struct snd_card_soundgen {
    struct snd_card *card;

    /*
    사운드 카드가 있어도 실제 오디오 데이터를 사용자에게 제공할 방법이 없다. 따라서 ALSA는 사용자 공간에 전달할 여러 형식 및 
    오디오 스트림을 지원하는 데 가장 일반적인 것이 "PCM"이다.
    최종 드라이버는 일부 외부 라인에서 8비트 PCM 샘플을 검색하므로 PCM 레이어에만 집중할 것이다. (MIDI support도 가능하다.)
    */
    struct snd_pcm *pcm;
    struct snd_pcm_hardware pcm_hw;
    spinlock_t mixer_lock;
};

struct soundgen_systimer_pcm {
    /* ops must be the first item */
    const struct soundgen_timer_ops *timer_ops;
    spinlock_t lock;
    struct timer_list timer;
    unsigned long base_time;
    unsigned int frac_pos;  /* fractional sample position (based HZ)  */
    unsigned int frac_period_rest;
    unsigned int frac_buffer_size;  /* buffer_size * HZ */
    unsigned int frac_period_size;  /* period_size * HZ */
    unsigned int rate;
    int elapsed;
    struct snd_pcm_substream *substream;
};

static void soundgen_systimer_rearm(struct soundgen_systimer_pcm *dpcm)
{
    mod_timer(&dpcm->timer, jiffies + 
        (dpcm->frac_period_rest + dpcm->rate -1) / dpcm->rate);
}

static void soundgen_systimer_update(struct soundgen_systimer_pcm *dpcm)
{
    unsigned long delta;

    delta = jiffies - dpcm->base_time;
    if(!delta)
        return;
    dpcm->base_time += delta;
    delta *= dpcm->rate;
    dpcm->frac_pos += delta;
    while (dpcm->frac_pos >= dpcm->frac_buffer_size)
        dpcm->frac_pos == dpcm->frac_buffer_size;
    while (dpcm->frac_period_rest <= delta) {
        dpcm->elapsed++;
        dpcm->frac_period_rest += dpcm->frac_period_size;
    }
    dpcm->frac_period_rest -= delta;
}

static int soundgen_systimer_start(struct snd_pcm_substream *substream)
{
    struct soundgen_systimer_pcm *dpcm = substream->runtime->private_data;
    spin_lock(&dpcm->lock);
    dpcm->base_time = jiffies;
    soundgen_systimer_rearm(dpcm);
    spin_unlock(&dpcm->lock);
    return 0;
}

static int soundgen_systimer_stop(struct snd_pcm_substream *substream)
{
    struct soundgen_systimer_pcm *dpcm = substream->runtime->private_data;
    spin_lock(&dpcm->lock);
    del_timer(&dpcm->timer);
    spin_unlock(&dpcm->lock);
    return 0;
}

/*
PCM 설정을 준비할 때 호출한다.
형식, 속도를 설정하거나 타이머를 설정한다.
hw_params와의 차이점은 snd_pcm_prepare를 호출할 때마다 이 함수가 호출된다는 것이다.
*/
static int soundgen_systimer_prepare(struct snd_pcm_substream *substream)
{
    struct snd_pcm_runtime *runtime = substream->runtime;
	struct soundgen_systimer_pcm *dpcm = runtime->private_data;

	dpcm->frac_pos = 0;
	dpcm->rate = runtime->rate;
	dpcm->frac_buffer_size = runtime->buffer_size * HZ;
	dpcm->frac_period_size = runtime->period_size * HZ;
	dpcm->frac_period_rest = dpcm->frac_period_size;
	dpcm->elapsed = 0;

	return 0;
}

/*
초기 드라이버의 마지막 부분은 전체 드라이버를 구동하는 인터럽트이다.
일반적으로 이것은 하드웨어에 의해 구동되는 인터럽트이다.
말했듯이 타이머를 사용하여 인터럽트를 생성하기 전에 하드웨어 인터럽트는 없다.
다음 코드는 타이머를 재 설정하고 위치를 계산하는 데 사용하는 카운터를 업데이트하며 기간이 경과하면 경과된
기간을 CPM 중간 계층에 알린다. 
여기서 데이터를 업데이트 할 수도 있다.
*/
static void soundgen_systimer_callback(unsigned long data)
{
    pr_info("Systimer callback\n");
    struct soundgen_systimer_pcm *dpcm = (struct soundgen_systimer_pcm *)data;
    unsigned long flags;
    int elapsed = 0;

    spin_lock_irqsave(&dpcm->lock, flags);
    soundgen_systimer_update(dpcm);
    soundgen_systimer_rearm(dpcm);
    elapsed = dpcm->elapsed;
    dpcm->elapsed = 0;
    spin_unlock_irqrestore(&dpcm->lock, flags);
    if(elapsed)
        snd_pcm_period_elapsed(dpcm->substream);
}

static snd_pcm_uframes_t soundgen_systimer_pointer(struct snd_pcm_substream *substream)
{
    struct soundgen_systimer_pcm *dpcm = substream->runtime->private_data;
    snd_pcm_uframes_t pos;

    spin_lock(&dpcm->lock);
    soundgen_systimer_update(dpcm);
    pos = dpcm->frac_pos / HZ;
    spin_unlock(&dpcm->lock);
    return pos;
}

static int soundgen_systimer_create(struct snd_pcm_substream *substream)
{
    struct soundgen_systimer_pcm *dpcm;

    dpcm = kzalloc(sizeof(*dpcm), GFP_KERNEL);
    if (!dpcm)
        return -ENOMEM;
    substream->runtime->private_data = dpcm;
    setup_timer(&dpcm->timer, soundgen_systimer_callback, (unsigned long) dpcm);
    spin_lock_init(&dpcm->lock);
    dpcm->substream = substream;
    return 0;
}

static void soundgen_systimer_free(struct snd_pcm_substream *substream)
{
    kfree(substream->runtime->private_data);
}

static const struct soundgen_timer_ops soundgen_systimer_ops = {
    .create =   soundgen_systimer_create,
    .free =     soundgen_systimer_free,
    .prepare =  soundgen_systimer_prepare,
    .start =    soundgen_systimer_start,
    .stop =     soundgen_systimer_stop,
    .pointer =  soundgen_systimer_pointer,
};

// 스트림이 시작되거나 중지되면 트리거 콜백을 통해 수행된다. 필요할 경우 일시정지 및 재개도 처리할 수 있다.
static int soundgen_pcm_trigger(struct snd_pcm_substream *substream, int cmd)
{
    pr_info("PCM Trigger\n");
    pr_debug("Command: %d\n", cmd);
    switch(cmd) {
        case SNDRV_PCM_TRIGGER_START:
            /* do something to start the PCM engine */
            return get_soundgen_ops(substream)->start(substream);
        case SNDRV_PCM_TRIGGER_STOP:
            return get_soundgen_ops(substream)->stop(substream);
    }
    return -EINVAL;
}

static int soundgen_pcm_prepare(struct snd_pcm_substream *substream)
{
    return get_soundgen_ops(substream)->prepare(substream);
}

/*
snd_pcm_period_elpased가 호출될 때 실행된다.
일반적으로 snd_pcm_period_elpased는 일종의 인터럽트에 의해 호출된다.
현재 하드웨어 인터럽ㅌ크가 없으므로 타이머를 사용하여 생성한다. 포인터 오프셋도 타이머의 경과시간에 따라 계산된다.
*/
static snd_pcm_uframes_t soundgen_pcm_pointer(struct snd_pcm_substream *substream)
{
    pr_info("PCM Pointer\n");
    return get_soundgen_ops(substream)->pointer(substream);
}

static const struct snd_pcm_hardware snd_soundgen_hw = {
    .info =     (SNDRV_PCM_INFO_MMAP | SNDRV_PCM_INFO_INTERLEAVED | 
                SNDRV_PCM_INFO_BLOCK_TRANSFER | SNDRV_PCM_INFO_MMAP_VALID),
    .formats =  SNDRV_PCM_FMTBIT_S8,
    .rates =    SNDRV_PCM_RATE_8000,
    .rate_min = 8000,
    .rate_max = 8000,
    .channels_min = 2,
    .channels_max = 2,
    .buffer_bytes_max = 32768,
    .period_bytes_min = 64,
    .period_bytes_max = 32768,
    .periods_min = 1,
    .periods_max = 1024,
    .fifo_size = 0,
};

/*
모든 매개 변수가 하드웨어에 의해 설정되는 동안 이 함수는 여러번 호출 된다.
snd_pcm_hw_params 구조체에서 정보를 검색하는 데 사용할 수 있는 여러 매크로가 있고 필요한 경우
여기에서 하드웨어를 올바르게 구성하는 것이 중요하다.
*/
static int soundgen_pcm_hw_params(struct snd_pcm_substream *substream, struct snd_pcm_hw_params *params)
{
    pr_info("HW Params\n");
    pr_debug("Buffer size %d\n", params_buffer_bytes(params));
    /*
    snd_pcm_lib_malloc_pages() 함수를 이용해 약간의 메모리를 할당한다.
    일부 버퍼가 이미 사전 할당된 경우에만 작동한다. 우리의 경우 우리는
    snd_pcm_lib_malloc_pages_all()을 도는 동안 snd_card_pcm_new 일부 메모리를 미리 할당하는 기능을 사용한다.
    (이 함수는 초기화 중에 여러번 호출 될 수 있으므로 사용자 지정 메모리 할당을 사용하는 경우 많은 메모리에 할당하지 않도록 주의)
    */
    return snd_pcm_lib_malloc_pages(substream, params_buffer_bytes(params));
}

// hw_params 함수에 의해 할당 된 데이터를 해제
static int soundgen_pcm_hw_free(struct snd_pcm_substream *substream)
{
    pr_info("HW free\n");
    return snd_pcm_lib_free_pages(substream);
}

/*
PCM 열기 함수는 서브 스트림이 열릴 때마다 호출된다. ( 녹음 또는 재생이 시작될 때 )
이 기능을 수행하는 동안 하드웨어를 올바르게 설정하는 것이 중요하다.
snd_pcm_hardware 구조체를 runtime -. hw/chip -> pcm_hw 구조체에 복사한다.
여기에 일부 장치별 개인 데이터를 할당 할 수 도 있다.
우리의 경우 장치에 따라 다르기 때문에 여기에 타이머를 할당한다.
*/
static int soundgen_pcm_open(struct snd_pcm_substream *substream)
{
    int err;
    struct snd_card_soundgen *chip = snd_pcm_substream_chip(substream);
    if (!chip) {
        pr_info("Faiuled to retreiver chipo\n");
        return -1;
    }
    struct snd_pcm_runtime *runtime = substream->runtime;
    pr_info("Opening PCM\n");
    const struct soundgen_timer_ops *ops;
    ops = &soundgen_systimer_ops;

    get_soundgen_ops(substream) = ops;
    
    runtime->hw = snd_soundgen_hw;
    chip->pcm_hw = runtime->hw;

    if(substream->pcm->device & 1) {
        runtime->hw.info &= ~SNDRV_PCM_INFO_INTERLEAVED;
        runtime->hw.info |= SNDRV_PCM_INFO_NONINTERLEAVED;
    }
    if(substream->pcm->device & 2) {
        runtime->hw.info &= ~(SNDRV_PCM_INFO_MMAP || SNDRV_PCM_INFO_MMAP_VALID);
    }

    return 0;
}
// 서브스트림을 닫을 때 호출. open함수에 할당된 개인 데이터를 정리한다.
static int soundgen_pcm_close(struct snd_pcm_substream *substream)
{
    pr_info("CLosing PCM\n");
    get_soundgen_ops(substream)->free(substream);
    return 0;
}

/*
snd_pcm_ops구조체
*/

/*
주로 사용할 snd_pcm_ops 구조체
열기 함수는 서브 스트림이 열릴 때마다 호출된다.
예를 들어 녹음 또는 재생이 시작될 때. 이 그능을 수행하는 동안 하드웨어를 올바르게 설정하는 것이 중요하다.
*/
static struct snd_pcm_ops soundgen_pcm_ops = {
    .open = soundgen_pcm_open,
    .close = soundgen_pcm_close,
#ifdef DEBUG
    .ioctl = soundgen_pcm_ioctl_wrap,
#else
    .ioctl = snd_pcm_lib_ioctl,
#endif
    .hw_params = soundgen_pcm_hw_params,
    .hw_free = soundgen_pcm_hw_free,
    .prepare = soundgen_pcm_prepare,
    .trigger = soundgen_pcm_trigger,
    .pointer = soundgen_pcm_pointer
};



/*
PCM 장치를 만드는 것은 커널의 다른 모든 장치와 동일하다.
새 장치를 만들고 함수 포인터를 포함하는 구조체를 채우고 어떻게든 이를 새 장치에 연결한다.
*/
static int snd_soundgen_new_pcm(struct snd_card_soundgen *soundgen_card)
{
    // PCM의 주요 구조는 snd_pcm 구조체이다.
    struct snd_pcm *pcm;
    struct snd_pcm_ops *ops;

    int err;

    /*
    사운드 카드와 마찬가지로 일부 매개 변수를 사용하는 snd_pcm_new라는 내장 ALSA함수 중 하나를 사용한다.
    첫 번째 매개변수는 sndc_card 구조형태의 PCM 장치에 대한 상위이다.
    두 번째와 세 번째 매개변수는 다시 ID이다. 하나는 문자열 형식이고 다른 하나는 정수이다.
    다음으로 재생 및 녹화 서브 스트림의 수를 나타내는 2개의 정수가 있다.
    현재는 단지 1개의 레코딩(녹화) 서브 스트림이다.
    마지막으로 모든 것이 잘 풀리면 할당된 PCM을 보유할 pcm 장치의 주소에 대한 포인터이다.
    */
    err = snd_pcm_new(soundgen_card->card, "Soundgen PCM", 0, 0, 1, &pcm);

    if (err < 0) {
        return err;
    }

    /*
    사운드 카드와 마찬가지로 일부 함수가 PCM레이어를 통해 호출되더라도 사운드 카드에 액세스 할 수 있기를 원하므로
    사운드 카드와 PCM 장치를 연결해야 한다.
    PCM 장치를 추적하기 위해 struct snd_pcm을 struct snd_card_soundgen 구조에 추가하였다.
    */
    pcm->private_data = soundgen_card;
    strcpy(pcm->name, "Soundgen PCM");
    soundgen_card->pcm = pcm;

    ops = &soundgen_pcm_ops;

    // 레코딩 서브 스트림에 대해 snd_pcm_ops를 설정한다.
    snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_CAPTURE, ops);

    /*
    마지막으로 PCM 데이터를 저장할 메모리가 필요하다. 전용 하드웨어 DMA 또는 메모리가 없으므로 커널이 나중에
    사용할 수 있는 일부 메모리를 미리 할당한다.
    snd_pcm_lib_preallocate_pages_for_all을 사용해 주어진 메모리 크기를 미리 할당하고 이 메모리 블록이 증가할 수 있는 크기로 제한한다.
    이 크기는 마지막 2개의 매개변수로 제어된다.
    따라서 이 경우 아직 메모리를 미리 할당하지 않았지만 버퍼가 64Kb만큼 거질 수 있는 것이다.
    */
    snd_pcm_lib_preallocate_pages_for_all(pcm, SNDRV_DMA_TYPE_CONTINUOUS,
        snd_dma_continuous_data(GFP_KERNEL), 0, (64*1024));
    return 0;
}

/*
<<플랫폼 디바이스 프로브>>
장치거가 검색될 때마다 사운드 카드를 등록한다.
*/
static int snd_soundgen_driver_probe(struct platform_device *devptr)
{
    int err;
    struct snd_card *card;
    struct snd_card_soundgen *soundgen;

    dev_info(&devptr->dev, "sound gen driver probed\n");
    /*
    먼저 새 사운드 카드를 만들고 첫번 째 매개 변수는 사운드 카운드에 연결된 장치이다.
    두 번째와 세 번째 매개변수는 id이며, 하나는 정수형식이고, 다른하나는 문자열이다.
    (만약 이 장치가 여러 장치를 지원한다면 이 값을 동적으로 반복해야 한다.)
    네 번째 매개변수는 사운드 카드를 모듈에 연결하는 데 사용한다.
    다섯 번째 매개변수는 snd_card 구조체 내부에 추가 private_date를 할당하는 데 사용하는 size이다.
    이 매개변수는 자체 구조체를 저장하는 데 사용할 수 있다.
    마지막으로는 할당된 snd_card 구조체를 반환하는 데 사용되는 snd_card의 주소에 대한 포인터이다.
    */
    err = snd_card_new(&devptr->dev, 0, NULL, THIS_MODULE, 
                    sizeof(struct snd_card_soundgen), &card);

    if (err < 0) {
        dev_err(&devptr->dev, "Failed to create new soundcard\n");
        return err;
    }

    soundgen = card->private_data;
    soundgen->card = card;
    
    /* 
    자체 구조체를 연결하며 사용자 공간에서 볼 수 있고 다른 카드를 구별하는 데 사용되는 이름을 설정한다.
    */
    strcpy(card->driver, "VoIP");
    strcpy(card->shortname, "VoIP");
    sprintf(card->longname, "VoIP with Linphone");
    
    /*
    ALSA 하위 시스템에 사운드 카드를 등록한다. 플랫폼 디바이스와 마찬가지로 snd_card_register 함수로 사용한다.
    */
    err = snd_card_register(card);
    if(err < 0) {
        dev_err(&devptr->dev, "Failed to register soundcard\n");
        goto error;
    }
    /*
    장치 인터페이스를 통해 데이터를 검색하거나 카드를 작성할 수 있는지 확인한다.
    장치의 drvdata를 설정하여 이를 수행한다.
    */
    platform_set_drvdata(devptr, card);
    return 0;

error:
    snd_card_free(card);
    return err;
}
/*
플랫폼 디바이스 삭제
카드를 제거하면 모든 데이터를 올바르게 정리해야 한다.
그렇지 않으면 모듈을 두 번 로드 할 수 없거나 시간이 진마에 따라 손상이 발생할 수 있다.
=> 여기서 우리는 drvdata를 설정하는 것이 왜 중요한지 알 수 있다.
remove함수는 드라이버 인터페이스를 통해 호출되지만 사운드 카드를 정리해야 한다.
*/
static int snd_soundgen_driver_remove(struct platform_device *devptr)
{
    dev_info(&devptr->dev, "Sound gen driver removed\n");
    snd_card_free(platform_get_drvdata(devptr));
    return 0;
}

/*
플랫폼 드라이버와 드라이버에 등록할 장치와 정의 프로브 및 제거
*/
#define SND_SOUNDGEN_DRIVER "snd_soundgen"
static struct platform_driver snd_soundgen_driver = {
    .probe = snd_soundgen_driver_probe,
    .remove = snd_soundgen_driver_remove,
    .driver = {
        .name = SND_SOUNDGEN_DRIVER,
    },
};

/*
1. 플랫폼 드라이버와 그 드라이버에에 등록할 디바이스 정의
2. 플랫폼 드라이버 정의 프로브 및 제거.
*/
static int __init alsa_soundgen_card_init(void)
{
    int err;

    err = platform_driver_register(&snd_soundgen_driver);
    if(err < 0) {
        pr_err("Failed to register platform driver\n");
        return err;
    }

    pr_info("Sound Generator platform device registered\n");
    struct platform_device *device;
    device = platform_device_register_simple(SND_SOUNDGEN_DRIVER, 0, NULL, 0);

    if(IS_ERR(device)) {
        pr_err("Failed to register platform device\n");
        platform_driver_unregister(&snd_soundgen_driver);
        return PTR_ERR(device);
    }

    return 0;
}

static void __exit alsa_soundgen_card_exit(void)
{
    platform_driver_unregister(&snd_soundgen_driver);
}

module_init(alsa_soundgen_card_init);
module_exit(alsa_soundgen_card_exit);
```

# **28. 커널 프로그래밍에서 쉘 명령을 실행하는 방법**

유저레벨에서 커널에 있는 API 를 동작시키는 인터페이스는 시스템 콜로써 항상 사용되는 것이다.

하지만 커널 레벨에서 사용자 공간의 어플리케이션을 실행하는 것은 그다지 알려져 있지 않다.



하지만 가만히 생각해 보면 핫플러그인과 같이 USB 가 꼽히면 자동으로 마운트되는 것을 우리는 알고 있다.

과연 어플리케이션에서 주기적으로 USB 꼽힌 것을 체크해서 마운트 하는 것일까?

그렇지 않다.

커널 레벨에서 USB가 인식되면 종류에 따라 마운트를 시켜주는 함수가 실행되게 된다.

혹은 모듈이 필요하면 모듈까지도 로딩을 시켜준다.

이런 인터페이스가 가능한 커널 함수를 소개한다.



그 인터페이스의 이름은 `usermodehelper` API 이다.

그 API를 한번 살펴보면

```bash
# usermod 헬퍼 API 핵심함수
call_usermodehelper_setup		사용자 지역 호출을 위한 핸들러 준비	
call_usermodehelper_setkeys		헬퍼의 세션 키 설정
call_usermodehelper_setcleanup	헬퍼의 정리 함수 설정
call_usermodehelper_stdinpipe		헬퍼의 stdin 파이프 작성
call_usermodehelper_exec		사용자 지역 호출 호출
```

자료구조를 살펴보면(`kmod.h`)

![image-20201109180023689](https://user-images.githubusercontent.com/58545240/98521330-9d193800-22b6-11eb-8940-fdebfb410497.png)

의외로 아주 간단한 구조를 가지고 있다.

이제 사용자 모드 헬퍼 API의 내부 구조를 살펴보면

`kernel_execve` 를 사용하여 커널 공간 커널모듈 로더로 사용하는 구현을 한다.

`kernel_execve` 는 부팅시 init 프로세스를 시작하는데 사용되는 함수이며 사용자 모드 헬퍼 API는 사용하지 않는다.

**`#include <linux/kmod.h>` 추가 -> `call_usermodhelper()` 함수 사용가능!!**

=> *`call_usermodhelper()` : 커널에서 유저레벨 응용프로그램 실행하게 해준다.*

```bash
# call_usermodehelper()
- 디바이스 드라이버(커널)에서 응용 프로그램을 수행할 수 있도록 지원하는 함수
- static inline int call_usermodehelper(char *path, char **argv, char **envp, int wait);
- Argument :
    . path : 수행하고자하는 응용 프로그램을 지정
    . argv : 응용 프로그램의 인자를 지정
    . envp : 응용 프로그램의 동작 환경 변수를 설정
    . wait : call_usermodehelper() 함수를 종료할 때 응용 프로그램의 수행 결과를 기다릴 필요가 있는지를 설정
- Return value :
    . 함수가 성공적으로 수행되었다면 0을 반환.
    . 응용 프로그램을 발견하지 못했거나 응용 프로그램이 0이외의 값을 반환하면 0이외의 값 반환.
- 기타
    . call_usermodehelper() 함수에 의해 수행될 때 출력 함수 printf()로 출력된는 값은 메인 콘솔로 나타나므로 주의 바람.
    . keventd 의 자식프로세스로 수행하고, root 권한을 가진채로 수행된다.
    . keventd 는 자식프로세스가 exit 할때 결과를 받는다.
    . 반드시 프로세스 context 에서 호출되어야 하고, 성공시 0, 실패는 - 에러코드를 리턴한다.
```

# **29. 파일 입출력**

---

**유닉스 시스템**에서는 거의 모든 것을 파일로 표현하므로 '**파일 입출력**'은 매우 중요한 부분입니다. 알다시피 파일은 '**읽기**'나 '**쓰기**' 전에 반드시 '**열기(open)**'를 해야합니다. 그리고 커널은 '**파일 테이블**'이라고 하는 프로세스별로 열린 파일 목록을 관리합니다. 각 프로세스에는 기본적으로 0, 1, 2 값을 가지는 파일 디스크립터가 open되어 있습니다. 

**0** - 표준 입력(**stdin**)

**1** - 표준 출력(**stdout**)

**2** - 표준 에러(**stderr**)

위의 3가지 파일 디스크립터를 직접 참조하는대신 C 라이브러리는 **#define 매크로 정의**를 제공합니다.

따라서, 해당 파일 디스크립터를 사용하고싶다면 아래 정의된 **매크로로 참조**하는게 좋습니다.

```c
#define STDIN_FILENO 0
#define STDOUT_FILENO 1
#define STDERR_FILENO 2
```

들어가기에 앞서, '**리눅스 시스템 프로그래밍**'이란 것은 '**리눅스(커널)에서 제공하는 API(이하 시스템콜)'**를 사용하여 하드웨어와 직접적으로 연결되는 프로그램을 작성하는 것을 말합니다. 하지만 **시스템콜은 아주 Low한 레벨의 동작 방식을 정의**해놓은 것이기에 이것만 가지고 작업(프로그래밍)을 하기엔 아주 곤란합니다. **작업량이 상당**해지기 때문입니다. 그래서 이러한 시스템콜을 적절히 '**래핑(Wrapping)**'해서 만들어놓은 것이 그 유명한 '**glibc**' 라이브러리입니다. 앞으로 함수들을 소개할 때 2가지를 구분하겠습니다.

1. **시스템콜**(System Call_줄여서 syscall)
2. **glibc 라이브러리**(GNU C Library)

리눅스에서 파일에 접근하는 가장 기본(원론)적인 방법은 **open(), read(), write() 시스템콜**입니다.

이 4가지를 세세하게 알아보겠습니다.

## 29.1 open() 시스템 콜

```c
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>

int open(const char* name, int flags);
int open(const char* name, int flags, mode_t mode);
```

첫 번째 매개변수는 'open'할 **파일의 경로이름**입니다. 여기서 경로는 '**절대경로**', '**상대경로**' 모두 가능합니다.

두 번째 매개변수는 해당 파일을 '**읽기**' 위해 열지 '**쓰기**' 위해 열지 아님 '**읽기쓰기**' 둘다 할 것인지를 표기합니다. 각각 '**O_RDONLY**', '**O_WRONLY**', '**O_RDWR**'로 나타냅니다. 여기서 간단히 끝나면 좋겠지만, 아까 얘기했듯이 '시스템콜'은 아주 Low한 레벨의 동작 방식을 정의합니다. 이 말은, **아주 디테일한 동작 하나하나를 설정하여 커널에 알려줘야한다**는 얘기입니다. 이어서, 두 번째 매개변수 flags에는 '**비트 OR 연산(|)**'을 통해 **열기 동작의 플래그를 설정**할 수 있습니다.

설정할 수 있는 **플래그** 값들을 살펴보겠습니다.

1. **O_APPEND**: 덧붙이기 모드(Append Mode)로 파일을 오픈합니다.
2. **O_ASYNC**: 비동기적으로 특정 파일에서 '읽기'나 '쓰기'가 가능해질 때 '시그널'이 발생합니다.(터미널과 소켓에서만 이 옵션을 사용할 수 있습니다.)
3. **O_SYNC**: 파일을 '동기식 입출력'으로 오픈합니다. 데이터를 물리적으로 디스크에 '쓰기'전까지는 쓰기 연산이 완료되 지 않습니다.
4. **O_CREAT**: 첫 번째 매개변수 name에 적은 파일이 없으면 새로 생성합니다. 파일이 이미 있다면 'O_EXCL' 플래그를  추가하지 않는 이상 아무런 일도하지 않습니다.
5. **O_DIRECT**: '직접 입출력'을 수행하기 위해 파일을 오픈합니다.
6. **O_DIRECTORY**: 첫 번째 매개변수 name이 디렉터리가 아니면 open() 호출이 실패합니다.
7. **O_EXCL**: 'O_CREAT'와 함께 이 플래그를 사용하면 첫 번째 매개변수 name으로 지정한 파일이 이미 있을 때 open()  호출이 실패합니다. 이것은 파일 생성 과정에서 '경쟁 상태'를 회피하기 위해 사용됩니다.
8. **O_LARGEFILE**: 2GB를 초과하는 파일을 오픈하기 위해 64-bit 오프셋을 사용합니다.
9. **O_NOATIME+**: '읽기'에 의해서 파일의 마지막 접근 시간이 갱신되지 않도록 합니다.
10. **O_NOCTTY**: 첫 번째 매개변수 name이 '터미널 디바이스(/dev/tty)'라면 프로세스에 현재 제어 중인 터미널이 없더  라도 프로세스의 제어 터미널이 되지 않습니다.
11. **O_NOFOLLOW**: 첫 번째 매개변수 name이 '심벌릭 링크'라면 open() 호출이 실패합니다.
12. **O_NONBLOCK**: 파일을 논블록킹(Non-Blocking) 모드로 오픈합니다.
13. **O_CLOEXEC**: 열린 파일에 close-on-exec 플래그를 설정합니다. 새 프로세스를 실행하면 이 파일은 자동으로 닫힙니 다.
14. **O_TRUNC**: 파일이 존재하고, 일반 파일이며 flags 매개변수에 '쓰기'가 가능하도록 명시되어 있다면 파일 길이를 0  으로 잘라버립니다.



2번째 flags의 설정으로 파일이 존재하지않을 때 새로 생성한다면 생각해볼 3가지 문제가 있습니다.

1. 해당 파일의 '**uid**'는? 프로세스의 '**euid(유효 uid)**'가 됩니다.
2. 해당 파일의 '**guid**'는? 기본 동작은 파일을 생성한 프로세스의 '**egid(유효 gid)**'로 설정합니다.
3. 해당 파일의 '**접근 권한**'은? 3번째 매개변수를 통해 설정합니다.

'**접근 권한**' 설정을 위한 3번째 매개변수를 살펴보겠습니다.

'**0644**'와 같은 '**접근 권한**' 설정은 아주 익숙하리라 생각됩니다. 하지만, 이렇게 단순히 표기한다면 쉽고 가독성도 좋겠지만, 1가지 문제가 있습니다. **타 유닉스 시스템으로의 '이식성'을 보장받을 수 없다**는 것입니다. 이를 위해 '**POSIX**'에서는 유닉스 시스템간의 호환이 가능하도록 '**매크로 상수**'를 제공합니다.

 **S_IRWXU**: **소유자**에게 읽기, 쓰기, 실행 권한이 있다.

 **S_IRUSR**: 소유자에게 읽기 권한이 있다.

 **S_IWUSR**: 소유자에게 쓰기 권한이 있다.

 **S_IXUSR**: 소유자에게 실행 권한이 있다.

 **S_IRWXG**: **그룹**에게 읽기, 쓰기, 실행 권한이 있다.

 **S_IRGRP**: 그룹에게 읽기 권한이 있다.

 **S_IWGRP**: 그룹에게 쓰기 권한이 있다.

 **S_IXGRP**: 그룹에게 실행 권한이 있다.

 **S_IRWXO**: **그 외 모든 사용자**에게 읽기, 쓰기, 실행 권한이 있다.

 **S_IROTH**: 그 외 모든 사용자에게 읽기 권한이 있다.

 **S_IWOTH**: 그 외 모든 사용자에게 쓰기 권한이 있다.

 **S_IXOTH**: 그 외 모든 사용자에게 실행 권한이 있다.

## 29.2 read() 시스템 콜

```c
#include <unistd.h>

ssize_t read(int fd, void* buf, size_t len);
```

'**`fd(파일 디스크립터)`**'가 참조하는 파일의 현재 '파일 오프셋'에서 len 바이트만큼 buf로 읽어 들입니다. 만약 **읽어들일 데이터가 없는데 read()를 호출**한다면 프로그램은 '**블럭(Block)**'됩니다.

**호출 성공**: buf에 쓴 바이트 숫자를 반환합니다.

**호출 실패**: -1을 반환하며 '**errno**'를 설정합니다.

여담으로 '파일 디스크립터'를 매개변수로 받는 시스템콜은 대부분 1번째 매개변수에 위치해있습니다.

 전체 바이트 읽기 예제입니다.

```c
ssize_t ret;
 
while (len != 0 && (ret = read(fd, buf, len)) != 0) {
    if (ret == -1) {
        if (errno == EINTR)
            continue;
        perror("read");
        break;
    }
    
    len -= ret;
    buf += ret;
}
```

## 29.3 write() 시스템 콜

```c
#include <unistd.h>
 
ssize_t write(int fd, const void* buf, size_t count);
```

count 바이트만큼 fd가 참조하는 파일의 현재 위치에 시작 지점이 buf인 내용을 기록합니다.

**호출 성공**: 쓰기에 성공한 바이트 수를 반환합니다.

**호출 실패**: -1을 반환하며 '**errno**'를 적절한 값으로 설정합니다.

'**일반 파일**'을 대상으로 쓰기 작업을 수행할 경우에는 루프 내에서 돌릴 필요가 없습니다. 하지만 **다른 파일 유형(소켓, 파이프 등)**을 대상으로 쓰기 작업을 수행할 경우에는 요청한 모든 바이트를 정확히 썼는지 보장하기 위해 루프가 필요합니다. 또한, 루프를 사용함으로써 숨어 있던 에러 코드를 얻을 수도 있습니다.

```c
ssize_t ret, nr;
 
while(len != 0 && (ret = write(fd, buf, len)) != 0) {
    if (ret == -1) {
        if (errno == EINTR)
            continue;
        perror("write");
        break;
    }
    
    len -= ret;
    buf += ret;
}
```

# **30. 다중 입출력** - select()

---

'**다중 입출력**'은 프로그램(**단일 스레드**)에서 **여러 개의 파일을 작업**하고자 할 때 사용할 수 있는 **메커니즘**입니다. 사실 '단일 스레드'에서 여러 개의 파일을 작업하고자 할 때 사용할 수 있는 다른 방법으로는 '**논블록(Non-Block) 입출력**'을 사용하는 방법도 있긴 합니다만, '**논블록(Non-Block) 입출력**'은 **프로그래밍 작업이 까다롭습니다**. 이제부터 설명할 '**select**'는 '**블록/비동기적 입출력**'에서의 '**다중 입출력**' 모델입니다.

 

A, B, C, D 4개의 파일을 다루는 작업을 하고싶다고 가정해볼때 다음과 같은 시나리오를 생각해볼 수 있습니다.

1. A 파일에 대한 작업을 합니다.
2. A 파일에 대한 작업을 마칩니다.
3. B 파일에 대한 작업을 합니다.
4. B 파일에 대한 작업을 하다가 프로그램이 '**블록(Block)**' 상태에 빠집니다.

 

파일에 대한 작업 중 '**블록(Block)**' 상태가 되는 이유야 다양하겠지만 대표적으로 **read() 함수를 호출했는데 읽을 파일이 없다면** 프로그램은 읽을 내용이 생길 때까지 '**블록(Block)**'됩니다. 그럼 위의 상황에서 '블록' 없이 어떻게 4개의 파일에 대한 작업을 정상적으로 실행할 수 있을까요? 답은 간단합니다. A, B, C, D 파일을 **순서대로 작업할게 아니라 작업할 준비가된** 파일에 대해서만 작업을 하면됩니다. 이러한 생각이 시초가 되어 최초로 탄생한 함수가 '**select**' 시스템콜입니다.

```c
#include <sys/select.h>
 
int select(int n,
    fd_set* readfds,
    fd_set* writefds,
    fd_set* exceptfds,
    struct timeval* timeout);
    
FD_CLR(int fd, fd_set* set);
FD_ISSET(int fd, fd_set* set);
FD_SET(int fd, fd_set* set);
FD_ZERO(fd_set* set);
```

 

**`select()`**는 해당 파일 디스크립터가 입출력을 수행할 준비가 되거나 마지막 매개변수인 **`timeout` 변수**에 정해진 시간이 경과할 때까지만 '**블록**'됩니다. '**감시 대상 파일 디스크립터**'는 3가지 집합으로 나뉘어 각각 다른 '이벤트'를 기다립니다.

**`readfds`**: '읽기'가 가능한지 감시합니다. (블록되지 않고 read() 작업이 가능한지)

**`writefds`**: '쓰기'가 가능한지 감시합니다. (블록되지 않고 write() 작업이 가능한지)

**`exceptfds`**: 예외가 발생했거나 대역을 넘어서는 데이터(소켓)가 존재하는지 감시합니다.

 

만약 select() 함수의 매개변수로 **NULL**을 넘기면 해당 이벤트는 감시하지 않습니다. select() 시스템콜의 예제를 살펴보겠습니다.

```c
#include <stdio.h>
#include <sys/time.h>
#include <sys/types.h>
#include <unistd.h>
 
#define TIMEOUT 5
#define BUF_LEN 1024
 
int main() {
    struct timeval tv;
    fd_set readfds;
    int ret;
    
    // 표준 입력에서 입력을 기다리기 위한 준비를 합니다.
    FD_ZERO(&readfds);
    FD_SET(STDIN_FILENO, &readfds);
    
    // select가 5초 동안 기다리도록 timeval 구조체를 설정합니다.
    tv.tv_sec = TIMEOUT;
    tv.tv_usec = 0;
    
    // select() 시스템콜을 이용해 입력을 기다립니다.
    ret = select(STDIN_FILENO + 1, &readfds, NULL, NULL, &tv);
    
    if (ret == -1) {
        perror("select");
        return 1;
    }
    else if (!ret){
        printf("%d seconds elapsed.\n", TIMEOUT);
        return 0;
    }
    
    // select() 시스템콜이 양수를 반환했다면 '블록(block)'없이 즉시 읽기가 가능합니다.
    if (FD_ISSET(STDIN_FILENO, &readfds)) {
        char buf[BUF_LEN + 1];
        int len;
        
        // '블록(block)'없이 읽기가 가능합니다.
        len = read(STDIN_FILENO, buf, BUF_LEN);
        if (len == -1) return 1;
        if (len) {
            buf[len] = '\0';
            printf("read: %s\n", buf);
        }
        
        return 0;
    }
}
```

 # **31. 다중 입출력 - poll()**

---

**`poll()` 시스템콜**은 유닉스 운영체제의 최초 상용화 버전 중 하나인 **'`Unix System V`'에서 제공하는 다중 입출력 방식**이었습니다. 리눅스에서 제공하고 있던 **`select()` 시스템콜보다 더 좋았던 까닭에** **리눅스에서도 `poll()` 시스템콜을 도입**하였습니다. select() 시스템콜의 단점을 보완한 poll() 시스템콜이지만, 기존에 select()로 다중 입출력을 구현했던 개발자의 습관과 타 시스템으로의 이식성을 이유로 덜 사용된다고 합니다.

```c
#include <poll.h>
 
int poll(struct pollfd* fds, nfds_t nfds, int timeout);
 
struct pollfd {
    int fd;         // 파일 디스크립터
    short events;   // 감시할 이벤트
    short revents;  // 발생한 이벤트
}
```

**`struct pollfd\* fds`**: 감시하고자 하는 파일 디스크립터와 이벤트를 설정한 후 넘겨줍니다.

**`nfds_t nfds`**: 그럼 첫 번째 매개변수로 설정한 구조체 1개만 넘겨줄 수 있느냐? 그렇지 않습니다. poll() 시스템콜은 단일 pollfd 구조체 배열을 nfds 개수만큼 사용합니다. 즉 몇 개의 구조체 배열을 넘겼는지 알려줍니다.

**`int timeout`**: 해당 시간만큼 poll() 시스템콜은 작동합니다.

`select()` 시스템콜은 '읽기', '쓰기', '예외' 3가지를 독립적으로 설정한 후, 매개변수로 넘겨줘야했습니다. 그에 반면, poll() 시스템콜은 **구조체 배열**을 사용함으로써 설계상으로도 탐색 시간상으로도 훨씬 더 좋아졌다는걸 알 수 있습니다. pollfd 구조체 멤버를 보면 **감시할 이벤트를 설정할 수 있는 events 필드**가 있습니다. 이 필드를 설정한 후 poll() 시스템콜을 작동시키면 **등록한 이벤트 중 발생한 이벤트가 revents 필드에 설정**됩니다. **revents 필드는 등록한 이벤트 중 발생된 이벤트 정보를 커널이 설정**해주므로 사용자는 해당 필드를 통해 확인할 수 있습니다. 그럼 events 필드에 등록할 수 있는 이벤트는 어떤 것들이 있을지 살펴보겠습니다.

- POLLIN - 읽을 데이터가 존재한다. 즉, 읽기가 블록(blokc)되지 않는다.
- POLLRDNORM - 일반 데이터를 읽을 수 있다.
- POLLRDBAND - 우선권이 있는 데이터를 읽을 수 있다.
- POLLPRI - 시급히 읽을 데이터가 존재한다.
- POLLOUT - 쓰기가 블록(block)되지 않는다.
- POLLWRNORM - 일반 데이터 쓰기가 블록(block)되지 않는다.
- POLLWRBAND - 우선권이 있는 데이터 쓰기가 블록(block)되지 않는다.
- POLLMSG - SIGPOLL 메시지가 사용 가능하다.

 

그리고 revents 필드에는 다음 이벤트가 설정될 수 있습니다.

- POLLER - 주어진 파일 디스크립터에 에러가 있다.
- POLLHUP - 주어진 파일 디스크립터에서 이벤트가 지체되고 있다.
- POLLNVAL - 주어진 파일 디스크립터가 유효하지 않다.

```c
#include <stdio.h>
#include <unistd.h>
#include <poll.h>
 
#define TIMEOUT 5
 
int main() {
    struct pollfd fds[2];
    int ret;
    
    // 표준 입력에 대한 이벤트를 감시하기 위한 준비를 합니다.
    fds[0].fd = STDIN_FILENO;
    fds[0].events = POLLIN;
    
    // 표준 출력에 쓰기가 가능한지 감시하기 위한 준비를 합니다.
    fds[1].fd = STDOUT_FILENO;
    fds[1].events = POLLOUT;
    
    // 위에서 pollfd 구조체 설정을 모두 마쳤으니 poll() 시스템콜을 작동시킵니다.
    ret = poll(fds, 2, TIMEOUT * 1000);
    
    if (ret == -1) {
        perror("poll");
        return 1;
    }
    
    if (!ret) {
        printf("%d seconds elapsed.\n", TIMEOUT);
        return 0;
    }
    
    if (fds[0].revents & POLLIN) printf("stdin is readable\n");
    if (fds[1].revents & POLLOUT) printf("stdout is writeable\n");
    
    return 0;
}
```

# **32. 가상 파일 시스템**

---

프로그래밍을 하다보면 빠지지 않는 것이 파일 처리인데요, 혹시 파일 작업을 하기 위해 read(), write()와 같은 시스템콜을 사용하면서 **"왜 매개변수로 어떤 파일 시스템인지 넘겨주지않지?"**라고 생각해보신적 있으신가요? 사실 조금만 생각해보면 답이 금방나오는 질문입니다. 이유는 **파일 시스템이 추상화**되어있기 때문입니다. 구조를 표현하면 다음 그림과 같습니다.

![image-20201110103737349](https://user-images.githubusercontent.com/58545240/98646302-8c2ffb80-2376-11eb-8d64-57b17805a657.png)

파일 시스템간의 **공통된 인터페이스**를 둠으로써 얻는 장점은 여러 가지가 있습니다.

1. 동일한 방법으로 접근이 가능하다.
2. 파일 시스템간의 이식성이 좋아진다.
3. 새로운 파일 시스템이 추가되더라도 설계를 변경할 필요가 없다.

# **33. 버퍼 입출력 - 표준 입출력 라이브러리**

---

파일 시스템의 최소 저장 단위는 '**블록**'이라는 추상 개념입니다. 따라서, **모든 입출력 연산은 블록 크기의 정수배**에 맞춰서 일어납니다. 단지 1Byte를 읽고싶다하더라도, 512Byte만큼(1블록 = 512Byte라 가정) 읽어와야한다는 얘기입니다. 또는 내가 단지 2.5블록만큼(대략 1250Byte) '쓰기' 연산을 하고싶다하더라도 3블록에 대해 '쓰기' 연산을 해야한다는 얘기입니다.

![image-20201110103841985](https://user-images.githubusercontent.com/58545240/98646327-93efa000-2376-11eb-9a42-e0b43fd7eedb.png)

그런데 잘 생각해보면 사용자 애플리케이션에서는 512Byte 단위로 입출력 연산이 이루어지지 않는 경우가 대부분입니다. 보통 **CSV 파일을 다루기 위해 '필드' 단위의 입출력 연산**이 필요하다거나, **JSON 파일을 다루기 위해 단순히 '문자열' 단위의 입출력 연산**이 필요한 경우가 대부분입니다. 그렇기 때문에 **사용자 애플리케이션 코드 레벨에서 인위적으로 버퍼링을 구현하여 사용해야합니다.** 그렇다면 항상 사용자 버퍼링을 구현해서 사용해야되냐? 직접 구현하는 것도 좋은 방법이지만 그것보다 더 좋은 해법은 견고하고 뛰어난 사용자 버퍼링 구현체를 가져다 사용하는 것입니다. 가장 인기있고 대중적인 구현체가 바로 '**표준 입출력 라이브러리(stdio)**'와 '**표준 C++ iostream**'입니다.

## 표준입출력 라이브러리

> 표준 입출력 라이브러리는 **플랫폼 독립적인 사용자 버퍼링 해법**을 제공합니다. 이는 **사용하기 쉬우면서도 강력**합니다. 당연한 말이겠지만 애플리케이션이 표준 입출력 라이브러리를 사용할지 시스템콜을 통해 더 저수준으로 작업할지는 애플리케이션의 요구사항과 동작 방식에 따른 개발자의 선택입니다. 먼저 표준 입출력 라이브러리에서 사용되는 용어를 짚고 넘어가겠습니다. 
>
> - **파일 포인터**: 표준 입출력은 파일 디스크립터를 직접 다루지 않고 '파일 포인터'라는 개념을 통해 파일에 접근합니다. '파일 포인터'는 파일 디스크립터를 래핑한 개념입니다.
> - **스트림(stream)**: 표준 입출력에선 열린 파일을 '스트림(stream)'이라고 부릅니다. '읽기' 모드로 파일이 열렸다면 '**입력 스트림**', '쓰기' 모드로 파일이 열렸다면 '**출력 스트림**', '읽기/쓰기' 모드로 파일이 열렸다면 '**입출력 스트림**'입니다.

표준 입출력 라이브러리의 인터페이스를 살펴보겠습니다

```c
#include <stdio.h>
 
FILE* fopen(const char* path, const char* mode);
```

fopen() 함수는 path의 파일을 mode에 따라 원하는 용도로 새로운 스트림을 생성합니다. mode 매개변수로 넘길 수 있는 값은 다음과 같습니다.

- **`r`**   '읽기' 모드로 파일을 오픈합니다. 즉, '읽기 스트림'을 생성합니다.
- **`r+`**  '읽기/쓰기' 모드로 파일을 오픈합니다. 
- **`w`**  '쓰기' 모드로 파일을 오픈합니다. 파일이 이미 존재하면 길이를 0으로 잘라버립니다. 파일이 없다면 새로 생성합니다.
- **`w+`** '읽기/쓰기' 모드로 파일을 오픈합니다. 파일이 이미 존재하면 길이를 0으로 잘라버립니다. 파일이 없다면 새로 생성합니다. 
- **`a`**  '쓰기' 모드로 파일을 오픈합니다. 파일이 이미 존재한다면 맨 끝에서부터 append합니다. 파일이 없다면 새로 생성합니다.
- **`a+`** '읽기 쓰기' 모드로 파일을 오픈합니다. 파일이 이미 존재한다면 맨 끝에서부터 append합니다. 파일이 없다면 새로 생성합니다.

 파일의 스트림을 닫고 싶다면 `fclose()` 함수를 사용합니다.

```c
#include <stdio.h>
 
int fclose(FILE* stream);
```

 만약, 하나의 스트림이 아닌 현재 프로세스의 모든 스트림을 닫고 싶다면 `fcloseall()` 함수를 사용합니다. 리눅스에서만 사용이 가능합니다.

```c
#define _GNU_SOURCE
#include <stdio.h>
 
int fcloseall(void);
```

 **표준 C 라이브러리**는 일반적인 형태부터 흔히 접하기 어려운 형태에 이르기까지 **열린 스트림에서 데이터를 읽기 위한 다양한 함수를 구현**하고 있습니다. 가장 많이 사용되는 3가지 함수를 살펴보겠습니다.

```c
#include <stdio.h>
 
int fgetc(FILE* stream);
char* fgets(char* str, int size, FILE* stream);
size_t fread(void* buf, size_t size, size_t nr, FILE* stream);
```

- **`fgetc()`**: 스트림에서 문자 하나를 읽어들입니다.
- **`fgets()`**: 스트림에서 하나의 문자열을 읽어들여 str에 저장합니다. 즉, 문자를 읽어들이다가 EOF나 개행문자('\n')를 만나면 읽기를 종료합니다. 이 함수의 특징 중 하나가 개행문자를 만나 읽기가 종료되었다면 str의 마지막에 개행문자를 추가로 저장한다는 것입니다. 그리고 스트림의 위치가 개행문자 다음으로 가기 때문에 다시 호출되면 다음 문자열을 읽어들일 수 있습니다.
- **`fread()`**: size 크기의 '블록'을 nr개 읽어들여 buf에 저장합니다. 구조체 단위로 데이터를 읽어들이고 싶다면 이 함수를 사용하면 됩니다.

여담으로 표준 입출력 함수를 살펴보면 공통된 특징이 있습니다. 스트림을 넘겨주는 매개변수는 가장 마지막에 위치하는 것인데요, 리눅스 시스템콜이 파일 디스크립터를 넘겨주는 매개변수는 항상 첫 번째에 위치해 있는 것과 반대다는걸 알 수 있습니다.

 위의 읽기 함수와 대응되는 '쓰기' 함수는 다음과 같습니다.

```c
#include <stdio.h>
 
int fputc(int c, FILE* stream);
int fputs(const char* str, FILE* stream);
size_t fwrite(void* buf, size_t size, size_t nr, FILE* stream);
```

 함수의 사용법은 '읽기' 함수와 비슷하니 바로 예제를 살펴보겠습니다.

```c
#include <stdio.h>
 
int main() {
    FILE* in;
    FILE* out;
    struct meminfo {
        char name[20];
        uint8_t age;
        bool sex;
    } tmp, bob = { "Hong Gil Dong", 100, 1 };
    
    out = fopen("data", "w");
    if (!out) {
        perror("fopen");
        return 1;
    }
    
    if (!fwrite(&bob, sizeof(struct meminfo), 1, out)) {
        perror("fwrite");
        return 1;
    }
    
    if (fclose(out)) {
        perror("fclose");
        return 1;
    }
    
    in = fopen("data", "r");
    if (!in) {
        perror("fopen");
        return 1;
    }
    
    if (!fread(&tmp, sizeof(struct meminfo), 1, in)) {
        perror("fread");
        return 1;
    }
    
    if (fclose(in)) {
        perror("fclose");
        return 1;
    }
    
    printf("name: %s  age: %d  sex: %d", tmp.name, tmp.age, tmp.sex);
    
    return 0;
}
```

 

표준 입출력에서 스트림의 위치를 조작하고 싶다면 `fseek()` 함수를 사용합니다. 스트림의 위치는 메모장에서 커서의 위치같은 개념입니다.

```c
#include <stdio.h>
 
int fseek(FILE* stream, long offset, int whence);
```

whence는 스트림의 기준 위치를 정하는 매개변수로 3가지 값이 올 수 있습니다.

1. `SEEK_SET`: 맨 처음을 기준으로 offset 값을 더해 스트림의 위치를 결정합니다.
2. `SEEK_CUR`: 현재 커서의 위치를 기준으로 offset 값을 더해 스트림의 위치를 결정합니다.
3. `SEEK_END`: 맨 끝을 기준으로 offset 값을 더해 스트림의 위치를 결정합니다.

 

현재 스트림의 위치를 확인하고 싶다면 `ftell()` 함수를 사용합니다.

```c
#include <stdio.h>
 
long ftell(FILE* stream);
```

# **34. 스트림(Stream)이란?**

---

> 우리는 프로그램을 실행할 때 키보드를 통해서 입력을하고 모니터를 통해서 출력을 하고 있죠?
>
> 우리는 별다른 무리없이 간단하게 입출력을 합니다. 생각을 해보면 키보드에서 입력을 받고, 하드웨어적인 부분을 처리하여야 입력이 되고 화면으로 출력이 되겠죠?
>
> 예로 A라는 문자를 눌렀다고 가정해봅시다. 그러면 키보드에서 신호가 나갈 것이고, 그것을 컴퓨터에서 처리하여, 출력장치인 모니터로 신호를 보내 결과적으로 화면에 A라는 문자가 찍힐 것입니다.
>
> 그러나, 우리는 이렇게 하드웨어적인 세세한 부분까지 신경을 쓰지 않습니다. **입출력을 도와주는 `스트림(Stream)`이란 녀석이 있기 때문이죠**
>
> 스트림을 사용하는 입출력 장치(Input/Output Device)는 많습니다. 키보드나 모니터 뿐만 아니라, 프린터, 마우스, 네트워크, 메모리 등등 많은 입출력 장치에서 쓰인다. 물론 스트림의 종류도 입출력 장치의 종류만큼 다양하다. 
>
> 파일에 데이터를 쓰고 싶다고 한다면 파일관련 스트림을 찾아서 목표지점으로 삼을 것이다. 목표지점이란 입력스트림의 경우 데이터가 위치하는 곳을 말하고, 출력 스트림의 경우 데이터가 저장될 곳을 말한다.
>
> 입출력 장치에 데이터를 입력하거나 출력하는 처리를 하기 위해 스트림이 존재하는데, 스트림은 방향에 따라 2가지로 나눌 수 있다.
>
> => **사용자 입장에서 데이터가 들어오면 `입력 스트림(Input Stream)`이고, 데이터가 나가면 `출력 스트림(Output Stream)`이다.**
>
> - `입력 스트림`은 데이터를 스트림으로 읽어 들이고, 스트림에 존재하는 데이터를 하나하나 읽는다.
> - 출력 스트림은 출력된 데이터를 스트림으로 보내고 스트림에 있는 데이터를 비워 버린다. 비워진 데이터는 모두 목표지점에 저장이 되는 것이다.
>
> **결국 스트림은 데이터의 입출력 처리의 중간자 역할**을 한다. 데이터의 목표지점은 네트워크건 메모리건 프린터건 중요하지 않는다.
>
> *그저 해당되는 데이터를 스트림으로 읽어 들이거나 스트림으로 내보내면 끝인것이다. 하드웨어적인 복잡한 작업은 스트림이 알아서 해주고 사용자는 편하게 사용만 하면 되는 것!!*

1. **스트림(Stream) 이란?**

   - 일반적으로 데이터, 패킷, 비트 등의 **일련의 연속성을 갖는 흐름**을 의미

     - 음성, 영상, 데이터 등의 **작은 조각들이 하나의 줄기를 이루며 전송되는 데이터 열**

     *호스트 사옿간 또는 동일 호스트 내 프로세스 상호간 통신에서 큐에 의한 메세지 전달방식을 이용한 가상 연결통로를 의미하기도 한다*

2. **멀티미디어 스트리밍 기술**

   - 전송 방식 구분

     - 다운로드 방식 : 재생(Play) 전에 데이터를 완전히 다운로드 받는 방식

       *(웹 서버)/HTTP/TCP/IP 상에서 동작*

     - 스트리밍 방식 : 데이터 수신과 동시에 재생, 사용자에게 동시성 제공. 실시간 정손기

       *(미디어 서버)/RTP/UDP/IP 상에서 동작*

   - 스트리밍 종류

     - `RTP 스트림` : 인터넷 스트리밍 프로토콜

       - `RTP/UDP/IP` 패킷에 실릴 수 있도록 한 스트리밍 프로토콜
       - 오디오, 비디오 등의 정보를 담은 일련의 패킷화된 정보 앞에 `RTP` 헤더를 붙힌 스트림

     - `MPEG 스트림` : 주로 디지털 방송용 스트리밍 프로토콜

       - `MPEG 다중화` 방식에 의해 **패킷** 스트림화 된것

         *인터넷 상에서는 `MPEG 다중화` 스트림이 `RTP` 스트림 위에 또한 실릴 수 있음

       - `PS, TS, ES, PES` 등

   - 스트리밍 관련 프로토콜

     - 인터넷 실시간 미디어 흐름 및 제어 관련 프로토콜
       - 물리정 정보의 빠른 흐름 및 제어 : RTP(빠른 수송) / RTSP(수송 제어)
       - 스트리밍 재생 제어 : RTSP(RTP 스트림 전반에 대한 제어)
     - 디지털 방송용
       - 비디오 압축 전송용 프로토콜 : `MPEG-2, H.264` 등
       - 음성 압축 전송용 프로토콜 : `MPEG-2 AUDIO, AC-3` 등

   - 멀티미디어 스트림 재생

3. **프로그래밍 언어상의 스트림**

   - `C 언어`에서의 스트림

     - 연속된 문자 또는 데이터

       *크게 텍스트9바이트) 스트림 및 바이너리(이진) 스트림으로 구분*

     - `ANSI C`의 표준 파일 스트림 : `stdin, stdout, stderr` 등

   - 스트림 사용 이유

     => **물리 디스크 상의 파일, 장치들을 통일된 방식으로 다루기 위한 가상적인 개념**

     => **따라서 스트림은 어디서 나왔는지 어디로 가는지 신경 쓸 필요 없이 자유롭게 어떤 장치 및 프로세스, 파일들과 연결될 수 있어 많은 편리성 제공**

# **35. 메모리 맵 파일(mmap)**

---

**프로그래밍은 데이터를 입력받아 처리하고 출력하는 작업**이고, 이 데이터는 파일로써 디스크에 존재합니다. 내가 만약 '회원명단.csv'란 엑셀 파일을 가지고 여러 데이터 처리 작업을 해야한다고 가정해봅시다. 그렇다면 해당 파일에서 '읽기', '쓰기'를 할 때마다 디스크까지 접근해야할텐데 이러면 속도가 너무 느리지 않을까요? 만약 그 **파일이 디스크가 아닌 메모리에 있다면 접근 속도는 엄청나게 향상**되지않을까요? 이러한 발상은 최소한의 디스크 접근을 위한 버퍼 입출력에서 한 단계 더 나아간 발상입니다. '버퍼'란 개념이 아닌 디스크-메모리간의 '**페이지(page)**' 개념을 활용한 방식으로, 프로세스의 가상 메모리 주소 공간에 파일을 매핑한 뒤 가상 메모리 주소에 직접 접근하는 것으로 파일 읽기/쓰기를 수행합니다. 이 때, 가상메모리-디스크 간의 동기화 문제는 어떤 식으로 해결할 수 있는지도 알아보겠습니다.

먼저 메모리 맵 파일의 장점과 단점을 살펴보겠습니다.

***장점***

1. 버퍼나 파일 처리를 위한 추가적인 자료구조가 필요없습니다. OS에서 페이징 기법을 사용하여 파일의 내용을 관리하며, 페이지 크기(일반적으로 4KB)만큼 '읽기/쓰기' 작업이 가능합니다.
2. 대용량의 데이터를 처리할 때 매우 효율적입니다. 파일의 크기가 매우 크더라도 필요한 부분만 페이지로 불러와 작업할 수 있습니다.
3. 전통적인 파일 입출력 API보다 속도가 빠릅니다. API는 내부적으로 시스템콜을 이용하기 때문에 작업을 수행하는 동안 유저모드-커널모드간의 전환을 위한 인터럽트가 오버헤드로 작용합니다. 메모리 맵 파일은 페이지 단위로 자료를 불러올때 발생하는 '페이지 폴트' 외에 어떠한 비용도 없습니다.

***단점***

1. 메모리 맵 파일은 항상 페이지 크기의 정수배만 가능합니다. 즉, 크기가 작은 파일이라면 메모리 맵 파일을 이용하는 것이 메모리 공간의 낭비로 이어질 수 있습니다.
2. 메모리 맵 파일의 오버헤드는 크기가 큰 파일에서 '페이지 폴트'로 인해 페이지를 새로 불러와야할 때입니다. 이 비용이 모든 경우에 가장 저렴하지는 않습니다.

- 리눅스에선 **`mmap()` 시스템콜**을 사용해서 객체를 메모리에 맵핑할 수 있습니다.

```c
#include <sys.mman.h>

void* mmap(void* addr, // 어느 주소에 맵핑되길 원하는지 커널에 알려줍니다.(보통 0을 넘김)
    size_t len,
    int prot,       // 메모리 보호 정책을 설정합니다.
    int flags,      // 맵핑의 유형과 그 동작에 관한 요소를 명시합니다.
    int fd,
    off_t offset);  // 해당 offset 위치에서 len바이트만큼 메모리에 맵핑하도록 요청합니다.
```

prot 매개변수는 '**메모리 보호 정책**'을 설정하기위한 것으로, 다음에 있는 플래그 중에서 하나 이상을 OR 연산으로 묶을 수 있습니다.

- **PROT_NONE**: 접근이 불가능한 페이지(거의 사용되지 않습니다.)
- **PROT_READ**: '읽기'가 가능한 페이지
- **PROT_WRITE**: '쓰기'가 가능한 페이지
- **PROT_EXEC**: '실행'이 가능한 페이지

flags 매개변수는 매핑할 메모리의 유형과 그 동작에 관한 몇 가지 요소를 명시합니다. 마찬가지로 OR 연산으로 묶을 수 있습니다.

- **MAP_FIXED**: mmap() 시스템콜의 addr 매개변수를 원하는 메모리 주소 공간을 알려준다는 목적을 넘어 해당 주소의 메모리 공간이 아니면 호출이 실패하게끔 확고히 커널에 요청합니다.
- **MAP_PRIVATE**: 매핑된 메모리 공간의 '쓰기'가 발생하더라도 실제 파일과 해당 메모리 공간을 공유하고있는 다른 프로세스에 반영하지 않습니다. 원본을 훼손하지 않고, 수정된 복사본이 생깁니다. (copy-on-write) 
- **MAP_SHARED**: 같은 파일을 메모리에 매핑한 모든 프로세스와 매핑된 메모리 영역을 공유합니다. 당연히 다른 프로세스에 의해 수정됬다면 해당 영역에 '읽기'를 할때 반영됩니다.

 

다음 예제는 fd가 가리키는 파일의 첫 바이트부터 len 바이트까지를 '읽기' 전용으로 메모리에 매핑합니다.

```
void* p;
p = mmap(0, len, PROT_READ, MAP_SHARED, fd, 0);
if (p == MAP_FAILED)
    perror("mmap");
```

 

mmap() 시스템콜은 호출이 성공하면 매핑된 메모리 주소를 반환합니다. 실패시 'MAP_FAILED'를 반환하고 errno를 적절한 값으로 설정합니다.

- **EACCESS**: 주어진 파일 디스크립터가 일반 파일이 아니거나 파일이 prot이나 flags 매개변수와 충돌을 일으키는 모드로 오픈되었을 때
- **EAGAIN**: 파일 락으로 파일이 잠긴 상태일 때
- **EBADF**: 주어진 파일 디스크립터가 유효하지 않을 때
- **EINVAL**: addr, len, off 중 하나 이상의 매개변수가 유효하지 않을 때
- **ENFILE**: 시스템에서 오픈할 수 있는 파일 개수를 초과했을 때
- **ENODEV**: 파일시스템에서 해당 파일에 대한 메모리 매핑을 지원하지 않을 때 
- **ENOMEM**: 프로세스에 사용 가능한 메모리가 부족할 때
- **EOVERFLOW**: addr+len 값이 주소 공간의 크기를 초과했을 때
- **EPERM**: PROT_EXEC가 설정되었지만 파일시스템이 noexec 모드로 마운트되었을 때

 

mmap() 시스템콜로 생성한 매핑된 메모리 영역을 해제하고 싶다면 **munmap() 시스템콜**을 사용합니다.

```c
#include <sys/mman.h>
 
int munmap(void* addr, size_t len);
```

 

다음은 mmap() 시스템콜을 이용해서 표준 출력으로 사용자가 선택한 파일의 내용을 출력하는 예제입니다.

```c
#include <stdio.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/mman.h>
 
int main(int argc, char* argv[]) {
    struct stat sb;
    off_t len;
    char* p;
    int fd;
    
    if (argc < 2) {
        fprintf(stderr, "usage: %s [file] \n", argv[0]);
        return 1;
    }
    
    // 이하 에러 처리문 생략합니다.
    fd = open(argv[1], O_RDONLY);
    
    fstat(fd, &sb);
    if (!S_ISREG(sb.st_mode)) return 1;
    
    p = mmap(0, sb.st_size, PROT_READ, MAP_SHARED, fd, 0);
    
    for (len = 0; len < sb.st_size; ++len)
        putchar(p[len]);
    
    close(fd);
    munmap(p, sb.st_size); // 매핑된 메모리 영역을 해제합니다.
}
```

# **36. 윈도우에서 작성한 파일을 리눅스에 올릴 때 생기는 ^M 제거하기**

윈도우에서 작성한 파일을 Unix/Linux상에 올리면 개행 문자가 깨져서 ^M가 보이는 경우를 봤을 것이다.

이것은 윈도우에서는 CRLF 가 개행인데 Linux/Unix에서는 LF가 개행이라 나타나는 현상으로 보면 된다.



**CR,LF 뜻**

- 라인피드(LF : Line Feed) => 현재 위치에서 바로 아래로 이동
- 캐리지리턴(CR: Carriage return) => 커서의 위치를 앞으로 이동

설명은 이정도로 하고 제거 방법을 알아 보자

**vi, vim에서 제거하는 방법**

```bash
:%s/^M//g
# 여기서 ^M은 ^+M이 아니고 Ctrl + v + m 이다.
```

vi에서 입력한 각 명령어는 아래와 같다.

```bash
:			# vi, vim에서 명령어를 입력
:%s			# 문자열을 치환하겠다는 명령
:%s/^M		# ^M 문자열을 치환하겠다
:%s/^M//	# ^M 문자열을 공백으로 치환하겠다.
:%s/^M//g	# 해당 문서 전체에서 ^M 문자열을 공백으로 치환하겠다.
```

**윈도우에서 Unix / Linux 로 파일 올릴 때 `^M` 안생기게 하는 방법**

운영체제 마다 줄바꿈 정의가 다르다. 

각 운영체제 마다 정의를 살펴 보자.

- `윈도우/DOS` : CRLF 조합으로 줄바꿈을 정의
- `Unix/Linux/C` : LF 만으로 줄바꿈을 정의

이래서 윈도우에서 작성 된 것을 Unix/Linux 계열로 올리면 vi로 편집했을 때 `^M`이 붙는 것을 확인 할 수 있다.

물론 ftp로 올릴 때 방식이 텍스트/2진 어떤 것을 선택하느냐에 따라 달라질 수 있다.

텍스트 방식으로 올리면 데이터 변환이 생겨 /r/n -> /n 으로 된다. 

2진 모드로 올릴때는 변환이 생기지 않아 윈도우에서 작성된 파일이 컴파일이 안되거나 script작성 된것이 동작 안할 수가 있다.



[sftp 전송 방식]

- 텍스트 방식 : 줄바꿈에 대한 변환이 일어남
- 2진 모드 : 줄바굼에 대한 변환이 일어나지 않음 ( 문제가 발생할 수 있다. )

# **37. 콜 스택(Call Stack)**

> 실행 중인 **코드를 추적**하는 공간이 **콜스택**
>
> => **`함수(function)`**의 **`호출(call)`**을 기록하는 **`스택(stack)`** 자료구조

**`콜 스택(call stack)`**이란 컴퓨터 프로그램에서 현재 실행 중인 서브루틴에 관한 정보를 저장하는 스택 자료구조이다.

`실행 스택(execution stack)`, `제어 스택(control stack)`, `런타인 스택(run-time)`, `기계 스택(machine stack)`이라고도 하며, 그냥 줄여서 **`스택(stack)`**이라고도 한다. 소프트웨어 프로그램의 기능 수행에 있어 콜 스택의 관리가 중요함에도 불구하고, 상세한 구현은 고급 프로그래미이 언어에서는 보통 감추어지며 자동화되어 있따. 많은 컴퓨터들이 스택 관리를 위한 특별한 명령어(기계어)를 제공하고 있다.

콜 스택을 사용하는 목적은 여러 가지이지만, 주된 이유는 현재 실행중인 서브루틴의 실행이 끝났을 때, 제어를 반환할 지점을 보관하기 위해서이다. 실행 중인 서브루틴은, 호출되어서 그 실행이 아직 완료되지는 않았지만, 완료된 후에는 호출된 지점으로 제어를 넘겨야 한다.

서브루틴의 이러한 실행은 여러 단계로 중첩될 수도 있는데, 다른 호출에 의해서 또 다른 서브루틴으로 넘어가버리거나, 재귀같은 특별한 경우가 있다. 이러한 중첩의 특성 때문에 스택 자료구조를 사용하는 것이다.

예를 들어 만약 `DrawSquare` 라는 서브루틴이 있고 `DrawSquare`가 `DrawLine`을 호출한다고 해보자. (Square는 Line 4개니까 4번 호출하게 될 것이다.)

`DrawLine` 서브루틴은 자신이 호출된 뒤 어디로 돌아가야할지 알고 있어야만 한다. (그래야 4번만 불릴 것이다.) 이를 구현하기 위해 **메모리 주소**가 서브루틴이 끝날 때 **반환 주소(return address)**를 확인하도록 하고, 이를 위해 서브루틴 호출 시 콜 스택에 반환 주소를 제일 먼저 집어 넣는다.

# **38. ARM 프로세서**

> *`ARM 아키텍쳐`가 무엇인지 모르는 사람은 아래  포스팅을 보고 오시길 바란다.*
>
> https://ko.wikipedia.org/wiki/ARM_%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98
>
> https://m.blog.naver.com/PostView.nhn?blogId=suresofttech&logNo=221249244004&proxyReferer=https:%2F%2Fwww.google.com%2F
>
> 
>
> *그렇다면 ARM 프로세서를 배워야 하는 이유가 무엇일까?*
>
> - **많은 소형기기에서 ARM 프로세서를 탑재**
>
>   - `ARM 프로세스`를 배우는 가장 큰 이유는 `ARM 프로세서`를 많이 사용하기 때문이다. 대부분의 휴대기기에는 `ARM 프로세서`가 탑재돼 있다. 우리가 항상 들고다니는 안드로이드 스마트폰에 탑재된 `CPU`가 바로 `ARM 프로세서`이다.
>
>   - 휴대전화 뿐만 아니라 사물인터넷(IoT)에도 많이 사용하고 있다. `ARM 프로세서`가 저전력에 소모 전류가 적기 대문에 이렇듯 소형기기에 많이들 사용하고 있는 것!
>
>     또한, 많은 `ARM 프로세서` 개발자 풀이 존재한다.
>
> - **프로그램의 세부 동작 원리를 파악**
>
>   - `ARM 프로세서`의 기본 원리를 알면 `CPU`가 프로그램을 어떻게 돌리는지 정확히 알 수 가 있다. 우리가 작성하는 코드는 C, C++, JAVA, Python 등의 언어일텐데, 이 코드들은 모두 컴파일 후 **ARM 어셈블리 코드**로 돌게 되어있다. 따라서, `ARM 프로세서`의 동작 원리를 알면 자연히 프로그램이 정말 어떻게 돌아가는지 알 수 있다. 때문에 프로세서의 원리를 제대로 파악하는 개발자는 더욱 안정적인 코드를 작성할 수 있게 된다.
>   - 물론, `ARM 프로세서`의 세부 원리를 잘 몰라도 개발하는 데 큰 지장은 없지만, 우리가 원하는 *고급 개발자*가 되기 위해서는 반드시 넘어야 하는 산이 바로 `ARM 프로세서`인 것!!
>   - `스택 오염, Stack Overflow, StackUnderflow`등 의 문제는 `ARM 프로세서`의 세부 동작 원리를 모르면 문제를 분석하거나 해결하기 힘들다. `Secure Monitor Call` 도 마찬가지이다.
>
> - **RTOS나 운영체제 커널의 세부 동작 원리를 파악**
>
>   - 마지막으로 ARM 프로세서를 알아야 하는 이유는 `RTOS`나 `운영체제(OS)`의 세부 동작 원리를 알아야 할 때이다. `ARM 프로세서`는 `RTOS`나 리눅스와 같은 범용 운영체제와 함께 돌아가기 때문이다. -> *`RTOS`나 리눅스와 같은 `운영체제(os)`의 핵심 원리를 알기 위해서 `ARM 프로세서`를 알아야 한다.* 
>   - `Exception, Scheduling`과 같은 세부 동작은 대부분 `ARM 어셈블리 코드`로 되어있기 때문이다.
>   - 시스템이 처음 부팅할 때 실행하는 코드를 **스타트업 코드**라고 하는데, 이를 작성하기 위해서는 `ARM 프로세서`의 기본 원리를 알아야 한다.
>
> *ARM 프로세서을 알고 모를 때 어떤 차이가 있을까?*
>
> 이 차이는 평소에 잘 드러나지 않지만, 해결하기 어려운 문제를 만났을 때 드러난다.
>
> **`Secure Monitor Call`**을 실행하고 나서 시스템이 무감됐다는 예시를 들어보자. 이 문제를 리포트 받은 후 `ARM 프로세서`를 잘 아는 개발자라면 이렇게 생각할 것이다.
>
> ->  'Secure Monitor Call'을 실행하면 트러스트 존으로 동작 모드를 변경할 텐데. 이 때 실행하는 어셈블리 코드의 아규먼트는 제대로 전달이 된 것일까?
>
> ->  Secure Mode와 Non-Secure 모드 별로 페이지 테이블의 시작주소인 TTBR 이 다른데. 혹시 이 설정이 잘못됐나?
>
> -> 트러스트 존에 로그를 추가해서 어디까지 실행되는지 확인해볼까?
>
> -> 'Secure Monitor Call'을 여러 CPU가 실행하려고 시도하지는 않나? 해당 코드에 락을 좀 걸어볼까?
>
> 
>
> ***이렇게 전체 그림을 그리고 문제를 분석하는 개발자와 'Secure Monitor Call'이란 용어가 뭔 소리인지도 몰라 조용히 구글링을 하는 개발자 중 누가 먼저 빨리 문제를 해결할까요? 여러분이 관리자라면 둘 중 어느 개발자에게 문제를 맡길까요? 구글링을 하는 개발자는 아니겠죠?***



# **NN. ARM DATA ABORT EXCEPTION**



# to do study

- 리눅스의 Process, Task, Thread ...
- Kconfig , Makefile, defconfig ...
- Message queue
- timer handler
- proc
- 가상파일시스템

